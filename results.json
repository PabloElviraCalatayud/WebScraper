[
  {
    "repository": "PubMed",
    "title": "Bioinspired algorithm based on Physarum polycephalum for the formation of decentralized mesh networks in multi-robot systems.",
    "authors": "Dieisson Martinelli; André Schneider de Oliveira; Vivian Cremer Kalempa",
    "abstract": "This work proposes a bio-inspired approach for decentralized coordination in multi-robot systems, applied to a simulated port scenario. The methodology integrates the Robot Operating System (ROS) with the Stage simulator, enabling modeling of a port environment with three autonomous robots, each capable of navigation and obstacle avoidance. The main contribution is a connectivity module inspired by Physarum polycephalum, which manages the mesh network and allows decentralized task sharing whenever connections exist. The algorithm adapts continuously to robot movement and environmental changes, ensuring efficient communication when possible and autonomous operation when disconnected. Experiments confirmed that robots relying only on local perception can form and maintain a functional network. Results showed connections established in less than two seconds on average and reconfigured almost instantly after fault, demonstrating resilience. New robots were integrated in only 0.092 seconds on average, validating scalability. A comparison between scenarios with and without communication revealed a 17.87% efficiency improvement, reducing execution time from 621 to 510 seconds thanks to dynamic load balancing. In summary, the study demonstrates the feasibility of a bio-inspired solution for decentralized coordination in multi-robot systems, capable of generating efficient, resilient, and adaptable communication networks, essential for cooperation in real-world environments. A demonstration video is available at https://youtu.be/ZGPswbfeRKA. ",
    "doi": "10.1038/s41598-025-33456-y"
  },
  {
    "repository": "PubMed",
    "title": "Development of an active counter unmanned aerial system with integrated autonomous mobile robot for inspection and defense.",
    "authors": "Shuliang Li; Kuangang Fan; Wenlong Cai; Lingqiu Wang; Aigen Fan",
    "abstract": "As the issue of unauthorized drone flights becomes increasingly severe, the demand for intelligent counter-drone systems is growing. Existing fixed-deployment counter-drone systems suffer from limitations such as restricted coverage and passive defense. This study proposes integrating Autonomous Mobile Robot (AMR) into the Counter-Unmanned Aerial Systems (C-UAS) architecture to develop a mobile solution with active inspection capabilities. First, a kinematic model was established to describe the mutual mapping between the motor speed and the robot speed. Second, a manual mapping method was introduced; by integrating the Extended Kalman Filter (EKF) algorithm to fuse multi-sensor data, this method enables centimeter-level positioning. Finally, the system was integrated and verified in both simulated and real-world environments. Experimental results show that the Active C-UAS can successfully navigate to the drone intrusion area while achieving high positioning accuracy, path quality, and control performance. Specifically, the Mean Absolute Error(MAE) of the actual navigation target point is only 0.35 m, and the attitude estimation error is less than 1°. Moreover, the system maintains excellent trajectory smoothness in long-distance dynamic scenarios, achieving 89.9% of the Smoothness Coefficient (SMC) benchmark from global planning while only incurring a 1.14% increase in actual path length. Finally, high control accuracy and rapid response are confirmed by an MAE below 0.02 m/s between the commanded and actual speeds, alongside a Pearson correlation coefficient ([Formula: see text]) exceeding 0.9 between the two speed profiles. ",
    "doi": "10.1038/s41598-025-28565-7"
  },
  {
    "repository": "PubMed",
    "title": "Development of an Orchard Inspection Robot: A ROS-Based LiDAR-SLAM System with Hybrid A*-DWA Navigation.",
    "authors": "Jiwei Qu; Yanqiu Gu; Zhinuo Qiu; Kangquan Guo; Qingzhen Zhu",
    "abstract": "The application of orchard inspection robots has become increasingly widespread. How-ever, achieving autonomous navigation in unstructured environments continues to pre-sent significant challenges. This study investigates the Simultaneous Localization and Mapping (SLAM) navigation system of an orchard inspection robot and evaluates its performance using Light Detection and Ranging (LiDAR) technology. A mobile robot that integrates tightly coupled multi-sensors is developed and implemented. The integration of LiDAR and Inertial Measurement Units (IMUs) enables the perception of environmental information. Moreover, the robot's kinematic model is established, and coordinate transformations are performed based on the Unified Robotics Description Format (URDF). The URDF facilitates the visualization of robot features within the Robot Operating System (ROS). ROS navigation nodes are configured for path planning, where an improved A* algorithm, combined with the Dynamic Window Approach (DWA), is introduced to achieve efficient global and local path planning. The comparison of the simulation results with classical algorithms demonstrated the implemented algorithm exhibits superior search efficiency and smoothness. The robot's navigation performance is rigorously tested, focusing on navigation accuracy and obstacle avoidance capability. Results demonstrated that, during temporary stops at waypoints, the robot exhibits an average lateral deviation of 0.163 m and a longitudinal deviation of 0.282 m from the target point. The average braking time and startup time of the robot at the four waypoints are 0.46 s and 0.64 s, respectively. In obstacle avoidance tests, optimal performance is observed with an expansion radius of 0.4 m across various obstacle sizes. The proposed combined method achieves efficient and stable global and local path planning, serving as a reference for future applications of mobile inspection robots in autonomous navigation. ",
    "doi": "10.3390/s25216662"
  },
  {
    "repository": "PubMed",
    "title": "Multi-Domain Robot Swarm for Industrial Mapping and Asset Monitoring: Technical Challenges and Solutions.",
    "authors": "Fethi Ouerdane; Ahmed Abubaker; Mubarak Badamasi Aremu; Mohammed Abdel-Nasser; Ahmed Eltayeb; Karim Asif Sattar; Abdulrahman Javaid; Ahmed Ibnouf; Sami El Ferik; Mustafa Alnasser",
    "abstract": "Industrial environments are complex, making the monitoring of gauge meters challenging. This is especially true in confined spaces, underground, or at high altitudes. These difficulties underscore the need for intelligent solutions in the inspection and monitoring of plant assets, such as gauge meters. In this study, we plan to integrate unmanned ground vehicles and unmanned aerial vehicles to address the challenge, but the integration of these heterogeneous systems introduces additional complexities in terms of coordination, interoperability, and communication. Our goal is to develop a multi-domain robotic swarm system for industrial mapping and asset monitoring. We created an experimental setup to simulate industrial inspection tasks, involving the integration of a TurtleBot 2 and a QDrone 2. The TurtleBot 2 utilizes simultaneous localization and mapping (SLAM) technology, along with a LiDAR sensor, for mapping and navigation purposes. The QDrone 2 captures high-resolution images of meter gauges. We evaluated the system's performance in both simulation and real-world environments. The system achieved accurate mapping, high localization, and landing precision, with 84% accuracy in detecting meter gauges. It also reached 87.5% accuracy in reading gauge indicators using the paddle OCR algorithm. The system navigated complex environments effectively, showcasing the potential for real-time collaboration between ground and aerial robotic platforms. ",
    "doi": "10.3390/s25206295"
  },
  {
    "repository": "PubMed",
    "title": "A*-TEB: An Improved A* Algorithm Based on the TEB Strategy for Multi-Robot Motion Planning.",
    "authors": "Xu Li; Tuanjie Li; Yan Zhang; Yulin Zhang; Ziang Li; Lixiang Ban; Kecheng Sun",
    "abstract": "Multi-robot motion planning (MRMP) requires each robot to possess strong local planning capabilities while maintaining global consistency. However, existing research often fails to address both global and local planning simultaneously, resulting in conflicts in real-time path execution. The A* algorithm is widely used for global path planning due to its adaptability and search efficiency, while the Timed Elastic Band (TEB) algorithm excels in local trajectory optimization and real-time dynamic obstacle avoidance. This paper presents a novel motion planning framework integrating an improved A* algorithm with an enhanced TEB strategy to address both levels of planning collaboratively. The proposed improvements include the introduction of steering costs and dynamic weights into the A* algorithm to enhance path smoothness and efficiency, and a hierarchical obstacle treatment in TEB for improved local avoidance. Simulation and real-world experiments conducted with ROS confirmed the feasibility and effectiveness of the method. Compared to the traditional A* algorithm, the proposed framework reduces the average path length by 5.2%, shortens completion time by 11.5%, and decreases inflection points by 66.7%, demonstrating superior performance for multi-robot systems in dynamic environments. ",
    "doi": "10.3390/s25196117"
  },
  {
    "repository": "PubMed",
    "title": "Risk-Aware Reinforcement Learning with Dynamic Safety Filter for Collision Risk Mitigation in Mobile Robot Navigation.",
    "authors": "Bingbing Guo; Guina Wang; Yiyang Chen; Yue Gao; Qian Xie",
    "abstract": "Mobile robots face collision risk avoidance challenges in dynamic environments, necessitating that we address the safety and adaptability shortcomings of traditional navigation methods. Traditional methods rely on predefined rules, making it difficult to achieve flexible, safe, and real-time obstacle avoidance in complex, dynamic environments. To address this issue, a risk-aware, dynamic, adaptive regulation barrier policy optimization (RADAR-BPO) method is proposed, combining proximal policy optimization (PPO) with the control barrier function (CBF). RADAR-BPO generates exploratory actions using PPO and constructs a real-time safety filter using the CBF. This method uses quadratic programming to minimize risky actions, thereby ensuring safe obstacle avoidance while maintaining navigation efficiency. Testing of three phased learning environments in the ROS Gazebo simulation environment demonstrated that the proposed method achieves an obstacle avoidance success rate of nearly 90% in complex, dynamic, multi-obstacle environments and improves the overall mission success rate, validating its robustness and effectiveness in complex dynamic scenarios. ",
    "doi": "10.3390/s25175488"
  },
  {
    "repository": "PubMed",
    "title": "A multi-robot collaborative manipulation framework for dynamic and obstacle-dense environments: integration of deep learning for real-time task execution.",
    "authors": "Afnan Ahmed Adil; Saber Sakhrieh; Jinane Mounsef; Noel Maalouf",
    "abstract": "This paper presents a multi-robot collaborative manipulation framework, implemented in the Gazebo simulation environment, designed to enable the execution of autonomous tasks by mobile manipulators in dynamic environments and dense obstacles. The system consists of multiple mobile robot platforms, each equipped with a robotic manipulator, a simulated RGB-D camera, and a 2D LiDAR sensor on the mobile base, facilitating task coordination, object detection, and advanced collision avoidance within a simulated warehouse setting. A leader-follower architecture governs collaboration, allowing for the dynamic formation of teams to tackle tasks requiring combined effort, such as transporting heavy objects. Task allocation and control are achieved through a centralized control structure architecture in which the leader robot coordinates subordinate units based on high-level task assignments. The framework incorporates deep learning-based object detection (YOLOv2) to identify target objects using a simulated RGB-D camera mounted on the manipulator's end-effector. Path planning is achieved through a sampling-based algorithm that is integrated with the LiDAR data to facilitate precise obstacle avoidance and localization. It also provides real-time path rerouting for safe navigation when dynamically moving obstacles, such as humans or other entities, intersect planned paths. This functionality ensures uninterrupted task execution and enhances safety in human-robot shared spaces. High-level task scheduling and control transitions are managed using MATLAB and Stateflow logic, while ROS facilitates data communication between MATLAB, Simulink, and Gazebo. This multirobot architecture is adaptable, allowing configuration of team size for collaborative tasks based on load requirements and environmental complexity. By integrating computer vision and deep learning for visual processing, and YOLOv2 for object detection, the system efficiently identifies, picks, and transports objects to designated locations, demonstrating the scalability of multi-robot framework for future applications in logistics automation, collaborative manufacturing, and dynamic human-robot interaction scenarios. ",
    "doi": "10.3389/frobt.2025.1585544"
  },
  {
    "repository": "PubMed",
    "title": "Utilizing a deep neural network for robot semantic classification in indoor environments.",
    "authors": "Tareq Alhmiedat; Osama Moh'd Alia",
    "abstract": "The utilization of semantic knowledge has ushered in a new era in robot navigation and localization, enabling heightened information representation. This paper introduces an enhanced semantic classification system that leverages a cost-effective, low-processing LiDAR unit in conjunction with a proficient deep neural network (DNN) model. Unlike vision-based methods, which are often susceptible to lighting conditions and environmental variability, LiDAR offers more robust and consistent performance in diverse settings. The Robot Operating System (ROS) development environment was employed alongside a two-wheel-drive robot platform to evaluate the system's efficiency and accuracy. The efficacy of the proposed system has been rigorously validated through both simulation studies and real-world scenarios across two distinct experimental testbeds characterized by varying features. Encouragingly, the results obtained showcase a high level of semantic classification accuracy, standing competitively against diverse semantic classification systems. Furthermore, the developed system successfully generated a semantic map of the navigational area with exceptional classification precision. ",
    "doi": "10.1038/s41598-025-07921-7"
  },
  {
    "repository": "PubMed",
    "title": "Obstacle Avoidance Technique for Mobile Robots at Autonomous Human-Robot Collaborative Warehouse Environments.",
    "authors": "Lucas C Sousa; Yago M R Silva; Vinícius B Schettino; Tatiana M B Santos; Alessandro R L Zachi; Josiel A Gouvêa; Milena F Pinto",
    "abstract": "This paper presents an obstacle avoidance technique for a mobile robot in human-robot collaborative (HRC) tasks. The proposed solution uses fuzzy logic rules and a convolutional neural network (CNN) in an integrated approach to detect objects during vehicle movement. The goal is to improve the robot's navigation autonomously and ensure the safety of people and equipment in dynamic environments. Using this technique, it is possible to provide important references to the robot's internal control system, guiding it to continuously adjust its velocity and yaw in order to avoid obstacles (humans and moving objects) while following the path planned for its task. The approach aims to improve operational safety without compromising productivity, addressing critical challenges in collaborative robotics. The system was tested in a simulated environment using the Robot Operating System (ROS) and Gazebo to demonstrate the effectiveness of navigation and obstacle avoidance. The results obtained with the application of the proposed technique indicate that the framework allows real-time adaptation and safe interaction between robot and obstacles in complex and changing industrial workspaces. ",
    "doi": "10.3390/s25082387"
  },
  {
    "repository": "PubMed",
    "title": "Design and Implementation of an Interactive System for Service Robot Control and Monitoring.",
    "authors": "Jonas Machado Santana; Bruno Duarte Silveira; Crescencio Lima; Jose Diaz-Amado; Cléia Santos Libarino; Joao E Soares Marques; Dennis Barrios-Aranibar; Raquel E Patiño-Escarcina",
    "abstract": "This project aims to develop an interactive control system for an autonomous service robot using an ROS (robot operating system). The system integrates an intuitive web interface and an interactive chatbot supported by Google Gemini to enhance the control experience and personalization for the user. The methodology includes the integration of an API (application programming interface) to access a database storing user preferences, such as speed and frequent destinations. Furthermore, the system employs facial recognition, people groups' recognition, and adaptive responses from the chatbot for autonomous navigation, ensuring a service tailored to the individual needs of each user. To validate the proposal, it was implemented on an autonomous service robot, integrated into a motorized wheelchair. Tests demonstrated that the system effectively adjusts the wheelchair's behavior to user preferences, resulting in safer and more personalized navigation. The use of facial recognition and chatbot interaction provided more intuitive and efficient control. The developed system significantly improves the autonomy and quality of life for wheelchair users, proving to be a viable and efficient solution for autonomous and personalized control. The results indicate that integrating technologies like ROS, intuitive web interfaces, and interactive chatbots can transform the user experience of autonomous wheelchairs, better meeting the specific needs of users. ",
    "doi": "10.3390/s25040987"
  },
  {
    "repository": "PubMed",
    "title": "A novel navigation assistant method for substation inspection robot based on multisensory information fusion.",
    "authors": "Qiang Yang; Jingze Dong; Minyao Tan; Jiawei Wang; Dequan Guo; Hao Kang; Ping Wang",
    "abstract": "Due to the complex environment of the substation, the inspection work of the substation becomes time-consuming and laborious. As a result, the substation inspection robot has gradually become a hot research point. At present, the mainstream intelligent inspection robots for substations use high-precision LiDAR sensors for navigation, which has high navigation accuracy but cannot identify the types of obstacles and the road contour boundary, seriously affecting the inspection performance and efficiency. Meanwhile, the high-precision 3D laser radar is too expensive to afford. In order to solve these problems, a novel navigation assistant method is proposed in this paper, which is based on multisensory information fusion. Asynchronous information matching with multiple sensors was used to match the information collected by different sensors to deal with the time asynchrony. To express the height of obstacles, 2D laser radar was applied to create 3D imaging by being combined with inertial measurement unit (IMU). For perceiving and under-standing the substation environment independently by the inspection robot, ENet was given over to segment color point cloud maps, which was built by introducing optical sensor data. The method was implemented based on the ROS system and transplanted to embedded platform of the inspection robot. Finally, experimental results show that, compared with VLP-32C 3D laser radar sensors, the data volume of navigation assistance module had reduced by 95%. Meanwhile, after training the ENet network, the mean average accuracy value had achieved 86%, which meets the needs of practical engineering applications. In addition, the inspection robot equipped with the navigation assistance module is successfully tested in multiple substations, which shows that the robot can not only identify the road contour and the type of obstacles on the way, but also reduce the amount of data and the cost of hardware. ",
    "doi": "10.1016/j.jare.2025.01.016"
  },
  {
    "repository": "PubMed",
    "title": "A Real-Time Semantic Map Production System for Indoor Robot Navigation.",
    "authors": "Raghad Alqobali; Reem Alnasser; Asrar Rashidi; Maha Alshmrani; Tareq Alhmiedat",
    "abstract": "Although grid maps help mobile robots navigate in indoor environments, some lack semantic information that would allow the robot to perform advanced autonomous tasks. In this paper, a semantic map production system is proposed to facilitate indoor mobile robot navigation tasks. The developed system is based on the employment of LiDAR technology and a vision-based system to obtain a semantic map with rich information, and it has been validated using the robot operating system (ROS) and you only look once (YOLO) v3 object detection model in simulation experiments conducted in indoor environments, adopting low-cost, -size, and -memory computers for increased accessibility. The obtained results are efficient in terms of object recognition accuracy, object localization error, and semantic map production precision, with an average map construction accuracy of 78.86%. ",
    "doi": "10.3390/s24206691"
  },
  {
    "repository": "PubMed",
    "title": "Apple-Harvesting Robot Based on the YOLOv5-RACF Model.",
    "authors": "Fengwu Zhu; Weijian Zhang; Suyu Wang; Bo Jiang; Xin Feng; Qinglai Zhao",
    "abstract": "To address the issue of automated apple harvesting in orchards, we propose a YOLOv5-RACF algorithm for identifying apples and calculating apple diameters. This algorithm employs the robot operating dystem (ROS) to control the robot's locomotion system, Lidar mapping, and navigation, as well as the robotic arm's posture and grasping operations, achieving automated apple harvesting and placement. The tests were conducted in an actual orchard environment. The algorithm model achieved an average apple detection accuracy (mAP@0.5) of 98.748% and a (mAP@0.5:0.95) of 90.02%. The time to calculate the diameter of one apple was 0.13 s, with a measurement accuracy within an error range of 1-3 mm. The robot takes an average of 9 s to pick an apple and return to the initial pose. These results demonstrate the system's efficiency and reliability in real agricultural environments. ",
    "doi": "10.3390/biomimetics9080495"
  },
  {
    "repository": "PubMed",
    "title": "Path planning of mobile robot based on improved TD3 algorithm in dynamic environment.",
    "authors": "Peng Li; Donghui Chen; Yuchen Wang; Lanyong Zhang; Shiquan Zhao",
    "abstract": "This paper proposes an improved TD3 (Twin Delayed Deep Deterministic Policy Gradient) algorithm to address the flaws of low success rate and slow training speed, when using the original TD3 algorithm in mobile robot path planning in dynamic environment. Firstly, prioritized experience replay and transfer learning are introduced to enhance the learning efficiency, where the probability of beneficial experiences being sampled in the experience pool is increased, and the pre-trained model is applied in an obstacle-free environment as the initial model for training in a dynamic environment. Secondly, dynamic delay update strategy is devised and OU noise is added to improve the success rate of path planning, where the probability of missing high-quality value estimate is reduced through changing the delay update interval dynamically, and the correlated exploration of the mobile robot inertial navigation system in the dynamic environment is temporally improved. The algorithm is tested by simulation where the Turtlebot3 robot model as a training object, the ROS melodic operating system and Gazebo simulation software as an experimental environment. Meanwhile, the result shows that the improved TD3 algorithm has a 16.6 % increase in success rate and a 23.5 % reduction in algorithm training time. A generalization experiment was designed finally, and it indicates that superior generation performance has been acquired in mobile robot path planning with continuous action spaces through the improved TD3 algorithm. ",
    "doi": "10.1016/j.heliyon.2024.e32167"
  },
  {
    "repository": "PubMed",
    "title": "Inspection Robot Navigation Based on Improved TD3 Algorithm.",
    "authors": "Bo Huang; Jiacheng Xie; Jiawei Yan",
    "abstract": "The swift advancements in robotics have rendered navigation an essential task for mobile robots. While map-based navigation methods depend on global environmental maps for decision-making, their efficacy in unfamiliar or dynamic settings falls short. Current deep reinforcement learning navigation strategies can navigate successfully without pre-existing map data, yet they grapple with issues like inefficient training, slow convergence, and infrequent rewards. To tackle these challenges, this study introduces an improved two-delay depth deterministic policy gradient algorithm (LP-TD3) for local planning navigation. Initially, the integration of the long-short-term memory (LSTM) module with the Prioritized Experience Re-play (PER) mechanism into the existing TD3 framework was performed to optimize training and improve the efficiency of experience data utilization. Furthermore, the incorporation of an Intrinsic Curiosity Module (ICM) merges intrinsic with extrinsic rewards to tackle sparse reward problems and enhance exploratory behavior. Experimental evaluations using ROS and Gazebo simulators demonstrate that the proposed method outperforms the original on various performance metrics. ",
    "doi": "10.3390/s24082525"
  },
  {
    "repository": "PubMed",
    "title": "Improved Hybrid Model for Obstacle Detection and Avoidance in Robot Operating System Framework (Rapidly Exploring Random Tree and Dynamic Windows Approach).",
    "authors": "Ndidiamaka Adiuku; Nicolas P Avdelidis; Gilbert Tang; Angelos Plastropoulos",
    "abstract": "The integration of machine learning and robotics brings promising potential to tackle the application challenges of mobile robot navigation in industries. The real-world environment is highly dynamic and unpredictable, with increasing necessities for efficiency and safety. This demands a multi-faceted approach that combines advanced sensing, robust obstacle detection, and avoidance mechanisms for an effective robot navigation experience. While hybrid methods with default robot operating system (ROS) navigation stack have demonstrated significant results, their performance in real time and highly dynamic environments remains a challenge. These environments are characterized by continuously changing conditions, which can impact the precision of obstacle detection systems and efficient avoidance control decision-making processes. In response to these challenges, this paper presents a novel solution that combines a rapidly exploring random tree (RRT)-integrated ROS navigation stack and a pre-trained YOLOv7 object detection model to enhance the capability of the developed work on the NAV-YOLO system. The proposed approach leveraged the high accuracy of YOLOv7 obstacle detection and the efficient path-planning capabilities of RRT and dynamic windows approach (DWA) to improve the navigation performance of mobile robots in real-world complex and dynamically changing settings. Extensive simulation and real-world robot platform experiments were conducted to evaluate the efficiency of the proposed solution. The result demonstrated a high-level obstacle avoidance capability, ensuring the safety and efficiency of mobile robot navigation operations in aviation environments. ",
    "doi": "10.3390/s24072262"
  },
  {
    "repository": "PubMed",
    "title": "Design and implementation of origami robot ROS-based SLAM and autonomous navigation.",
    "authors": "Lijuan Zhao; Tianyi Zhang; Zuen Shang",
    "abstract": "In this study an innovative parameterized water-bomb wheel modeling method based on recursive solving are introduced, significantly reducing the modeling workload compared to traditional methods. A multi-link supporting structure is designed upon the foundation of the water-bomb wheel model. The effectiveness of the supporting structure is verified through simulations and experiments. For robots equipped with this water-bomb wheel featuring the multi-link support, base on the kinematic model of multi-link structure, a mapping algorithm that incorporates parameterized kinematic solutions and IMU-fused parameterized odometry is proposed. Based on this algorithm, SLAM and autonomous navigation experiments are carried out in simulation environment and real environment respectively. Compared with the traditional algorithm, this algorithm the precision of SLAM is enhanced, achieving high-precision SLAM and autonomous navigation with a robot error rate below 5%. ",
    "doi": "10.1371/journal.pone.0298951"
  },
  {
    "repository": "PubMed",
    "title": "How can social robot use cases in healthcare be pushed - with an interoperable programming interface.",
    "authors": "Robin Glauser; Jürgen Holm; Matthias Bender; Thomas Bürkle",
    "abstract": "Research into current robot middleware has revealed that most of them are either too complicated or outdated. These facts have motivated the development of a new middleware to meet the requirements of usability by non-experts. The proposed middleware is based on Android and is intended to be placed over existing robot SDKs and middleware. It runs on the android tablet of the Cruzr robot. Various toolings have been developed, such as a web component to control the robot via a webinterface, which facilitates its use. The middleware was developed using Android Java and runs on the Cruzr tablet as an app. It features a WebSocket server that interfaces with the robot and allows control via Python or other WebSocket-compatible languages. The speech interface utilizes Google Cloud Voice text-to-speech and speech-to-text services. The interface was implemented in Python, allowing for easy integration with existing robotics development workflows, and a web interface was developed for direct control of the robot via the web. The new robot middleware was created and deployed on a Cruzr robot, relying on the WebSocket API and featuring a Python implementation. It supports various robot functions, such as text-to-speech, speech-to-text, navigation, displaying content and scanning bar codes. The system's architecture allows for porting the interface to other robots and platforms, showcasing its adaptability. It has been demonstrated that the middleware can be run on a Pepper robot, although not all functions have been implemented yet. The middleware was utilized to implement healthcare use cases and received good feedback. Cloud and local speech services were discussed in regard to the middleware's needs, to run without having to change any code on other robots. An outlook on how the programming interface can further be simplified by using natural text to code generators has been/is given. For other researchers using the aforementioned platforms (Cruzr, Pepper), the new middleware can be utilized for testing human-robot interaction. It can be used in a teaching setting, as well as be adapted to other robots using the same interface and philosophy regarding simple methods. ",
    "doi": "10.1186/s12911-023-02210-7"
  },
  {
    "repository": "PubMed",
    "title": "ROMR: A ROS-based open-source mobile robot.",
    "authors": "Linus Nwankwo; Clemens Fritze; Konrad Bartsch; Elmar Rueckert",
    "abstract": "Currently, commercially available intelligent transport robots that are capable of carrying up to 90 kg of load can cost $5,000 or even more. This makes real-world experimentation prohibitively expensive and limits the applicability of such systems to everyday home or industrial tasks. Aside from their high cost, the majority of commercially available platforms are either closed-source, platform-specific or use difficult-to-customize hardware and firmware. In this work, we present a low-cost, open-source and modular alternative, referred to herein as \"\"ROS-based Open-source Mobile Robot (ROMR)\"\". ROMR utilizes off-the-shelf (OTS) components, additive manufacturing technologies, aluminium profiles, and a consumer hoverboard with high-torque brushless direct current (BLDC) motors. ROMR is fully compatible with the robot operating system (ROS), has a maximum payload of 90 kg, and costs less than $1500. Furthermore, ROMR offers a simple yet robust framework for contextualizing simultaneous localization and mapping (SLAM) algorithms, an essential prerequisite for autonomous robot navigation. The robustness and performance of the ROMR were validated through real-world and simulation experiments. All the design, construction and software files are freely available online under the GNU GPL v3 license at https://doi.org/10.17605/OSF.IO/K83X7. A descriptive video of ROMR can be found at https://osf.io/ku8ag. ",
    "doi": "10.1016/j.ohx.2023.e00426"
  },
  {
    "repository": "PubMed",
    "title": "ROS-Based Autonomous Navigation Robot Platform with Stepping Motor.",
    "authors": "Shengmin Zhao; Seung-Hoon Hwang",
    "abstract": "Indoor navigation robots, which have been developed using a robot operating system, typically use a direct current motor as a motion actuator. Their control algorithm is generally complex and requires the cooperation of sensors such as wheel encoders to correct errors. For this study, an autonomous navigation robot platform named Owlbot was designed, which is equipped with a stepping motor as a mobile actuator. In addition, a stepping motor control algorithm was developed using polynomial equations, which can effectively convert speed instructions to generate control signals for accurately operating the motor. Using 2D LiDAR and an inertial measurement unit as the primary sensors, simultaneous localization, mapping, and autonomous navigation are realised based on the particle filtering mapping algorithm. The experimental results show that Owlbot can effectively map the unknown environment and realise autonomous navigation through the proposed control algorithm, with a maximum movement error being smaller than 0.015 m. ",
    "doi": "10.3390/s23073648"
  },
  {
    "repository": "PubMed",
    "title": "LiDAR-Only Crop Navigation for Symmetrical Robot.",
    "authors": "Rémy Guyonneau; Franck Mercier; Gabriel Oliveira Freitas",
    "abstract": "This paper presents a navigation approach for autonomous agricultural robots based on LiDAR data. This navigation approach is divided into two parts: a line finding algorithm and a control algorithm. The paper proposes several line finding algorithms (based on PEARL/Ruby approach) that extract lines from a LiDAR data set. Once the lines have been processed from the data set, a control algorithm filters these lines and, using a fuzzy controller, generates the wheel speed commands to move the robot among the crop rows. This navigation approach was tested using a simulator built on ROS middle-ware and Gazebo (the source codes of the simulation are available on GitHub). The results of the simulated experiments show that the proposed approach performs well for a large range of crop configurations (with or without considering weeds, with or without holes in the crop rows…). ",
    "doi": "10.3390/s22228918"
  },
  {
    "repository": "PubMed",
    "title": "Kinematics Calibration and Validation Approach Using Indoor Positioning System for an Omnidirectional Mobile Robot.",
    "authors": "Alexandru-Tudor Popovici; Constantin-Catalin Dosoftei; Cristina Budaciu",
    "abstract": "Monitoring and tracking issues related to autonomous mobile robots are currently intensively debated in order to ensure a more fluent functionality in supply chain management. The interest arises from both theoretical and practical concerns about providing accurate information about the current and past position of systems involved in the logistics chain, based on specialized sensors and Global Positioning System (GPS). The localization demands are more challenging as the need to monitor the autonomous robot's ongoing activities is more stringent indoors and benefit from accurate motion response, which requires calibration. This practical research study proposes an extended calibration approach for improving Omnidirectional Mobile Robot (OMR) motion response in the context of mechanical build imperfections (misalignment). A precise indoor positioning system is required to obtain accurate data for calculating the calibration parameters and validating the implementation response. An ultrasound-based commercial solution was considered for tracking the OMR, but the practical observed errors of the readily available position solutions requires special processing of the raw acquired measurements. The approach uses a multilateration technique based on the point-to-point distances measured between the mobile ultrasound beacon and a current subset of fixed (reference) beacons, in order to obtain an improved position estimation characterized by a confidence coefficient. Therefore, the proposed method managed to reduce the motion error by up to seven-times. Reference trajectories were generated, and robot motion response accuracy was evaluated using a Robot Operating System (ROS) node developed in Matlab-Simulink that was wireless interconnected with the other ROS nodes hosted on the robot navigation controller. ",
    "doi": "10.3390/s22228590"
  },
  {
    "repository": "PubMed",
    "title": "Mobile robot path planning with reformative bat algorithm.",
    "authors": "Gongfeng Xin; Lei Shi; Guanxu Long; Weigang Pan; Yiming Li; Jicun Xu",
    "abstract": "Mobile robot path planning has attracted much attention as a key technology in robotics research. In this paper, a reformative bat algorithm (RBA) for mobile robot path planning is proposed, which is employed as the control mechanism of robots. The Doppler effect is applied to frequency update to ameliorate RBA. When the robot is in motion, the Doppler effect can be adaptively compensated to prevent the robot from prematurely converging. In the velocity update and position update, chaotic map and dynamic disturbance coefficient are introduced respectively to enrich the population diversity and weaken the limitation of local optimum. Furthermore, Q-learning is incorporated into RBA to reasonably choose the loudness attenuation coefficient and the pulse emission enhancement coefficient to reconcile the trade-off between exploration and exploitation, while improving the local search capability of RBA. The simulation experiments are carried out in two different environments, where the success rate of RBA is 93.33% and 90%, respectively. Moreover, in terms of the results of success rate, path length and number of iterations, RBA has better robustness and can plan the optimal path in a relatively short time compared with other algorithms in this field, thus illustrating its validity and reliability. Eventually, by the aid of the Robot Operating System (ROS), the experimental results of real-world robot navigation indicate that RBA has satisfactory real-time performance and path planning effect, which can be considered as a crucial choice for dealing with path planning problems. ",
    "doi": "10.1371/journal.pone.0276577"
  },
  {
    "repository": "PubMed",
    "title": "Automatically Annotated Dataset of a Ground Mobile Robot in Natural Environments via Gazebo Simulations.",
    "authors": "Manuel Sánchez; Jesús Morales; Jorge L Martínez; J J Fernández-Lozano; Alfonso García-Cerezo",
    "abstract": "This paper presents a new synthetic dataset obtained from Gazebo simulations of an Unmanned Ground Vehicle (UGV) moving on different natural environments. To this end, a Husky mobile robot equipped with a tridimensional (3D) Light Detection and Ranging (LiDAR) sensor, a stereo camera, a Global Navigation Satellite System (GNSS) receiver, an Inertial Measurement Unit (IMU) and wheel tachometers has followed several paths using the Robot Operating System (ROS). Both points from LiDAR scans and pixels from camera images, have been automatically labeled into their corresponding object class. For this purpose, unique reflectivity values and flat colors have been assigned to each object present in the modeled environments. As a result, a public dataset, which also includes 3D pose ground-truth, is provided as ROS bag files and as human-readable data. Potential applications include supervised learning and benchmarking for UGV navigation on natural environments. Moreover, to allow researchers to easily modify the dataset or to directly use the simulations, the required code has also been released. ",
    "doi": "10.3390/s22155599"
  },
  {
    "repository": "PubMed",
    "title": "Group Emotion Detection Based on Social Robot Perception.",
    "authors": "Marco Quiroz; Raquel Patiño; José Diaz-Amado; Yudith Cardinale",
    "abstract": "Social robotics is an emerging area that is becoming present in social spaces, by introducing autonomous social robots. Social robots offer services, perform tasks, and interact with people in such social environments, demanding more efficient and complex Human-Robot Interaction (HRI) designs. A strategy to improve HRI is to provide robots with the capacity of detecting the emotions of the people around them to plan a trajectory, modify their behaviour, and generate an appropriate interaction with people based on the analysed information. However, in social environments in which it is common to find a group of persons, new approaches are needed in order to make robots able to recognise groups of people and the emotion of the groups, which can be also associated with a scene in which the group is participating. Some existing studies are focused on detecting group cohesion and the recognition of group emotions; nevertheless, these works do not focus on performing the recognition tasks from a robocentric perspective, considering the sensory capacity of robots. In this context, a system to recognise scenes in terms of groups of people, to then detect global (prevailing) emotions in a scene, is presented. The approach proposed to visualise and recognise emotions in typical HRI is based on the face size of people recognised by the robot during its navigation (face sizes decrease when the robot moves away from a group of people). On each frame of the video stream of the visual sensor, individual emotions are recognised based on the Visual Geometry Group (VGG) neural network pre-trained to recognise faces (VGGFace); then, to detect the emotion of the frame, individual emotions are aggregated with a fusion method, and consequently, to detect global (prevalent) emotion in the scene (group of people), the emotions of its constituent frames are also aggregated. Additionally, this work proposes a strategy to create datasets with images/videos in order to validate the estimation of emotions in scenes and personal emotions. Both datasets are generated in a simulated environment based on the Robot Operating System (ROS) from videos captured by robots through their sensory capabilities. Tests are performed in two simulated environments in ROS/Gazebo: a museum and a cafeteria. Results show that the accuracy in the detection of individual emotions is 99.79% and the detection of group emotion (scene emotion) in each frame is 90.84% and 89.78% in the cafeteria and the museum scenarios, respectively. ",
    "doi": "10.3390/s22103749"
  },
  {
    "repository": "PubMed",
    "title": "A UWB-Based Lighter-Than-Air Indoor Robot for User-Centered Interactive Applications.",
    "authors": "Khawar Naheem; Ahmed Elsharkawy; Dongwoo Koo; Yundong Lee; Munsang Kim",
    "abstract": "Features such as safety and longer flight times render lighter-than-air robots strong candidates for indoor navigation applications involving people. However, the existing interactive mobility solutions using such robots lack the capability to follow a long-distance user in a relatively larger indoor space. At the same time, the tracking data delivered to these robots are sensitive to uncertainties in indoor environments such as varying intensities of light and electromagnetic field disturbances. Regarding the above shortcomings, we proposed an ultra-wideband (UWB)-based lighter-than-air indoor robot for user-centered interactive applications. We developed the data processing scheme over a robot operating system (ROS) framework to accommodate the robot's integration needs for a user-centered interactive application. In order to explore the user interaction with the robot at a long-distance, the dual interactions (i.e., user footprint following and user intention recognition) were proposed by equipping the user with a hand-held UWB sensor. Finally, experiments were conducted inside a professional arena to validate the robot's pose tracking in which 3D positioning was compared with the 3D laser sensor, and to reveal the applicability of the user-centered autonomous following of the robot according to the dual interactions. ",
    "doi": "10.3390/s22062093"
  },
  {
    "repository": "PubMed",
    "title": "Lio-A Personal Robot Assistant for Human-Robot Interaction and Care Applications.",
    "authors": "Justinas Miseikis; Pietro Caroni; Patricia Duchamp; Alina Gasser; Rastislav Marko; Nelija Miseikiene; Frederik Zwilling; Charles de Castelbajac; Lucas Eicher; Michael Fruh; Hansruedi Fruh",
    "abstract": "Lio is a mobile robot platform with a multi-functional arm explicitly designed for human-robot interaction and personal care assistant tasks. The robot has already been deployed in several health care facilities, where it is functioning autonomously, assisting staff and patients on an everyday basis. Lio is intrinsically safe by having full coverage in soft artificial-leather material as well as collision detection, limited speed and forces. Furthermore, the robot has a compliant motion controller. A combination of visual, audio, laser, ultrasound and mechanical sensors are used for safe navigation and environment understanding. The ROS-enabled setup allows researchers to access raw sensor data as well as have direct control of the robot. The friendly appearance of Lio has resulted in the robot being well accepted by health care staff and patients. Fully autonomous operation is made possible by a flexible decision engine, autonomous navigation and automatic recharging. Combined with time-scheduled task triggers, this allows Lio to operate throughout the day, with a battery life of up to 8 hours and recharging during idle times. A combination of powerful computing units provides enough processing power to deploy artificial intelligence and deep learning-based solutions on-board the robot without the need to send any sensitive data to cloud services, guaranteeing compliance with privacy requirements. During the COVID-19 pandemic, Lio was rapidly adjusted to perform additional functionality like disinfection and remote elevated body temperature detection. It complies with ISO13482 - Safety requirements for personal care robots, meaning it can be directly tested and deployed in care facilities. ",
    "doi": "10.1109/LRA.2020.3007462"
  },
  {
    "repository": "PubMed",
    "title": "The AMIRO Social Robotics Framework: Deployment and Evaluation on the Pepper Robot.",
    "authors": "Alexandra Ștefania Ghiță; Alexandru Florin Gavril; Mihai Nan; Bilal Hoteit; Imad Alex Awada; Alexandru Sorici; Irina Georgiana Mocanu; Adina Magda Florea",
    "abstract": "Recent studies in social robotics show that it can provide economic efficiency and growth in domains such as retail, entertainment, and active and assisted living (AAL). Recent work also highlights that users have the expectation of affordable social robotics platforms, providing focused and specific assistance in a robust manner. In this paper, we present the AMIRO social robotics framework, designed in a modular and robust way for assistive care scenarios. The framework includes robotic services for navigation, person detection and recognition, multi-lingual natural language interaction and dialogue management, as well as activity recognition and general behavior composition. We present AMIRO platform independent implementation based on a Robot Operating System (ROS). We focus on quantitative evaluations of each functionality module, providing discussions on their performance in different settings and the possible improvements. We showcase the deployment of the AMIRO framework on a popular social robotics platform-the Pepper robot-and present the experience of developing a complex user interaction scenario, employing all available functionality modules within AMIRO. ",
    "doi": "10.3390/s20247271"
  },
  {
    "repository": "PubMed",
    "title": "Multi-Sensor Orientation Tracking for a Façade-Cleaning Robot.",
    "authors": "Manuel Vega-Heredia; Ilyas Muhammad; Sriharsha Ghanta; Vengadesh Ayyalusami; Siti Aisyah; Mohan Rajesh Elara",
    "abstract": "Glass-façade-cleaning robots are an emerging class of service robots. This kind of cleaning robot is designed to operate on vertical surfaces, for which tracking the position and orientation becomes more challenging. In this article, we have presented a glass-façade-cleaning robot, Mantis v2, who can shift from one window panel to another like any other in the market. Due to the complexity of the panel shifting, we proposed and evaluated different methods for estimating its orientation using different kinds of sensors working together on the Robot Operating System (ROS). For this application, we used an onboard Inertial Measurement Unit (IMU), wheel encoders, a beacon-based system, Time-of-Flight (ToF) range sensors, and an external vision sensor (camera) for angular position estimation of the Mantis v2 robot. The external camera is used to monitor the robot's operation and to track the coordinates of two colored markers attached along the longitudinal axis of the robot to estimate its orientation angle. ToF lidar sensors are attached on both sides of the robot to detect the window frame. ToF sensors are used for calculating the distance to the window frame; differences between beam readings are used to calculate the orientation angle of the robot. Differential drive wheel encoder data are used to estimate the robot's heading angle on a 2D façade surface. An integrated heading angle estimation is also provided by using simple fusion techniques, i.e., a complementary filter (CF) and 1D Kalman filter (KF) utilizing the IMU sensor's raw data. The heading angle information provided by different sensory systems is then evaluated in static and dynamic tests against an off-the-shelf attitude and heading reference system (AHRS). It is observed that ToF sensors work effectively from 0 to 30 degrees, beacons have a delay up to five seconds, and the odometry error increases according to the navigation distance due to slippage and/or sliding on the glass. Among all tested orientation sensors and methods, the vision sensor scheme proved to be better, with an orientation angle error of less than 0.8 degrees for this application. The experimental results demonstrate the efficacy of our proposed techniques in this orientation tracking, which has never applied in this specific application of cleaning robots. ",
    "doi": "10.3390/s20051483"
  },
  {
    "repository": "PubMed",
    "title": "Automatic Waypoint Generation to Improve Robot Navigation Through Narrow Spaces.",
    "authors": "Francisco-Angel Moreno; Javier Monroy; Jose-Raul Ruiz-Sarmiento; Cipriano Galindo; Javier Gonzalez-Jimenez",
    "abstract": "In domestic robotics, passing through narrow areas becomes critical for safe and effective robot navigation. Due to factors like sensor noise or miscalibration, even if the free space is sufficient for the robot to pass through, it may not see enough clearance to navigate, hence limiting its operational space. An approach to facing this is to insert waypoints strategically placed within the problematic areas in the map, which are considered by the robot planner when generating a trajectory and help to successfully traverse them. This is typically carried out by a human operator either by relying on their experience or by trial-and-error. In this paper, we present an automatic procedure to perform this task that: (i) detects problematic areas in the map and (ii) generates a set of auxiliary navigation waypoints from which more suitable trajectories can be generated by the robot planner. Our proposal, fully compatible with the robotic operating system (ROS), has been successfully applied to robots deployed in different houses within the H2020 MoveCare project. Moreover, we have performed extensive simulations with four state-of-the-art robots operating within real maps. The results reveal significant improvements in the number of successful navigations for the evaluated scenarios, demonstrating its efficacy in realistic situations. ",
    "doi": "10.3390/s20010240"
  },
  {
    "repository": "PubMed",
    "title": "Stability Control and Turning Algorithm of an Alpine Skiing Robot.",
    "authors": "Si-Hyun Kim; Bumjoo Lee; Young-Dae Hong",
    "abstract": "This paper proposes a general stability control method that uses the concept of zero-moment-point (ZMP) and a turning algorithm with a light detection and ranging (LiDAR) sensor for a bipedal alpine skiing robot. There is no elaborate simulator for skiing robots since the snow has complicated characteristics, such as compression and melting. However, real experiments are laborious because of the many varied skiing conditions. The proposed skiing simulator could be used, so that a humanoid robot can track its desired turning radius by modeled forces that are similar to real ones in the snow. Subsequently, the robot will be able to pass through gates with LiDAR sensors. By using ZMP control, the robot can avoid falling down while tracking its desired path. The performance of the proposed stabilization method and autonomous turning algorithm are verified by a dynamics simulation software, Webots, and the simulation results are obtained while using the small humanoid robot platform DARwIn-OP. ",
    "doi": "10.3390/s19173664"
  },
  {
    "repository": "PubMed",
    "title": "Sensor Fusion Based Model for Collision Free Mobile Robot Navigation.",
    "authors": "Marwah Almasri; Khaled Elleithy; Abrar Alajlan",
    "abstract": "Autonomous mobile robots have become a very popular and interesting topic in the last decade. Each of them are equipped with various types of sensors such as GPS, camera, infrared and ultrasonic sensors. These sensors are used to observe the surrounding environment. However, these sensors sometimes fail and have inaccurate readings. Therefore, the integration of sensor fusion will help to solve this dilemma and enhance the overall performance. This paper presents a collision free mobile robot navigation based on the fuzzy logic fusion model. Eight distance sensors and a range finder camera are used for the collision avoidance approach where three ground sensors are used for the line or path following approach. The fuzzy system is composed of nine inputs which are the eight distance sensors and the camera, two outputs which are the left and right velocities of the mobile robot's wheels, and 24 fuzzy rules for the robot's movement. Webots Pro simulator is used for modeling the environment and the robot. The proposed methodology, which includes the collision avoidance based on fuzzy logic fusion model and line following robot, has been implemented and tested through simulation and real time experiments. Various scenarios have been presented with static and dynamic obstacles using one robot and two robots while avoiding obstacles in different shapes and sizes. ",
    "doi": "10.3390/s16010024"
  }
]
