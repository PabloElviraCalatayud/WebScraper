[
  {
    "repository": "PubMed",
    "title": "Meta-Learning for Fast Adaptation in Intent Inferral on a Robotic Hand Orthosis for Stroke.",
    "authors": "Pedro Leandro La Rotta; Jingxi Xu; Ava Chen; Lauren Winterbottom; Wenxi Chen; Dawn Nilsen; Joel Stein; Matei Ciocarlie",
    "abstract": "We propose MetaEMG, a meta-learning approach for fast adaptation in intent inferral on a robotic hand orthosis for stroke. One key challenge in machine learning for assistive and rehabilitative robotics with disabled-bodied subjects is the difficulty of collecting labeled training data. Muscle tone and spasticity often vary significantly among stroke subjects, and hand function can even change across different use sessions of the device for the same subject. We investigate the use of meta-learning to mitigate the burden of data collection needed to adapt high-capacity neural networks to a new session or subject. Our experiments on real clinical data collected from five stroke subjects show that MetaEMG can improve the intent inferral accuracy with a small session- or subject-specific dataset and very few fine-tuning epochs. To the best of our knowledge, we are the first to formulate intent inferral on stroke subjects as a meta-learning problem and demonstrate fast adaptation to a new session or subject for controlling a robotic hand orthosis with EMG signals. ",
    "doi": "10.1109/iros58592.2024.10801596"
  },
  {
    "repository": "PubMed",
    "title": "Deep Learning-Based Subject Independent Human Activity Recognition using Smart Lacelock Data.",
    "authors": "Najmeh Movahhed Neya; Edward Sazonov; Xiangrong Shen",
    "abstract": "Human Activity Recognition (HAR) field is rapidly growing and the classification of human activities based on sensor data is crucial for applications in healthcare, rehabilitation and numerous other sectors. In this paper we use a novel device and attempt Deep Learning-based HAR from the device data.Typically, sensor-based HAR tasks use data from accelerometer and gyroscope within Inertial Measurement Units (IMU). But in this work we use the data from Smart Lacelock device, which is home to IMU and loadcell, introducing an additional sensor, aimed at complementing IMU data. This novel device ensures user comfort by attaching to the user's shoe as a shoelace tensioning device without any shoe modification. The data for this study was collected by the UA HuB-Robotics Lab from eight participants.Using this comprehensive dataset, we propose a CNN based model to classify activities such as walking, stair climbing, and stair descending. The model comprises three consecutive CNN blocks, and within each block there is a convolutional layer, a max-pooling layer, a Rectified Linear Unit (ReLU) layer, and a normalization layer. The model has a dropout and a flatten layer right after the third block of CNN and concludes with 2 dense layers. Our model achieves an average recognition accuracy of 98.4% using the leave-one-out (L1O) technique.In this work Smart Lacelock device demonstrated feasibility in recognition of a set of human activities and the results support further investigation of its applications in HAR. ",
    "doi": "10.1109/EMBC53108.2024.10781739"
  },
  {
    "repository": "PubMed",
    "title": "Adaptive formation learning control for cooperative AUVs under complete uncertainty.",
    "authors": "Emadodin Jandaghi; Mingxi Zhou; Paolo Stegagno; Chengzhi Yuan",
    "abstract": "This paper addresses the critical need for adaptive formation control in Autonomous Underwater Vehicles (AUVs) without requiring knowledge of system dynamics or environmental data. Current methods, often assuming partial knowledge like known mass matrices, limit adaptability in varied settings. We proposed two-layer framework treats all system dynamics, including the mass matrix, as entirely unknown, achieving configuration-agnostic control applicable to multiple underwater scenarios. The first layer features a cooperative estimator for inter-agent communication independent of global data, while the second employs a decentralized deterministic learning (DDL) controller using local feedback for precise trajectory control. The framework's radial basis function neural networks (RBFNN) store dynamic information, eliminating the need for relearning after system restarts. This robust approach addresses uncertainties from unknown parametric values and unmodeled interactions internally, as well as external disturbances such as varying water currents and pressures, enhancing adaptability across diverse environments. Comprehensive and rigorous mathematical proofs are provided to confirm the stability of the proposed controller, while simulation results validate each agent's control accuracy and signal boundedness, confirming the framework's stability and resilience in complex scenarios. ",
    "doi": "10.3389/frobt.2024.1491907"
  },
  {
    "repository": "PubMed",
    "title": "Does the transfer of knowledge from the pioneer generation to the second-generation speed-up the learning curve of robot-assisted partial nephrectomies? TRANSFER trial (UroCCR n°83).",
    "authors": "Louis Vignot; Zine-Eddine Khene; Adil Mellouki; Arnoult Morrone; Jean-Christophe Bernhard; Karim Bensalah; Daniel Chevallier; Nicolas Doumerc; Morgan Roupret; Francois-Xavier Nouhaud; Cédric Lebacle; Jean-Alexandre Long; Pierre Pillot; Xavier Tillou; Brannwel Tibi; Matthieu Durand; Younes Ahallal; Imad Bentellis",
    "abstract": "The objective is to compare the learning curves between two pioneer and three second-generation surgeons for RAPN in terms of WIT, CD and positive surgical margins. The charts of consecutive RAPNs of three centres were reviewed from the UroCCR prospective database. The experience was assessed by a regression model for each group. There was a univariate analysis on three consecutive sequences of 15 procedures. The learning speed for WIT was explored graphically by polynomial regression after cubic splines. Finally, CUSUM charts were obtained. There were 1203 RAPN in the pioneer group and 119 performed by second-generation surgeons. There was a significant difference in the distribution of tumour size (p < 0.001) and the RENAL score (p < 0.001). The operative time was longer in the first group (p > 0.001). Independent factors for a higher WIT were the second group (p < 0.001), higher experience (p < 0.001) the collinearity between the group and experience (p < 0.001), the RENAL score (p < 0.001) and blood loss (p < 0.001). Adjusted Loess regressions showed a plateau of WIT at 400 procedures for the pioneers and a significant decrease at 20 procedures for the second generation. CUSUM chart analysis showed a 'staircase' pattern of the learning process, with three major steps at 150, 200 and 300 procedures. The major limitation is the difference in sample size between the two arms. Learning curve patterns would reflect a transfer of knowledge to the second-generation, as opposed to the establishment of standards by the pioneers. ",
    "doi": "10.1002/bco2.477"
  },
  {
    "repository": "PubMed",
    "title": "RL-QPSO net: deep reinforcement learning-enhanced QPSO for efficient mobile robot path planning.",
    "authors": "Yang Jing; Li Weiya",
    "abstract": "Path planning in complex and dynamic environments poses a significant challenge in the field of mobile robotics. Traditional path planning methods such as genetic algorithms, Dijkstra's algorithm, and Floyd's algorithm typically rely on deterministic search strategies, which can lead to local optima and lack global search capabilities in dynamic settings. These methods have high computational costs and are not efficient for real-time applications. To address these issues, this paper presents a Quantum-behaved Particle Swarm Optimization model enhanced by deep reinforcement learning (RL-QPSO Net) aimed at improving global optimality and adaptability in path planning. The RL-QPSO Net combines quantum-inspired particle swarm optimization (QPSO) and deep reinforcement learning (DRL) modules through a dual control mechanism to achieve path optimization and environmental adaptation. The QPSO module is responsible for global path optimization, using quantum mechanics to avoid local optima, while the DRL module adjusts strategies in real-time based on environmental feedback, thus enhancing decision-making capabilities in complex high-dimensional scenarios. Experiments were conducted on multiple datasets, including Cityscapes, NYU Depth V2, Mapillary Vistas, and ApolloScape, and the results showed that RL-QPSO Net outperforms traditional methods in terms of accuracy, computational efficiency, and model complexity. This method demonstrated significant improvements in accuracy and computational efficiency, providing an effective path planning solution for real-time applications in complex environments for mobile robots. In the future, this method could be further extended to resource-limited environments to achieve broader practical applications. ",
    "doi": "10.3389/fnbot.2024.1464572"
  },
  {
    "repository": "PubMed",
    "title": "The Transformative Impact of AI, Extended Reality, and Robotics in Interventional Radiology: Current Trends and Applications.",
    "authors": "Katelyn Vlastaris; Annabelle Alrez; Samantha Friedland; Antonina Randazzo; Rayan Abboud; Charles Martin",
    "abstract": "Interventional Radiology is at the forefront of integrating advanced imaging techniques and minimally-invasive procedures to enhance patient care. The advent of Digital Health Technologies (DHTs), including artificial intelligence (AI), robotics, and extended reality (XR), is revolutionizing healthcare, particularly in IR due to its reliance on innovative technology and advanced imaging. Since 2016, the proportion of these DHT-related publications in IR has consistently increased. The proportion of AI-related studies published in IR was 69% higher than in surgery, XR-related studies were 94% higher, and robotics studies were 192% higher, indicating a more rapid growth rate in IR compared to surgery. This article explores the transformative impact of these technologies on IR, emphasizing their potential to enhance precision, efficiency, and patient outcomes. Despite the promising advancements, there is a lack of standardization and clinical consensus on the optimal use of DHTs in IR. The variability in IR procedures and imaging systems across hospitals complicates the standardization of workflows and comparison of studies. This underscores the importance of integrating DHTs as aids to IR practitioners rather than replacement, ensuring that these technologies enhance both clinical and procedural practice. ",
    "doi": "10.1016/j.tvir.2024.101003"
  },
  {
    "repository": "PubMed",
    "title": "CabbageNet: Deep Learning for High-Precision Cabbage Segmentation in Complex Settings for Autonomous Harvesting Robotics.",
    "authors": "Yongqiang Tian; Xinyu Cao; Taihong Zhang; Huarui Wu; Chunjiang Zhao; Yunjie Zhao",
    "abstract": "Reducing damage and missed harvest rates is essential for improving efficiency in unmanned cabbage harvesting. Accurate real-time segmentation of cabbage heads can significantly alleviate these issues and enhance overall harvesting performance. However, the complexity of the growing environment and the morphological variability of field-grown cabbage present major challenges to achieving precise segmentation. This study proposes an improved YOLOv8n-seg network to address these challenges effectively. Key improvements include modifying the baseline model's final C2f module and integrating deformable attention with dynamic sampling points to enhance segmentation performance. Additionally, an ADown module minimizes detail loss from excessive downsampling by using depthwise separable convolutions to reduce parameter count and computational load. To improve the detection of small cabbage heads, a Small Object Enhance Pyramid based on the PAFPN architecture is introduced, significantly boosting performance for small targets. The experimental results show that the proposed model achieves a Mask Precision of 92.2%, Mask Recall of 87.2%, and Mask mAP50 of 95.1%, while maintaining a compact model size of only 6.46 MB. These metrics indicate superior accuracy and efficiency over mainstream instance segmentation models, facilitating real-time, precise cabbage harvesting in complex environments. ",
    "doi": "10.3390/s24248115"
  },
  {
    "repository": "PubMed",
    "title": "Editorial: Vision, learning, and robotics: AI for plants in the 2020s.",
    "authors": "Zhenghong Yu; Luca Iocchi; Jeffrey Too Chuan Tan; Huabing Zhou; Changcai Yang; Hao Lu",
    "abstract": "",
    "doi": "10.3389/fpls.2024.1539626"
  },
  {
    "repository": "PubMed",
    "title": "Trajectory Tracking Control for Robotic Manipulator Based on Soft Actor-Critic and Generative Adversarial Imitation Learning.",
    "authors": "Jintao Hu; Fujie Wang; Xing Li; Yi Qin; Fang Guo; Ming Jiang",
    "abstract": "In this paper, a deep reinforcement learning (DRL) approach based on generative adversarial imitation learning (GAIL) and long short-term memory (LSTM) is proposed to resolve tracking control problems for robotic manipulators with saturation constraints and random disturbances, without learning the dynamic and kinematic model of the manipulator. Specifically, it limits the torque and joint angle to a certain range. Firstly, in order to cope with the instability problem during training and obtain a stability policy, soft actor-critic (SAC) and LSTM are combined. The changing trends of joint position over time are more comprehensively captured and understood by employing an LSTM architecture designed for robotic manipulator systems, thereby reducing instability during the training of robotic manipulators for tracking control tasks. Secondly, the obtained policy by SAC-LSTM is used as expert data for GAIL to learn a better control policy. This SAC-LSTM-GAIL (SL-GAIL) algorithm does not need to spend time exploring unknown environments and directly learns the control strategy from stable expert data. Finally, it is demonstrated by the simulation results that the end effector of the robot tracking task is effectively accomplished by the proposed SL-GAIL algorithm, and more superior stability is exhibited in a test environment with interference compared with other algorithms. ",
    "doi": "10.3390/biomimetics9120779"
  },
  {
    "repository": "PubMed",
    "title": "Predictive Capacities of a Machine Learning Decision Tree Model Created to Analyse Feasibility of an Open or Robotic Kidney Transplant.",
    "authors": "Alessandro Martinino; Ojus Khanolkar; Erdem Koyuncu; Egor Petrochenkov; Giulia Bencini; Joanna Olazar; Pierpaolo Di Cocco; Jorge Almario-Alvarez; Mario Spaggiari; Enrico Benedetti; Ivo Tzvetanov",
    "abstract": "Machine learning has emerged as a potent tool in healthcare. A decision tree model was built to improve the decision-making process when determining the optimal choice between an open or robotic surgical approach for kidney transplant. 822 patients (OKT) and 169 (RKT) underwent kidney transplantation at our centre during the study period. A decision tree model was built in a two-step process consisting of: (1) Creating the model on the training data and (2) testing the predictive capabilities of the model using the test data. Our model correctly predicted an OKT in 148 patients out of 161 test cases who received an OKT (accuracy 91%) and predicted an RKT in 19 out of 25 test cases of patients receiving an RKT (accuracy 76%). Our model represents the inaugural data-driven model that furnishes concrete insights for the discernment between employing robotic and open surgery techniques. ",
    "doi": "10.1002/rcs.70035"
  },
  {
    "repository": "PubMed",
    "title": "Optimized inverse kinematics modeling and joint angle prediction for six-degree-of-freedom anthropomorphic robots with Explainable AI.",
    "authors": "Rakesh Chandra Joshi; Jaynendra Kumar Rai; Radim Burget; Malay Kishore Dutta",
    "abstract": "Inverse kinematics, crucial in robotics, involves computing joint configurations to achieve specific end-effector positions and orientations. This task is particularly complex for six-degree-of-freedom (six-DoF) anthropomorphic robots due to complicated mathematical equations, nonlinear behaviours, multiple valid solutions, physical constraints, non-generalizability and computational demands. The primary contribution of this work is to address the complex inverse kinematics problem for six-DoF anthropomorphic robots through the systematic exploration of AI models. This study involves rigorous evaluation and Bayesian optimization for hyperparameter tuning to identify the optimal regressor, balancing both accuracy and computational efficiency. Utilizing five-fold cross-validation on a publicly available dataset, the selected model demonstrates exceptional performance in predicting six joint angles for end effector configuration, yielding an average mean square error of 1.934 × 10-3 to 3.522 × 10-3. Its computational efficiency, with a prediction time of approximately 1.25 ms per sample, makes it a practical choice. Additionally, the study employs Explainable AI, using SHAP (SHapley Additive exPlanations) analysis to gain an understanding of feature importance. This analysis not only enhances model interpretability but also reaffirms the efficacy in this challenging multi-input multi-output predictive task. This research advances state-of-the-art models and neural networks by prioritizing computational efficiency alongside accuracy-a critical yet often overlooked factor. Pioneering a significant advancement in anthropomorphic robot kinematics, it balances accuracy and efficiency, offering practical robotic automation solutions. ",
    "doi": "10.1016/j.isatra.2024.12.008"
  },
  {
    "repository": "PubMed",
    "title": "AI technology to support adaptive functioning in neurodevelopmental conditions in everyday environments: a systematic review.",
    "authors": "Nina Perry; Carter Sun; Martha Munro; Kelsie A Boulton; Adam J Guastella",
    "abstract": "Supports for adaptive functioning in individuals with neurodevelopmental conditions (NDCs) is of umost importance to long-term outcomes. Artificial intelligence (AI)-assistive technologies has enormous potential to offer efficient, cost-effective, and personalized solutions to address these challenges, particularly in everday environments. This systematic review examines the existing evidence for using AI-assistive technologies to support adaptive functioning in people with NDCs in everyday settings. Searches across six databases yielded 15 studies meeting inclusion criteria, focusing on robotics, phones/computers and virtual reality. Studies most frequently recruited children diagnosed with autism and targeted social skills (47%), daily living skills (26%), and communication (16%). Despite promising results, studies addressing broader transdiagnostic needs across different NDC populations are needed. There is also an urgent need to improve the quality of evidence-based research practices. This review concludes that AI holds enormous potential to support adaptive functioning for people with NDCs and for personalized health support. This review underscores the need for further research studies to advance AI technologies in this field. ",
    "doi": "10.1038/s41746-024-01355-7"
  },
  {
    "repository": "PubMed",
    "title": "Benefits and harms associated with the use of AI-related algorithmic decision-making systems by healthcare professionals: a systematic review.",
    "authors": "Christoph Wilhelm; Anke Steckelberg; Felix G Rebitschek",
    "abstract": "Despite notable advancements in artificial intelligence (AI) that enable complex systems to perform certain tasks more accurately than medical experts, the impact on patient-relevant outcomes remains uncertain. To address this gap, this systematic review assesses the benefits and harms associated with AI-related algorithmic decision-making (ADM) systems used by healthcare professionals, compared to standard care. In accordance with the PRISMA guidelines, we included interventional and observational studies published as peer-reviewed full-text articles that met the following criteria: human patients; interventions involving algorithmic decision-making systems, developed with and/or utilizing machine learning (ML); and outcomes describing patient-relevant benefits and harms that directly affect health and quality of life, such as mortality and morbidity. Studies that did not undergo preregistration, lacked a standard-of-care control, or pertained to systems that assist in the execution of actions (e.g., in robotics) were excluded. We searched MEDLINE, EMBASE, IEEE Xplore, and Google Scholar for studies published in the past decade up to 31 March 2024. We assessed risk of bias using Cochrane's RoB 2 and ROBINS-I tools, and reporting transparency with CONSORT-AI and TRIPOD-AI. Two researchers independently managed the processes and resolved conflicts through discussion. This review has been registered with PROSPERO (CRD42023412156) and the study protocol has been published. Out of 2,582 records identified after deduplication, 18 randomized controlled trials (RCTs) and one cohort study met the inclusion criteria, covering specialties such as psychiatry, oncology, and internal medicine. Collectively, the studies included a median of 243 patients (IQR 124-828), with a median of 50.5% female participants (range 12.5-79.0, IQR 43.6-53.6) across intervention and control groups. Four studies were classified as having low risk of bias, seven showed some concerns, and another seven were assessed as having high or serious risk of bias. Reporting transparency varied considerably: six studies showed high compliance, four moderate, and five low compliance with CONSORT-AI or TRIPOD-AI. Twelve studies (63%) reported patient-relevant benefits. Of those with low risk of bias, interventions reduced length of stay in hospital and intensive care unit (10.3 vs. 13.0 days, p = 0.042; 6.3 vs. 8.4 days, p = 0.030), in-hospital mortality (9.0% vs. 21.3%, p = 0.018), and depression symptoms in non-complex cases (45.1% vs. 52.3%, p = 0.03). However, harms were frequently underreported, with only eight studies (42%) documenting adverse events. No study reported an increase in adverse events as a result of the interventions. The current evidence on AI-related ADM systems provides limited insights into patient-relevant outcomes. Our findings underscore the essential need for rigorous evaluations of clinical benefits, reinforced compliance with methodological standards, and balanced consideration of both benefits and harms to ensure meaningful integration into healthcare practice. This study did not receive any funding. ",
    "doi": "10.1016/j.lanepe.2024.101145"
  },
  {
    "repository": "PubMed",
    "title": "Green ILC: A Novel Energy-Efficient Iterative Learning Control Approach.",
    "authors": "Yu Dou; Emmanuel Prempain",
    "abstract": "In this paper, we introduce Green Iterative Learning Control (Green ILC), an innovative hybrid control method that addresses the critical need for energy-efficient control in dynamic, repetitive-task environments. By integrating the iterative refinement capabilities of traditional Iterative Learning Control (ILC) with the optimization strengths of gradient descent, Green ILC achieves a balanced trade-off between tracking accuracy and energy consumption. This novel approach introduces a cost function that minimizes both tracking errors and control effort, enabling the system to adaptively optimize performance over iterations. Theoretical analysis and simulation results demonstrate that Green ILC not only achieves faster convergence but also provides significant energy savings compared with traditional ILC methods. Notably, Green ILC reduces energy consumption by prioritizing efficiency, making it particularly suitable for energy-intensive applications such as robotics, manufacturing, and process control. While a slight decrease in tracking accuracy is observed, this trade-off is acceptable for scenarios where energy efficiency is paramount. This work establishes Green ILC as a promising solution for modern industrial systems requiring robust and sustainable control strategies. ",
    "doi": "10.3390/s24237787"
  },
  {
    "repository": "PubMed",
    "title": "Drivable path detection for a mobile robot with differential drive using a deep Learning based segmentation method for indoor navigation.",
    "authors": "Oğuz Mısır",
    "abstract": "The integration of artificial intelligence into the field of robotics enables robots to perform their tasks more meaningfully. In particular, deep-learning methods contribute significantly to robots becoming intelligent cybernetic systems. The effective use of deep-learning mobile cyber-physical systems has enabled mobile robots to become more intelligent. This effective use of deep learning can also help mobile robots determine a safe path. The drivable pathfinding problem involves a mobile robot finding the path to a target in a challenging environment with obstacles. In this paper, a semantic-segmentation-based drivable path detection method is presented for use in the indoor navigation of mobile robots. The proposed method uses a perspective transformation strategy based on transforming high-accuracy segmented images into real-world space. This transformation enables the motion space to be divided into grids, based on the image perceived in a real-world space. A grid-based RRT* navigation strategy was developed that uses images divided into grids to enable the mobile robot to avoid obstacles and meet the optimal path requirements. Smoothing was performed to improve the path planning of the grid-based RRT* and avoid unnecessary turning angles of the mobile robot. Thus, the mobile robot could reach the target in an optimum manner in the drivable area determined by segmentation. Deeplabv3+ and ResNet50 backbone architecture with superior segmentation ability are proposed for accurate determination of drivable path. Gaussian filter was used to reduce the noise caused by segmentation. In addition, multi-otsu thresholding was used to improve the masked images in multiple classes. The segmentation model and backbone architecture were compared in terms of their performance using different methods. DeepLabv3+ and ResNet50 backbone architectures outperformed the other compared methods by 0.21%-4.18% on many metrics. In addition, a mobile robot design is presented to test the proposed drivable path determination method. This design validates the proposed method by using different scenarios in an indoor environment. ",
    "doi": "10.7717/peerj-cs.2514"
  },
  {
    "repository": "PubMed",
    "title": "Exploring diabetes through the lens of AI and computer vision: Methods and future prospects.",
    "authors": "Ramesh Chundi; Sasikala G; Praveen Kumar Basivi; Anitha Tippana; Vishwanath R Hulipalled; Prabakaran N; Jay B Simha; Chang Woo Kim; Vijay Kakani; Visweswara Rao Pasupuleti",
    "abstract": "Early diagnosis and timely initiation of treatment plans for diabetes are crucial for ensuring individuals' well-being. Emerging technologies like artificial intelligence (AI) and computer vision are highly regarded for their ability to enhance the accessibility of large datasets for dynamic training and deliver efficient real-time intelligent technologies and predictable models. The application of AI and computer vision techniques to enhance the analysis of clinical data is referred to as eHealth solutions that employ advanced approaches to aid medical applications. This study examines several advancements and applications of machine learning, deep learning, and machine vision in global perception, with a focus on sustainability. This article discusses the significance of utilizing artificial intelligence and computer vision to detect diabetes, as it has the potential to significantly mitigate harm to human life. This paper provides several comments addressing challenges and recommendations for the use of this technology in the field of diabetes. This study explores the potential of employing Industry 4.0 technologies, including machine learning, deep learning, and computer vision robotics, as effective tools for effectively dealing with diabetes related aspects. ",
    "doi": "10.1016/j.compbiomed.2024.109537"
  },
  {
    "repository": "PubMed",
    "title": "Editorial: Autonomous (re)production, learning and bio-inspired robotics workshop.",
    "authors": "Andy M Tyrrell; Emma Hart; Alan Winfield; A E Eiben; Jon Timmis",
    "abstract": "",
    "doi": "10.3389/frobt.2024.1513495"
  },
  {
    "repository": "PubMed",
    "title": "AI-Powered Multimodal Modeling of Personalized Hemodynamics in Aortic Stenosis.",
    "authors": "Caglar Ozturk; Daniel H Pak; Luca Rosalia; Debkalpa Goswami; Mary E Robakowski; Raymond McKay; Christopher T Nguyen; James S Duncan; Ellen T Roche",
    "abstract": "Aortic stenosis (AS) is the most common valvular heart disease in developed countries. High-fidelity preclinical models can improve AS management by enabling therapeutic innovation, early diagnosis, and tailored treatment planning. However, their use is currently limited by complex workflows necessitating lengthy expert-driven manual operations. Here, we propose an AI-powered computational framework for accelerated and democratized patient-specific modeling of AS hemodynamics from computed tomography (CT). First, we demonstrate that the automated meshing algorithms can generate task-ready geometries for both computational and benchtop simulations with higher accuracy and 100 times faster than existing approaches. Then, we show that the approach can be integrated with fluid-structure interaction and soft robotics models to accurately recapitulate a broad spectrum of clinical hemodynamic measurements of diverse AS patients. The efficiency and reliability of these algorithms make them an ideal complementary tool for personalized high-fidelity modeling of AS biomechanics, hemodynamics, and treatment planning. ",
    "doi": "10.1002/advs.202404755"
  },
  {
    "repository": "PubMed",
    "title": "Editorial: Human-robot collaboration in Industry 5.0: a human-centric AI-based approach.",
    "authors": "Loris Roveda",
    "abstract": "",
    "doi": "10.3389/frobt.2024.1511126"
  },
  {
    "repository": "PubMed",
    "title": "A novel dataset of annotated oyster mushroom images with environmental context for machine learning applications.",
    "authors": "Sonay Duman; Abdullah Elewi; Abdulsalam Hajhamed; Rasheed Khankan; Amina Souag; Asma Ahmed",
    "abstract": "State-of-the-art technologies such as computer vision and machine learning, are revolutionizing the smart mushroom industry by addressing diverse challenges in yield prediction, growth analysis, mushroom classification, disease and deformation detection, and digital twinning. However, mushrooms have long presented a challenge to automated systems due to their varied sizes, shapes, and surface characteristics, limiting the effectiveness of technologies aimed at mushroom classification and growth analysis. Clean and well-labelled datasets are therefore a cornerstone for developing efficient machine-learning models. Bridging this gap in oyster mushroom cultivation, we present a novel dataset comprising 555 high-quality camera raw images, from which approximately 16.000 manually annotated images were extracted. These images capture mushrooms in various shapes, maturity stages, and conditions, photographed in a greenhouse using two cameras for comprehensive coverage. Alongside the images, we recorded key environmental parameters within the mushroom greenhouse, such as temperature, relative humidity, moisture, and air quality, for a holistic analysis. This dataset is unique in providing both visual and environmental time-point data, organized into four storage folders: \"\"Raw Images\"\"; \"\"Mushroom Labelled Images and Annotation Files\"\"; \"\"Maturity Labelled Images and Annotation Files\"\"; and \"\"Sensor Data\"",
    "doi": " which includes time-stamped sensor readings in Excel files. This dataset can enable researchers to develop high-quality prediction and classification machine learning models for the intelligent cultivation of oyster mushrooms. Beyond mushroom cultivation"
  },
  {
    "repository": "PubMed",
    "title": "Integrating Historical Learning and Multi-View Attention with Hierarchical Feature Fusion for Robotic Manipulation.",
    "authors": "Gaoxiong Lu; Zeyu Yan; Jianing Luo; Wei Li",
    "abstract": "Humans typically make decisions based on past experiences and observations, while in the field of robotic manipulation, the robot's action prediction often relies solely on current observations, which tends to make robots overlook environmental changes or become ineffective when current observations are suboptimal. To address this pivotal challenge in robotics, inspired by human cognitive processes, we propose our method which integrates historical learning and multi-view attention to improve the performance of robotic manipulation. Based on a spatio-temporal attention mechanism, our method not only combines observations from current and past steps but also integrates historical actions to better perceive changes in robots' behaviours and their impacts on the environment. We also employ a mutual information-based multi-view attention module to automatically focus on valuable perspectives, thereby incorporating more effective information for decision-making. Furthermore, inspired by human visual system which processes both global context and local texture details, we have devised a method that merges semantic and texture features, aiding robots in understanding the task and enhancing their capability to handle fine-grained tasks. Extensive experiments in RLBench and real-world scenarios demonstrate that our method effectively handles various tasks and exhibits notable robustness and adaptability. ",
    "doi": "10.3390/biomimetics9110712"
  },
  {
    "repository": "PubMed",
    "title": "Compassionate Care with Autonomous AI Humanoid Robots in Future Healthcare Delivery: A Multisensory Simulation of Next-Generation Models.",
    "authors": "Joannes Paulus Tolentino Hernandez",
    "abstract": "The integration of AI and robotics in healthcare raises concerns, and additional issues regarding autonomous systems are anticipated. Effective communication is crucial for robots to be seen as \"\"caring\"",
    "doi": " necessitating advanced mechatronic design and natural language processing (NLP). This paper examines the potential of humanoid robots to autonomously replicate compassionate care. The study employs computational simulations using mathematical and agent-based modeling to analyze human-robot interactions (HRIs) surpassing Tetsuya Tanioka's TRETON. It incorporates stochastic elements (through neuromorphic computing) and quantum-inspired concepts (through the lens of Martha Rogers' theory)"
  },
  {
    "repository": "PubMed",
    "title": "Analysis of Gender Issues in Computational Thinking Approach in Science and Mathematics Learning in Higher Education.",
    "authors": "Alejandro De la Hoz Serrano; Lina Viviana Melo Niño; Andrés Álvarez Murillo; Miguel Ángel Martín Tardío; Florentina Cañada Cañada; Javier Cubero Juánez",
    "abstract": "In the contemporary era, Computational Thinking has emerged as a crucial skill for individuals to possess in order to thrive in the 21st century. In this context, there is a need to develop a methodology for cultivating these skills within a science and mathematics content education framework, particularly among pre-service teachers. This study aimed to investigate the impact of Educational Robotics on the development of Computational Thinking skills, with a particular focus on the role of gender, through a scientific and mathematical content teaching approach. A pre-experimental design with a quantitative approach was employed, and it was implemented with a total of 116 pre-service teachers, 38 males and 78 females. The results demonstrated a notable enhancement between the pre-test (8.11) and post-test (9.63) scores, emphasising specific concepts such as simple functions, while, and compound conditional. With respect to gender, statistically significant differences were identified prior to the intervention, but not following its implementation. The high level of Computational Thinking exhibited by both genders was comparable (53.85% in females and 55.26% in males) following the intervention. This indicates that the intervention is a promising approach for enhancing Computational Thinking proficiency, independent of gender and initial proficiency levels. The implementation of Educational Robotics in the teaching of science and mathematics enables the enhancement of Computational Thinking abilities among pre-service teachers, while reducing the observed gender disparity in this area of skill development. ",
    "doi": "10.3390/ejihpe14110188"
  },
  {
    "repository": "PubMed",
    "title": "Integrating AI into Breast Reconstruction Surgery: Exploring Opportunities, Applications, and Challenges.",
    "authors": "Andrew Gorgy; Hong Hao Xu; Hassan El Hawary; Hillary Nepon; James Lee; Joshua Vorstenbosch",
    "abstract": "Background: Artificial intelligence (AI) has significantly influenced various sectors, including healthcare, by enhancing machine capabilities in assisting with human tasks. In surgical fields, where precision and timely decision-making are crucial, AI's integration could revolutionize clinical quality and health resource optimization. This study explores the current and future applications of AI technologies in reconstructive breast surgery, aiming for broader implementation. Methods: We conducted systematic reviews through PubMed, Web of Science, and Google Scholar using relevant keywords and MeSH terms. The focus was on the main AI subdisciplines: machine learning, computer vision, natural language processing, and robotics. This review includes studies discussing AI applications across preoperative, intraoperative, postoperative, and academic settings in breast plastic surgery. Results: AI is currently utilized preoperatively to predict surgical risks and outcomes, enhancing patient counseling and informed consent processes. During surgery, AI supports the identification of anatomical landmarks and dissection strategies and provides 3-dimensional visualizations. Robotic applications are promising for procedures like microsurgical anastomoses, flap harvesting, and dermal matrix anchoring. Postoperatively, AI predicts discharge times and customizes follow-up schedules, which improves resource allocation and patient management at home. Academically, AI offers personalized training feedback to surgical trainees and aids research in breast reconstruction. Despite these advancements, concerns regarding privacy, costs, and operational efficacy persist and are critically examined in this review. Conclusions: The application of AI in breast plastic and reconstructive surgery presents substantial benefits and diverse potentials. However, much remains to be explored and developed. This study aims to consolidate knowledge and encourage ongoing research and development within the field, thereby empowering the plastic surgery community to leverage AI technologies effectively and responsibly for advancing breast reconstruction surgery. ",
    "doi": "10.1177/22925503241292349"
  },
  {
    "repository": "PubMed",
    "title": "Nature redux: interrogating biomorphism and soft robot aesthetics through generative AI.",
    "authors": "Mads Bering Christiansen; Ahmad Rafsanjani; Jonas Jørgensen",
    "abstract": "Artificial Intelligence (AI) has rapidly become a widespread design aid through the recent proliferation of generative AI tools. In this work we use generative AI to explore soft robotics designs, specifically Soft Biomorphism, an aesthetic design paradigm emphasizing the inherent biomorphic qualities of soft robots to leverage them as affordances for interactions with humans. The work comprises two experiments aimed at uncovering how generative AI can articulate and expand the design space of soft biomorphic robotics using text-to-image (TTI) and image-to-image (ITI) generation techniques. Through TTI generation, Experiment 1 uncovered alternative interpretations of soft biomorphism, emphasizing the novel incorporation of, e.g., fur, which adds a new dimension to the material aesthetics of soft robotics. In Experiment 2, TTI and ITI generation were combined and a category of hybrid techno-organic robot designs discovered, which combined rigid and pliable materials. The work thus demonstrates in practice the specific ways in which AI image generation can contribute towards expanding the design space of soft robotics. ",
    "doi": "10.3389/frobt.2024.1472051"
  },
  {
    "repository": "PubMed",
    "title": "Deep Learning: A Primer for Neurosurgeons.",
    "authors": "Hongxi Yang; Chang Yuwen; Xuelian Cheng; Hengwei Fan; Xin Wang; Zongyuan Ge",
    "abstract": "This chapter explores the transformative impact of deep learning (DL) on neurosurgery, elucidating its pivotal role in enhancing diagnostic performance, surgical planning, execution, and postoperative assessment. It delves into various deep learning architectures, including convolutional and recurrent neural networks, and their applications in analyzing neuroimaging data for brain tumors, spinal cord injuries, and other neurological conditions. The integration of DL in neurosurgical robotics and the potential for fully autonomous surgical procedures are discussed, highlighting advancements in surgical precision and patient outcomes. The chapter also examines the challenges of data privacy, quality, and interpretability that accompany the implementation of DL in neurosurgery. The potential for DL to revolutionize neurosurgical practices through improved diagnostics, patient-specific surgical planning, and the advent of intelligent surgical robots is underscored, promising a future where technology and healthcare converge to offer unprecedented solutions in neurosurgery. ",
    "doi": "10.1007/978-3-031-64892-2_4"
  },
  {
    "repository": "PubMed",
    "title": "Towards industry-ready additive manufacturing: AI-enabled closed-loop control for 3D melt electrowriting.",
    "authors": "Pawel Mieszczanek; Peter Corke; Courosh Mehanian; Paul D Dalton; Dietmar W Hutmacher",
    "abstract": "Melt electrowriting (MEW) is an emerging high-resolution 3D printing technology used in biomedical engineering, regenerative medicine, and soft robotics. Its transition from academia to industry faces challenges such as slow experimentation, low printing throughput, poor reproducibility, and user-dependent operation, largely due to the nonlinear and multiparametric nature of the MEW process. To address these challenges, we applied computer vision and machine learning to monitor and analyze the process in real-time through imaging of the MEW jet between the nozzle-collector gap. To collect data for training we developed an automated data collection methodology that eases the experimental time from days to hours. A feedforward neural network, working in concert with optimization methods and a feedback loop, is used to develop closed-loop control ensuring reproducibility of the printed parts. We demonstrate that machine learning allows streamlining the MEW operation via closed-loop control of the highly nonlinear 3D printing technology. ",
    "doi": "10.1038/s44172-024-00302-4"
  },
  {
    "repository": "PubMed",
    "title": "Capturing forceful interaction with deformable objects using a deep learning-powered stretchable tactile array.",
    "authors": "Chunpeng Jiang; Wenqiang Xu; Yutong Li; Zhenjun Yu; Longchun Wang; Xiaotong Hu; Zhengyi Xie; Qingkun Liu; Bin Yang; Xiaolin Wang; Wenxin Du; Tutian Tang; Dongzhe Zheng; Siqiong Yao; Cewu Lu; Jingquan Liu",
    "abstract": "Capturing forceful interaction with deformable objects during manipulation benefits applications like virtual reality, telemedicine, and robotics. Replicating full hand-object states with complete geometry is challenging because of the occluded object deformations. Here, we report a visual-tactile recording and tracking system for manipulation featuring a stretchable tactile glove with 1152 force-sensing channels and a visual-tactile joint learning framework to estimate dynamic hand-object states during manipulation. To overcome the strain interference caused by contact with deformable objects, an active suppression method based on symmetric response detection and adaptive calibration is proposed and achieves 97.6% accuracy in force measurement, contributing to an improvement of 45.3%. The learning framework processes the visual-tactile sequence and reconstructs hand-object states. We experiment on 24 objects from 6 categories including both deformable and rigid ones with an average reconstruction error of 1.8 cm for all sequences, demonstrating a universal ability to replicate human knowledge in manipulating objects with varying degrees of deformability. ",
    "doi": "10.1038/s41467-024-53654-y"
  },
  {
    "repository": "PubMed",
    "title": "Leveraging imitation learning in agricultural robotics: a comprehensive survey and comparative analysis.",
    "authors": "Siavash Mahmoudi; Amirreza Davar; Pouya Sohrabipour; Ramesh Bahadur Bist; Yang Tao; Dongyi Wang",
    "abstract": "Imitation learning (IL), a burgeoning frontier in machine learning, holds immense promise across diverse domains. In recent years, its integration into robotics has sparked significant interest, offering substantial advancements in autonomous control processes. This paper presents an exhaustive insight focusing on the implementation of imitation learning techniques in agricultural robotics. The survey rigorously examines varied research endeavors utilizing imitation learning to address pivotal agricultural challenges. Methodologically, this survey comprehensively investigates multifaceted aspects of imitation learning applications in agricultural robotics. The survey encompasses the identification of agricultural tasks that can potentially be addressed through imitation learning, detailed analysis of specific models and frameworks, and a thorough assessment of performance metrics employed in the surveyed studies. Additionally, it includes a comparative analysis between imitation learning techniques and conventional control methodologies in the realm of robotics. The findings derived from this survey unveil profound insights into the applications of imitation learning in agricultural robotics. These methods are highlighted for their potential to significantly improve task execution in dynamic and high-dimensional action spaces prevalent in agricultural settings, such as precision farming. Despite promising advancements, the survey discusses considerable challenges in data quality, environmental variability, and computational constraints that IL must overcome. The survey also addresses the ethical and social implications of implementing such technologies, emphasizing the need for robust policy frameworks to manage the societal impacts of automation. These findings hold substantial implications, showcasing the potential of imitation learning to revolutionize processes in agricultural robotics. This research significantly contributes to envisioning innovative applications and tools within the agricultural robotics domain, promising heightened productivity and efficiency in robotic agricultural systems. It underscores the potential for remarkable enhancements in various agricultural processes, signaling a transformative trajectory for the sector, particularly in the realm of robotics and autonomous systems. ",
    "doi": "10.3389/frobt.2024.1441312"
  },
  {
    "repository": "PubMed",
    "title": "Medical Doctors' Perceptions of Artificial Intelligence (AI) in Healthcare.",
    "authors": "Arijita Banerjee; Pradosh Kumar Sarangi; Sumit Kumar",
    "abstract": "Introduction With the current exponential expansion of robotics, implants, and imaging technologies, diagnostic processes within the healthcare industry are becoming popular platforms for artificial intelligence (AI) use. Thus, an understanding of physicians' attitudes toward AI and the extent to which medical educators are ready to work with AI is necessary. This research aimed to study doctors' perceptions of AI in healthcare. Methods A web-based questionnaire organized into four sections, namely, demographics, concepts of AI, education in AI, and implementation challenges related to AI, was designed systematically based on a literature search and circulated among medical doctors from various fields. Results Study participants exhibited a lower score toward familiarity with AI. Only 52.12% (74/142) of physicians completed the survey. The greatest challenge associated with the use of AI in therapeutic settings was found to be the degree of autonomy, with a score of 3.56. Among the participants, 67.61% felt that the lack of human supervision was the most important limiting factor in the implementation of AI in clinical practice. However, the participants demonstrated a strong interest in understanding the concepts of AI in the near future. Conclusion This study revealed a low degree of familiarity with AI, highlighting the need for medical schools and hospitals to establish specialized education and training programs for physicians to improve patient outcomes. ",
    "doi": "10.7759/cureus.70508"
  },
  {
    "repository": "PubMed",
    "title": "Targeted weed management of Palmer amaranth using robotics and deep learning (YOLOv7).",
    "authors": "Amlan Balabantaray; Shaswati Behera; CheeTown Liew; Nipuna Chamara; Mandeep Singh; Amit J Jhala; Santosh Pitla",
    "abstract": "Effective weed management is a significant challenge in agronomic crops which necessitates innovative solutions to reduce negative environmental impacts and minimize crop damage. Traditional methods often rely on indiscriminate herbicide application, which lacks precision and sustainability. To address this critical need, this study demonstrated an AI-enabled robotic system, Weeding robot, designed for targeted weed management. Palmer amaranth (Amaranthus palmeri S. Watson) was selected as it is the most troublesome weed in Nebraska. We developed the full stack (vision, hardware, software, robotic platform, and AI model) for precision spraying using YOLOv7, a state-of-the-art object detection deep learning technique. The Weeding robot achieved an average of 60.4% precision and 62% recall in real-time weed identification and spot spraying with the developed gantry-based sprayer system. The Weeding robot successfully identified Palmer amaranth across diverse growth stages in controlled outdoor conditions. This study demonstrates the potential of AI-enabled robotic systems for targeted weed management, offering a more precise and sustainable alternative to traditional herbicide application methods. ",
    "doi": "10.3389/frobt.2024.1441371"
  },
  {
    "repository": "PubMed",
    "title": "AI security and cyber risk in IoT systems.",
    "authors": "Petar Radanliev; David De Roure; Carsten Maple; Jason R C Nurse; Razvan Nicolescu; Uchenna Ani",
    "abstract": "Internet-of-Things (IoT) refers to low-memory connected devices used in various new technologies, including drones, autonomous machines, and robotics. The article aims to understand better cyber risks in low-memory devices and the challenges in IoT risk management. The article includes a critical reflection on current risk methods and their level of appropriateness for IoT. We present a dependency model tailored in context toward current challenges in data strategies and make recommendations for the cybersecurity community. The model can be used for cyber risk estimation and assessment and generic risk impact assessment. The model is developed for cyber risk insurance for new technologies (e.g., drones, robots). Still, practitioners can apply it to estimate and assess cyber risks in organizations and enterprises. Furthermore, this paper critically discusses why risk assessment and management are crucial in this domain and what open questions on IoT risk assessment and risk management remain areas for further research. The paper then presents a more holistic understanding of cyber risks in the IoT. We explain how the industry can use new risk assessment, and management approaches to deal with the challenges posed by emerging IoT cyber risks. We explain how these approaches influence policy on cyber risk and data strategy. We also present a new approach for cyber risk assessment that incorporates IoT risks through dependency modeling. The paper describes why this approach is well suited to estimate IoT risks. ",
    "doi": "10.3389/fdata.2024.1402745"
  },
  {
    "repository": "PubMed",
    "title": "Attention Induced Dual Convolutional-Capsule Network (AIDC-CN): A deep learning framework for motor imagery classification.",
    "authors": "Ritesh Sur Chowdhury; Shirsha Bose; Sayantani Ghosh; Amit Konar",
    "abstract": "In recent times, Electroencephalography (EEG)-based motor imagery (MI) decoding has garnered significant attention due to its extensive applicability in healthcare, including areas such as assistive robotics and rehabilitation engineering. Nevertheless, the decoding of EEG signals presents considerable challenges owing to their inherent complexity, non-stationary characteristics, and low signal-to-noise ratio. Notably, deep learning-based classifiers have emerged as a prominent focus for addressing the EEG signal decoding process. This study introduces a novel deep learning classifier named the Attention Induced Dual Convolutional-Capsule Network (AIDC-CN) with the specific aim of accurately categorizing various motor imagination class labels. To enhance the classifier's performance, a dual feature extraction approach leveraging spectrogram and brain connectivity networks has been employed, diversifying the feature set in the classification task. The main highlights of the proposed AIDC-CN classifier includes the introduction of a dual convolution layer to handle the brain connectivity and spectrogram features, addition of a novel self-attention module (SAM) to accentuate the relevant parts of the convolved spectrogram features, introduction of a new cross-attention module (CAM) to refine the outputs obtained from the dual convolution layers and incorporation of a Gaussian Error Linear Unit (GELU) based dynamic routing algorithm to strengthen the coupling among the primary and secondary capsule layers. Performance analysis undertaken on four public data sets depict the superior performance of the proposed model with respect to the state-of-the-art techniques. The code for this model is available at https://github.com/RiteshSurChowdhury/AIDC-CN. ",
    "doi": "10.1016/j.compbiomed.2024.109260"
  },
  {
    "repository": "PubMed",
    "title": "The balancing act: Adopting AI and robotics in medicine with cautious optimism.",
    "authors": "Vaibhav Bagaria; Harvinder Singh Chhabra",
    "abstract": "",
    "doi": "10.1016/j.jcot.2024.102550"
  },
  {
    "repository": "PubMed",
    "title": "Classifying Residual Stroke Severity Using Robotics-Assisted Stroke Rehabilitation: Machine Learning Approach.",
    "authors": "Russell Jeter; Raymond Greenfield; Stephen N Housley; Igor Belykh",
    "abstract": "Stroke therapy is essential to reduce impairments and improve motor movements by engaging autogenous neuroplasticity. Traditionally, stroke rehabilitation occurs in inpatient and outpatient rehabilitation facilities. However, recent literature increasingly explores moving the recovery process into the home and integrating technology-based interventions. This study advances this goal by promoting in-home, autonomous recovery for patients who experienced a stroke through robotics-assisted rehabilitation and classifying stroke residual severity using machine learning methods. Our main objective is to use kinematics data collected during in-home, self-guided therapy sessions to develop supervised machine learning methods, to address a clinician's autonomous classification of stroke residual severity-labeled data toward improving in-home, robotics-assisted stroke rehabilitation. In total, 33 patients who experienced a stroke participated in in-home therapy sessions using Motus Nova robotics rehabilitation technology to capture upper and lower body motion. During each therapy session, the Motus Hand and Motus Foot devices collected movement data, assistance data, and activity-specific data. We then synthesized, processed, and summarized these data. Next, the therapy session data were paired with clinician-informed, discrete stroke residual severity labels: \"\"no range of motion (ROM),\"\" \"\"low ROM,\"\" and \"\"high ROM.\"\" Afterward, an 80%:20% split was performed to divide the dataset into a training set and a holdout test set. We used 4 machine learning algorithms to classify stroke residual severity: light gradient boosting (LGB), extra trees classifier, deep feed-forward neural network, and classical logistic regression. We selected models based on 10-fold cross-validation and measured their performance on a holdout test dataset using F1-score to identify which model maximizes stroke residual severity classification accuracy. We demonstrated that the LGB method provides the most reliable autonomous detection of stroke severity. The trained model is a consensus model that consists of 139 decision trees with up to 115 leaves each. This LGB model boasts a 96.70% F1-score compared to logistic regression (55.82%), extra trees classifier (94.81%), and deep feed-forward neural network (70.11%). We showed how objectively measured rehabilitation training paired with machine learning methods can be used to identify the residual stroke severity class, with efforts to enhance in-home self-guided, individualized stroke rehabilitation. The model we trained relies only on session summary statistics, meaning it can potentially be integrated into similar settings for real-time classification, such as outpatient rehabilitation facilities. ",
    "doi": "10.2196/56980"
  },
  {
    "repository": "PubMed",
    "title": "Reinforcement learning of biomimetic navigation: a model problem for sperm chemotaxis.",
    "authors": "Omar Mohamed; Alan C H Tsang",
    "abstract": "Motile biological cells can respond to local environmental cues and exhibit various navigation strategies to search for specific targets. These navigation strategies usually involve tuning of key biophysical parameters of the cells, such that the cells can modulate their trajectories to move in response to the detected signals. Here we introduce a reinforcement learning approach to modulate key biophysical parameters and realize navigation strategies reminiscent to those developed by biological cells. We present this approach using sperm chemotaxis toward an egg as a paradigm. By modulating the trajectory curvature of a sperm cell model, the navigation strategies informed by reinforcement learning are capable to resemble sperm chemotaxis observed in experiments. This approach provides an alternative method to capture biologically relevant navigation strategies, which may inform the necessary parameter modulations required for obtaining specific navigation strategies and guide the design of biomimetic micro-robotics. ",
    "doi": "10.1140/epje/s10189-024-00451-6"
  },
  {
    "repository": "PubMed",
    "title": "Using Reinforcement Learning to Develop a Novel Gait for a Bio-Robotic California Sea Lion.",
    "authors": "Anthony Drago; Shraman Kadapa; Nicholas Marcouiller; Harry G Kwatny; James L Tangorra",
    "abstract": "While researchers have made notable progress in bio-inspired swimming robot development, a persistent challenge lies in creating propulsive gaits tailored to these robotic systems. The California sea lion achieves its robust swimming abilities through a careful coordination of foreflippers and body segments. In this paper, reinforcement learning (RL) was used to develop a novel sea lion foreflipper gait for a bio-robotic swimmer using a numerically modelled computational representation of the robot. This model integration enabled reinforcement learning to develop desired swimming gaits in the challenging underwater domain. The novel RL gait outperformed the characteristic sea lion foreflipper gait in the simulated underwater domain. When applied to the real-world robot, the RL constructed novel gait performed as well as or better than the characteristic sea lion gait in many factors. This work shows the potential for using complimentary bio-robotic and numerical models with reinforcement learning to enable the development of effective gaits and maneuvers for underwater swimming vehicles. ",
    "doi": "10.3390/biomimetics9090522"
  },
  {
    "repository": "PubMed",
    "title": "Learning curves for adoption of robotic bariatric surgery: a systematic review of safety, efficiency and clinical outcomes.",
    "authors": "Faith Hirri; Oliver J Pickering; Nicholas C Carter; Gijsbert I van Boxel; Philip H Pucher",
    "abstract": "Robotic bariatric surgery may overcome challenges associated with laparoscopy, potentially achieving technically superior results. This review aims to summarise current literature reporting on learning curves for surgeons newly adopting robotic bariatrics and implications for safety, efficiency and outcomes. A systematic review was performed in line with the PRISMA guidelines. Electronic databases PubMed and MEDLINE were searched and articles reporting on learning curves in robotic bariatric surgery were identified. Studies that reported changes in outcome over time, or learning curves for surgeons newly adopting robotic bariatric surgery were included in this review. Eleven studies reporting on 1237 patients were included in this review. Most surgeons reported prior bariatric surgical experience. Differences were noted regarding the approach and adoption of robotics. Ten studies found significant reduction in operative time, with the shortest learning curve of 11 cases. Reporting of clinical outcomes was limited. Three studies reported statistically significant improvement in outcomes after the learning curve. Long-term outcomes were in line with current literature, though none assessed differences between learning curve groups. Reported learning curves in robotic bariatric surgery is variable, with limited reporting of clinical outcomes. With appropriate mentorship, surgeons can improve efficiency, safety and clinical outcomes, maximising the benefits of minimally invasive surgery. ",
    "doi": "10.1007/s11701-024-02100-8"
  },
  {
    "repository": "PubMed",
    "title": "Reinforcement learning as a robotics-inspired framework for insect navigation: from spatial representations to neural implementation.",
    "authors": "Stephan Lochner; Daniel Honerkamp; Abhinav Valada; Andrew D Straw",
    "abstract": "Bees are among the master navigators of the insect world. Despite impressive advances in robot navigation research, the performance of these insects is still unrivaled by any artificial system in terms of training efficiency and generalization capabilities, particularly considering the limited computational capacity. On the other hand, computational principles underlying these extraordinary feats are still only partially understood. The theoretical framework of reinforcement learning (RL) provides an ideal focal point to bring the two fields together for mutual benefit. In particular, we analyze and compare representations of space in robot and insect navigation models through the lens of RL, as the efficiency of insect navigation is likely rooted in an efficient and robust internal representation, linking retinotopic (egocentric) visual input with the geometry of the environment. While RL has long been at the core of robot navigation research, current computational theories of insect navigation are not commonly formulated within this framework, but largely as an associative learning process implemented in the insect brain, especially in the mushroom body (MB). Here we propose specific hypothetical components of the MB circuit that would enable the implementation of a certain class of relatively simple RL algorithms, capable of integrating distinct components of a navigation task, reminiscent of hierarchical RL models used in robot navigation. We discuss how current models of insect and robot navigation are exploring representations beyond classical, complete map-like representations, with spatial information being embedded in the respective latent representations to varying degrees. ",
    "doi": "10.3389/fncom.2024.1460006"
  },
  {
    "repository": "PubMed",
    "title": "Robotics in Arthroplasty: Historical Progression, Contemporary Applications, and Future Horizons With Artificial Intelligence (AI) Integration.",
    "authors": "Jagbir Singh; Priyankkumar Patel",
    "abstract": "Robotic technology is increasingly utilized in surgical procedures to enhance precision, particularly in tasks demanding delicate maneuvers beyond human capabilities. Robotic orthopedic surgery emerges as a dynamic and compelling technology reshaping the landscape of surgical practice. This aids surgeons in achieving enhanced accuracy and reproducibility, ultimately aiming for improved patient outcomes. As of now, the majority of these systems are in a developed stage and are gradually gaining broader adoption. These systems have to show that they are user-friendly, are successful in clinical settings, and have a good cost-effectiveness ratio before they can be widely adopted in the field of surgery. In this review, we examine the evolution of robotics in orthopedic surgery, assess its current applications, and provide insights into the future trajectory of this technology, particularly in light of advances in artificial intelligence (AI) and machine learning (ML). ",
    "doi": "10.7759/cureus.67611"
  },
  {
    "repository": "PubMed",
    "title": "Robotics and AI into healthcare from the perspective of European regulation: who is responsible for medical malpractice?",
    "authors": "Francesco De Micco; Simone Grassi; Luca Tomassini; Gianmarco Di Palma; Giulia Ricchezze; Roberto Scendoni",
    "abstract": "The integration of robotics and artificial intelligence into medical practice is radically revolutionising patient care. This fusion of advanced technologies with healthcare offers a number of significant benefits, including more precise diagnoses, personalised treatments and improved health data management. However, it is critical to address very carefully the medico-legal challenges associated with this progress. The responsibilities between the different players concerned in medical liability cases are not yet clearly defined, especially when artificial intelligence is involved in the decision-making process. Complexity increases when technology intervenes between a person's action and the result, making it difficult for the patient to prove harm or negligence. In addition, there is the risk of an unfair distribution of blame between physicians and healthcare institutions. The analysis of European legislation highlights the critical issues related to the attribution of legal personality to autonomous robots and the recognition of strict liability for medical doctors and healthcare institutions. Although European legislation has helped to standardise the rules on this issue, some questions remain unresolved. We argue that specific laws are needed to address the issue of medical liability in cases where robotics and artificial intelligence are used in healthcare. ",
    "doi": "10.3389/fmed.2024.1428504"
  },
  {
    "repository": "PubMed",
    "title": "How Can Materials Chemists Contribute to Food Supply Security in the Age of AI and Robotics?",
    "authors": "Ludovico Cademartiri",
    "abstract": "Global factors are compromising the security of the food supply. Many of us in our community might be unsure about how to help with our seemingly inapplicable expertise. In this piece, I describe how materials chemistry and materials chemists will be essential in addressing this problem, but not only in the way we might think. There is a gigantic and largely unexplored opportunity for real societal impact if we accept a broader view of what academic materials chemistry can be. ",
    "doi": "10.1002/anie.202319757"
  },
  {
    "repository": "PubMed",
    "title": "A bench-top evaluation of the Ily® robotics assisted tele console system in ureteroscopy by medical students, residents and urologists: does prior videogaming experience help shape the learning curve?",
    "authors": "Ahmed Alanazi; Aideen Madden; Lucien Vanpoperinghe; Paula Calugaru; Alberto Quarà; Lachlan Dokter; Stefano Moretto; Johan Cabrera; Mariela Corrales; Olivier Traxer",
    "abstract": "Previous studies have demonstrated an association between video-gaming experience (VGE) and improved robotics skills. We aimed to evaluate the initial learning curve for the Ily® robotics system (Sterlab, Sophia Antipolis, France) when applied to flexible ureteroscopy (FU) among both medical students and urology surgeons. There were two groups, surgeons and students. An initial questionnaire was completed detailing basic demographics and experience. In part one, both groups performed two simple timed tasks using an Ily® mounted single-use RAU. In part two, group 1 repeated both tasks using a hand-held FU. A subjective assessment of comfort, intuitiveness and a NASA Task Load Index were then completed. There was a total of 28 participants. Among medical students with VGE (n = 9, 64%)., average calyceal inspection time was 185 ± 80 s; 133 ± 42 s; 121 ± 71 s. For non-gamers (n = 5, 36%), average times were longer at 221 ± 97 s; 134 ± 35 s; 143 ± 68 s respectively. Average calyceal inspection time for videogaming surgeons (n = 8, 57%) was 126 ± 95 s; 98 ± 40 s; 107 ± 71 s, respectively. For non-gamers average inspection times were longer at 150 ± 73 s; 114 ± 82 s; 111 ± 47 s, respectively. None of these differences achieved statistical significance. Surgeons trial speeds were, however, significantly faster by hand-held compared to RAU: by 103, 81 and 82 s respectively (p < 0.05). These results show that ex- or current- video gamers do not have a significant advantage in time to perform FU. Any early advantage conferred to ex- or current- gamers may be rapidly overcome. ",
    "doi": "10.1007/s00345-024-05197-6"
  },
  {
    "repository": "PubMed",
    "title": "Eye-tracker and fNIRS: Using neuroscientific tools to assess the learning experience during children's educational robotics activities.",
    "authors": "Eneyse Dayane Pinheiro; João Ricardo Sato; Raimundo da Silva Soares Junior; Candida Barreto; Amanda Yumi Ambriola Oku",
    "abstract": "In technology education, there has been a paradigmatic shift towards student-centered approaches such as learning by doing, constructionism, and experiential learning. Educational robotics allows students to experiment with building and interacting with their creations while also fostering collaborative work. However, understanding the student's response to these approaches is crucial to adapting them during the teaching-learning process. In this sense, neuroscientific tools such as Functional Near-Infrared Spectroscopy and Eye-tracker could be useful, allowing the investigation of relevant states experienced by students. Although they have already been used in educational research, their practical relevance in the teaching-learning process has not been extensively investigated. In this perspective article expressing our position, we bring four examples of learning experiences in a robotics class with children, in which we illustrate the usefulness of these tools. ",
    "doi": "10.1016/j.tine.2024.100234"
  },
  {
    "repository": "PubMed",
    "title": "A Review of Safe Reinforcement Learning: Methods, Theories, and Applications.",
    "authors": "Shangding Gu; Long Yang; Yali Du; Guang Chen; Florian Walter; Jun Wang; Alois Knoll",
    "abstract": "Reinforcement Learning (RL) has achieved tremendous success in many complex decision-making tasks. However, safety concerns are raised during deploying RL in real-world applications, leading to a growing demand for safe RL algorithms, such as in autonomous driving and robotics scenarios. While safe control has a long history, the study of safe RL algorithms is still in the early stages. To establish a good foundation for future safe RL research, in this paper, we provide a review of safe RL from the perspectives of methods, theories, and applications. First, we review the progress of safe RL from five dimensions and come up with five crucial problems for safe RL being deployed in real-world applications, coined as \"\"2H3W\"\". Second, we analyze the algorithm and theory progress from the perspectives of answering the \"\"2H3W\"\" problems. Particularly, the sample complexity of safe RL algorithms is reviewed and discussed, followed by an introduction to the applications and benchmarks of safe RL algorithms. Finally, we open the discussion of the challenging problems in safe RL, hoping to inspire future research on this thread. To advance the study of safe RL algorithms, we release an open-sourced repository containing major safe RL algorithms at the link. ",
    "doi": "10.1109/TPAMI.2024.3457538"
  },
  {
    "repository": "PubMed",
    "title": "Deep learning approach for detecting tomato flowers and buds in greenhouses on 3P2R gantry robot.",
    "authors": "Rajmeet Singh; Asim Khan; Lakmal Seneviratne; Irfan Hussain",
    "abstract": "In recent years, significant advancements have been made in the field of smart greenhouses, particularly in the application of computer vision and robotics for pollinating flowers. Robotic pollination offers several benefits, including reduced labor requirements and preservation of costly pollen through artificial tomato pollination. However, previous studies have primarily focused on the labeling and detection of tomato flowers alone. Therefore, the objective of this study was to develop a comprehensive methodology for simultaneously labeling, training, and detecting tomato flowers specifically tailored for robotic pollination. To achieve this, transfer learning techniques were employed using well-known models, namely YOLOv5 and the recently introduced YOLOv8, for tomato flower detection. The performance of both models was evaluated using the same image dataset, and a comparison was made based on their Average Precision (AP) scores to determine the superior model. The results indicated that YOLOv8 achieved a higher mean AP (mAP) of 92.6% in tomato flower and bud detection, outperforming YOLOv5 with 91.2%. Notably, YOLOv8 also demonstrated an inference speed of 0.7 ms when considering an image size of 1920 × 1080  pixels resized to 640 × 640  pixels during detection. The image dataset was acquired during both morning and evening periods to minimize the impact of lighting conditions on the detection model. These findings highlight the potential of YOLOv8 for real-time detection of tomato flowers and buds, enabling further estimation of flower blooming peaks and facilitating robotic pollination. In the context of robotic pollination, the study also focuses on the deployment of the proposed detection model on the 3P2R gantry robot. The study introduces a kinematic model and a modified circuit for the gantry robot. The position-based visual servoing method is employed to approach the detected flower during the pollination process. The effectiveness of the proposed visual servoing approach is validated in both un-clustered and clustered plant environments in the laboratory setting. Additionally, this study provides valuable theoretical and practical insights for specialists in the field of greenhouse systems, particularly in the design of flower detection algorithms using computer vision and its deployment in robotic systems used in greenhouses. ",
    "doi": "10.1038/s41598-024-71013-1"
  },
  {
    "repository": "PubMed",
    "title": "Overtrust in AI Recommendations About Whether or Not to Kill: Evidence from Two Human-Robot Interaction Studies.",
    "authors": "Colin Holbrook; Daniel Holman; Joshua Clingo; Alan R Wagner",
    "abstract": "This research explores prospective determinants of trust in the recommendations of artificial agents regarding decisions to kill, using a novel visual challenge paradigm simulating threat-identification (enemy combatants vs. civilians) under uncertainty. In Experiment 1, we compared trust in the advice of a physically embodied versus screen-mediated anthropomorphic robot, observing no effects of embodiment; in Experiment 2, we manipulated the relative anthropomorphism of virtual robots, observing modestly greater trust in the most anthropomorphic agent relative to the least. Across studies, when any version of the agent randomly disagreed, participants reversed their threat-identifications and decisions to kill in the majority of cases, substantially degrading their initial performance. Participants' subjective confidence in their decisions tracked whether the agent (dis)agreed, while both decision-reversals and confidence were moderated by appraisals of the agent's intelligence. The overall findings indicate a strong propensity to overtrust unreliable AI in life-or-death decisions made under uncertainty. ",
    "doi": "10.1038/s41598-024-69771-z"
  },
  {
    "repository": "PubMed",
    "title": "Managing workplace AI risks and the future of work.",
    "authors": "John Howard; Paul Schulte",
    "abstract": "Artificial intelligence (AI)-the field of computer science that designs machines to perform tasks that typically require human intelligence-has seen rapid advances in the development of foundation systems such as large language models. In the workplace, the adoption of AI technologies can result in a broad range of hazards and risks to workers, as illustrated by the recent growth in industrial robotics and algorithmic management. Sources of risk from deployment of AI technologies across society and in the workplace have led to numerous government and private sector guidelines that propose principles governing the design and use of trustworthy and ethical AI. As AI capabilities become integrated in devices, machines, and systems across industry sectors, employers, workers, and occupational safety and health practitioners will be challenged to manage AI risks to worker health, safety, and well-being. Five risk management options are presented as ways to assure that only trustworthy and ethical AI enables workplace devices, machinery, and processes. AI technologies will play a significant role in the future of work. The occupational safety and health practice and research communities need to ensure that the promise of these new AI technologies results in benefit, not harm, to workers. ",
    "doi": "10.1002/ajim.23653"
  },
  {
    "repository": "PubMed",
    "title": "Toward an AI Era: Advances in Electronic Skins.",
    "authors": "Xuemei Fu; Wen Cheng; Guanxiang Wan; Zijie Yang; Benjamin C K Tee",
    "abstract": "Electronic skins (e-skins) have seen intense research and rapid development in the past two decades. To mimic the capabilities of human skin, a multitude of flexible/stretchable sensors that detect physiological and environmental signals have been designed and integrated into functional systems. Recently, researchers have increasingly deployed machine learning and other artificial intelligence (AI) technologies to mimic the human neural system for the processing and analysis of sensory data collected by e-skins. Integrating AI has the potential to enable advanced applications in robotics, healthcare, and human-machine interfaces but also presents challenges such as data diversity and AI model robustness. In this review, we first summarize the functions and features of e-skins, followed by feature extraction of sensory data and different AI models. Next, we discuss the utilization of AI in the design of e-skin sensors and address the key topic of AI implementation in data processing and analysis of e-skins to accomplish a range of different tasks. Subsequently, we explore hardware-layer in-skin intelligence before concluding with an analysis of the challenges and opportunities in the various aspects of AI-enabled e-skins. ",
    "doi": "10.1021/acs.chemrev.4c00049"
  },
  {
    "repository": "PubMed",
    "title": "AI4Work Project: Human-Centric Digital Twin Approaches to Trustworthy AI and Robotics for Improved Working Conditions in Healthcare and Education Sectors.",
    "authors": "Ilias Maglogiannis; Filimon Trastelis; Michael Kalogeropoulos; Arsalan Khan; Parisis Gallos; Andreas Menychtas; Christos Panagopoulos; Petros Papachristou; Najmul Islam; Annika Wolff; Salviano Soares; Scott Hansen; Sebastian Scholze",
    "abstract": "AI and robotics aim to transform workplace landscapes in a several sectors such as manufacturing, logistics, healthcare, construction, agriculture, and education. Central to this evolution is the innovative use of Digital Twin technology, which creates real-time updated virtual replicas of physical systems and entities. This technology is especially transformative in healthcare and education, promising customized and efficient experiences for all involved. This paper outlines the AI4Work project's approach to leveraging Digital Twin Technology to improve work environments in these sectors. The goal of AI4Work is to formulate a workplace where AI and robots seamlessly collaborate with humans, while explores how to best share tasks between humans and machines in six different domains. For healthcare, AI4Work will explore how Digital Twin technology can assist occupational doctors and psychologists in monitoring the physical and mental health of hospital personnel in order to predict burnout symptoms and to create a sustainable working environment. In education, AI4Work will investigate how to uphold the mental health of both educators and students while fostering a more supportive and enduring educational setting. ",
    "doi": "10.3233/SHTI240581"
  },
  {
    "repository": "PubMed",
    "title": "Neurosurgery: AI-driven precision, robotics, and personalized care.",
    "authors": "Gokul Sudhakaran",
    "abstract": "",
    "doi": "10.1007/s10143-024-02678-5"
  },
  {
    "repository": "PubMed",
    "title": "The Dawn of a New Pharmaceutical Epoch: Can AI and Robotics Reshape Drug Formulation?",
    "authors": "Pauric Bannigan; Riley J Hickman; Alán Aspuru-Guzik; Christine Allen",
    "abstract": "Over the last four decades, pharmaceutical companies' expenditures on research and development have increased 51-fold. During this same time, clinical success rates for new drugs have remained unchanged at about 10 percent, predominantly due to lack of efficacy and/or safety concerns. This persistent problem underscores the need to innovate across the entire drug development process, particularly in drug formulation, which is often deprioritized and under-resourced. ",
    "doi": "10.1002/adhm.202401312"
  },
  {
    "repository": "PubMed",
    "title": "Robotic Grasping of Unknown Objects Based on Deep Learning-Based Feature Detection.",
    "authors": "Kai Sherng Khor; Chao Liu; Chien Chern Cheah",
    "abstract": "In recent years, the integration of deep learning into robotic grasping algorithms has led to significant advancements in this field. However, one of the challenges faced by many existing deep learning-based grasping algorithms is their reliance on extensive training data, which makes them less effective when encountering unknown objects not present in the training dataset. This paper presents a simple and effective grasping algorithm that addresses this challenge through the utilization of a deep learning-based object detector, focusing on oriented detection of key features shared among most objects, namely straight edges and corners. By integrating these features with information obtained through image segmentation, the proposed algorithm can logically deduce a grasping pose without being limited by the size of the training dataset. Experimental results on actual robotic grasping of unknown objects over 400 trials show that the proposed method can achieve a higher grasp success rate of 98.25% compared to existing methods. ",
    "doi": "10.3390/s24154861"
  },
  {
    "repository": "PubMed",
    "title": "Biomimetic learning of hand gestures in a humanoid robot.",
    "authors": "Parthan Olikkal; Dingyi Pei; Bharat Kashyap Karri; Ashwin Satyanarayana; Nayan M Kakoty; Ramana Vinjamuri",
    "abstract": "Hand gestures are a natural and intuitive form of communication, and integrating this communication method into robotic systems presents significant potential to improve human-robot collaboration. Recent advances in motor neuroscience have focused on replicating human hand movements from synergies also known as movement primitives. Synergies, fundamental building blocks of movement, serve as a potential strategy adapted by the central nervous system to generate and control movements. Identifying how synergies contribute to movement can help in dexterous control of robotics, exoskeletons, prosthetics and extend its applications to rehabilitation. In this paper, 33 static hand gestures were recorded through a single RGB camera and identified in real-time through the MediaPipe framework as participants made various postures with their dominant hand. Assuming an open palm as initial posture, uniform joint angular velocities were obtained from all these gestures. By applying a dimensionality reduction method, kinematic synergies were obtained from these joint angular velocities. Kinematic synergies that explain 98% of variance of movements were utilized to reconstruct new hand gestures using convex optimization. Reconstructed hand gestures and selected kinematic synergies were translated onto a humanoid robot, Mitra, in real-time, as the participants demonstrated various hand gestures. The results showed that by using only few kinematic synergies it is possible to generate various hand gestures, with 95.7% accuracy. Furthermore, utilizing low-dimensional synergies in control of high dimensional end effectors holds promise to enable near-natural human-robot collaboration. ",
    "doi": "10.3389/fnhum.2024.1391531"
  },
  {
    "repository": "PubMed",
    "title": "On-receptor computing with classical associative learning in semiconductor oxide memristors.",
    "authors": "Dongyeol Ju; Jungwoo Lee; Sungjun Kim",
    "abstract": "The increasing demand for energy-efficient data processing leads to a growing interest in neuromorphic computing that aims to emulate cerebral functions. This approach offers cost-effective and rapid parallel data processing, surpassing the limitations of the conventional von Neumann architecture. Key to this emulation is the development of memristors that mimic biological synapses. Recently, research efforts have focused on the incorporation of nociceptors-sensory neurons capable of detecting external stimuli-into memristors for applications in robotics and artificial intelligence. This integration enables memristors to adapt to various circumstances while remaining cost-effective. A nonfilamentary gradual resistive switching memristor is utilized to implement artificial nociceptor and synaptic behaviors. The fabricated Pt/indium gallium zinc oxide (IGZO)/SnOx/TiN device exhibits essential properties of biological nociceptors, including threshold response, no-adaptation, relaxation, sensitization, and recovery. Furthermore, the device leverages short-term memory principles to emulate learning behaviors observed in the brain by showcasing \"\"forgetting\"\" paradigms. Additionally, control of the input spikes yields different synaptic plasticity responses, thus emulating the key functions of our synapse. Computational simulations demonstrate the device's ability to perform both computing and sensing tasks effectively, thus enabling on-receptor computing with associative learning capabilities. ",
    "doi": "10.1039/d4nr02132k"
  },
  {
    "repository": "PubMed",
    "title": "Sustainable biofabrication: from bioprinting to AI-driven predictive methods.",
    "authors": "Miriam Filippi; Manuel Mekkattu; Robert K Katzschmann",
    "abstract": "Biofabrication is potentially an inherently sustainable manufacturing process of bio-hybrid systems based on biomaterials embedded with cell communities. These bio-hybrids promise to augment the sustainability of various human activities, ranging from tissue engineering and robotics to civil engineering and ecology. However, as routine biofabrication practices are laborious and energetically disadvantageous, our society must refine production and validation processes in biomanufacturing. This opinion highlights the research trends in sustainable material selection and biofabrication techniques. By modeling complex biosystems, the computational prediction will allow biofabrication to shift from an error-trial method to an efficient, target-optimized approach with minimized resource and energy consumption. We envision that implementing bionomic rationality in biofabrication will render bio-hybrid products fruitful for greening human activities. ",
    "doi": "10.1016/j.tibtech.2024.07.002"
  },
  {
    "repository": "PubMed",
    "title": "Transformable Gaussian Reward Function for Socially Aware Navigation Using Deep Reinforcement Learning.",
    "authors": "Jinyeob Kim; Sumin Kang; Sungwoo Yang; Beomjoon Kim; Jargalbaatar Yura; Donghan Kim",
    "abstract": "Robot navigation has transitioned from avoiding static obstacles to adopting socially aware navigation strategies for coexisting with humans. Consequently, socially aware navigation in dynamic, human-centric environments has gained prominence in the field of robotics. One of the methods for socially aware navigation, the reinforcement learning technique, has fostered its advancement. However, defining appropriate reward functions, particularly in congested environments, holds a significant challenge. These reward functions, crucial for guiding robot actions, necessitate intricate human-crafted design due to their complex nature and inability to be set automatically. The multitude of manually designed reward functions contains issues such as hyperparameter redundancy, imbalance, and inadequate representation of unique object characteristics. To address these challenges, we introduce a transformable Gaussian reward function (TGRF). The TGRF possesses two main features. First, it reduces the burden of tuning by utilizing a small number of hyperparameters that function independently. Second, it enables the application of various reward functions through its transformability. Consequently, it exhibits high performance and accelerated learning rates within the deep reinforcement learning (DRL) framework. We also validated the performance of TGRF through simulations and experiments. ",
    "doi": "10.3390/s24144540"
  },
  {
    "repository": "PubMed",
    "title": "Active Inference for Learning and Development in Embodied Neuromorphic Agents.",
    "authors": "Sarah Hamburg; Alejandro Jimenez Rodriguez; Aung Htet; Alessandro Di Nuovo",
    "abstract": "Taking inspiration from humans can help catalyse embodied AI solutions for important real-world applications. Current human-inspired tools include neuromorphic systems and the developmental approach to learning. However, this developmental neurorobotics approach is currently lacking important frameworks for human-like computation and learning. We propose that human-like computation is inherently embodied, with its interface to the world being neuromorphic, and its learning processes operating across different timescales. These constraints necessitate a unified framework: active inference, underpinned by the free energy principle (FEP). Herein, we describe theoretical and empirical support for leveraging this framework in embodied neuromorphic agents with autonomous mental development. We additionally outline current implementation approaches (including toolboxes) and challenges, and we provide suggestions for next steps to catalyse this important field. ",
    "doi": "10.3390/e26070582"
  },
  {
    "repository": "PubMed",
    "title": "Insights into Flexible Bioinspired Fins for Unmanned Underwater Vehicle Systems through Deep Learning.",
    "authors": "Brian Zhou; Kamal Viswanath; Jason Geder; Alisha Sharma; Julian Lee",
    "abstract": "The last few decades have led to the rise of research focused on propulsion and control systems for bio-inspired unmanned underwater vehicles (UUVs), which provide more maneuverable alternatives to traditional UUVs in underwater missions. Recent work has explored the use of time-series neural network surrogate models to predict thrust and power from vehicle design and fin kinematics. We expand upon this work, creating new forward neural network models that encapsulate the effects of the material stiffness of the fin on its kinematic performance, thrust, and power, and are able to interpolate to the full spectrum of kinematic gaits for each material. Notably, we demonstrate through testing of holdout data that our developed forward models capture the thrust and power associated with each set of parameters with high resolution, enabling highly accurate predictions of previously unseen gaits and thrust and FOM gains through proper materials and kinematics selection. As propulsive efficiency is of utmost importance for flapping-fin UUVs in order to extend their range and endurance for essential operations, a non-dimensional figure of merit (FOM), derived from measures of propulsive efficiency, is used to evaluate different fin designs and kinematics and allow for comparison with other bio-inspired platforms. We use the developed FOM to analyze optimal gaits and compare the performance between different fin materials. The forward model demonstrates the ability to capture the highest thrust and FOM with good precision, which enables us to improve thrust generation by 83.89% and efficiency by 137.58% with proper fin stiffness and kinematics selection, allowing us to improve material selection for bio-inspired fin design. ",
    "doi": "10.3390/biomimetics9070434"
  },
  {
    "repository": "PubMed",
    "title": "Revolutionizing Reproduction: The Impact of Robotics and Artificial Intelligence (AI) in Assisted Reproductive Technology: A Comprehensive Review.",
    "authors": "Smruti A Mapari; Deepti Shrivastava; Gautam N Bedi; Utkarsh Pradeep; Aman Gupta; Paschyanti R Kasat; Pratiksha Sachani",
    "abstract": "Assisted reproductive technology (ART) has revolutionized the field of reproductive medicine, offering hope to millions of individuals and couples facing infertility challenges. In recent years, integrating robotics and artificial intelligence (AI) has emerged as a promising avenue for advancing ART. This comprehensive review explores the transformative impact of robotics and AI on ART, examining recent advancements, technological applications, clinical implications, and ethical considerations. Robotics enables precise and minimally invasive procedures, enhancing the efficiency and accuracy of various reproductive techniques such as sperm retrieval, embryo handling, and surgical interventions. Meanwhile, AI offers predictive analytics, personalized treatment protocols, and decision support systems tailored to individual patient needs, optimizing treatment outcomes and expanding access to reproductive care. Key findings highlight the significant advancements made possible by robotics and AI in ART, including improved success rates, reduced risks, and enhanced patient experience. However, challenges such as regulatory considerations, adoption barriers, and ethical dilemmas must be addressed to realize the full potential of these technologies. The transformative impact of robotics and AI on ART is profound, shaping the future of fertility treatment and family-building worldwide. Continued research, interdisciplinary collaboration, and investment are essential to further harness the potential of robotics and AI in advancing reproductive medicine and ensuring accessible, equitable, and effective care for all individuals and couples. ",
    "doi": "10.7759/cureus.63072"
  },
  {
    "repository": "PubMed",
    "title": "AI & robotics briefing: Digital primordial soup creates 'computational life'.",
    "authors": "Katrina Krämer",
    "abstract": "",
    "doi": "10.1038/d41586-024-02431-4"
  },
  {
    "repository": "PubMed",
    "title": "Asymmetric Multi-Task Learning for Interpretable Gaze-Driven Grasping Action Forecasting.",
    "authors": "Ivan Gonzalez-Diaz; Miguel Molina-Moreno; Jenny Benois-Pineau; Aymar de Rugy",
    "abstract": "This work tackles the automatic prediction of grasping intention of humans observing their environment. Our target application is the assistance to people with motor disabilities and potential cognitive impairments, using assistive robotics. Our proposal leverages the analysis of human attention captured in the form of gaze fixations recorded by an eye-tracker on the first person video, as the anticipation of prehension actions is a well studied and well known phenomenon. We propose a multi-task system that simultaneously addresses the prediction of human attention in the near future, and the anticipation of grasping actions. Visual attention is modeled as a competitive process between a discrete set of states, each one associated to a well-known gaze movement pattern from visual psychology. We additionally consider an asymmetric multi-task problem, where attention modeling is an auxiliary task that helps to regularize the learning process of the main action prediction task, and propose a constrained multi-task loss that naturally deals with this asymmetry. Our model shows superior performance than other losses for dynamic multi-task learning, current dominant deep architectures for general action forecasting and particularly-tailored models for predicting grasping intention. In particular, it provides state-of-the-art performance in three datasets for egocentric action anticipation, with an average precision of 0.569 and 0.524 in GITW and Sharon datasets, respectively, and an accuracy of 89.2% and a success rate of 51.7% in Invisible dataset. ",
    "doi": "10.1109/JBHI.2024.3430810"
  },
  {
    "repository": "PubMed",
    "title": "A Survey of Imitation Learning: Algorithms, Recent Developments, and Challenges.",
    "authors": "Maryam Zare; Parham M Kebria; Abbas Khosravi; Saeid Nahavandi",
    "abstract": "In recent years, the development of robotics and artificial intelligence (AI) systems has been nothing short of remarkable. As these systems continue to evolve, they are being utilized in increasingly complex and unstructured environments, such as autonomous driving, aerial robotics, and natural language processing. As a consequence, programming their behaviors manually or defining their behavior through the reward functions [as done in reinforcement learning (RL)] has become exceedingly difficult. This is because such environments require a high degree of flexibility and adaptability, making it challenging to specify an optimal set of rules or reward signals that can account for all the possible situations. In such environments, learning from an expert's behavior through imitation is often more appealing. This is where imitation learning (IL) comes into play - a process where desired behavior is learned by imitating an expert's behavior, which is provided through demonstrations.This article aims to provide an introduction to IL and an overview of its underlying assumptions and approaches. It also offers a detailed description of recent advances and emerging areas of research in the field. Additionally, this article discusses how researchers have addressed common challenges associated with IL and provides potential directions for future research. Overall, the goal of this article is to provide a comprehensive guide to the growing field of IL in robotics and AI. ",
    "doi": "10.1109/TCYB.2024.3395626"
  },
  {
    "repository": "PubMed",
    "title": "AI & robotics briefing: What is AI, really?",
    "authors": "Katrina Krämer",
    "abstract": "",
    "doi": "10.1038/d41586-024-02354-0"
  },
  {
    "repository": "PubMed",
    "title": "Real-time deep learning-based model predictive control of a 3-DOF biped robot leg.",
    "authors": "Haitham El-Hussieny",
    "abstract": "Our research utilized deep learning to enhance the control of a 3 Degrees of Freedom biped robot leg. We created a dynamic model based on a detailed joint angles and actuator torques dataset. This model was then integrated into a Model Predictive Control (MPC) framework, allowing for precise trajectory tracking without the need for traditional analytical dynamic models. By incorporating specific constraints within the MPC, we met operational and safety standards. The experimental results demonstrate the effectiveness of deep learning models in improving robotic control, leading to precise trajectory tracking and suggesting potential for further integration of deep learning into robotic system control. This approach not only outperforms traditional control methods in accuracy and efficiency but also opens the way for new research in robotics, highlighting the potential of utilizing deep learning models in predictive control techniques. ",
    "doi": "10.1038/s41598-024-66104-y"
  },
  {
    "repository": "PubMed",
    "title": "How AI and Robotics Will Advance Interventional Radiology: Narrative Review and Future Perspectives.",
    "authors": "Jiaming Zhang; Jiayi Fang; Yanneng Xu; Guangyan Si",
    "abstract": "The rapid advancement of artificial intelligence (AI) and robotics has led to significant progress in various medical fields including interventional radiology (IR). This review focuses on the research progress and applications of AI and robotics in IR, including deep learning (DL), machine learning (ML), and convolutional neural networks (CNNs) across specialties such as oncology, neurology, and cardiology, aiming to explore potential directions in future interventional treatments. To ensure the breadth and depth of this review, we implemented a systematic literature search strategy, selecting research published within the last five years. We conducted searches in databases such as PubMed and Google Scholar to find relevant literature. Special emphasis was placed on selecting large-scale studies to ensure the comprehensiveness and reliability of the results. This review summarizes the latest research directions and developments, ultimately analyzing their corresponding potential and limitations. It furnishes essential information and insights for researchers, clinicians, and policymakers, potentially propelling advancements and innovations within the domains of AI and IR. Finally, our findings indicate that although AI and robotics technologies are not yet widely applied in clinical settings, they are evolving across multiple aspects and are expected to significantly improve the processes and efficacy of interventional treatments. ",
    "doi": "10.3390/diagnostics14131393"
  },
  {
    "repository": "PubMed",
    "title": "Glove-Net: Enhancing Grasp Classification with Multisensory Data and Deep Learning Approach.",
    "authors": "Subhash Pratap; Jyotindra Narayan; Yoshiyuki Hatta; Kazuaki Ito; Shyamanta M Hazarika",
    "abstract": "Grasp classification is pivotal for understanding human interactions with objects, with wide-ranging applications in robotics, prosthetics, and rehabilitation. This study introduces a novel methodology utilizing a multisensory data glove to capture intricate grasp dynamics, including finger posture bending angles and fingertip forces. Our dataset comprises data collected from 10 participants engaging in grasp trials with 24 objects using the YCB object set. We evaluate classification performance under three scenarios: utilizing grasp posture alone, utilizing grasp force alone, and combining both modalities. We propose Glove-Net, a hybrid CNN-BiLSTM architecture for classifying grasp patterns within our dataset, aiming to harness the unique advantages offered by both CNNs and BiLSTM networks. This model seamlessly integrates CNNs' spatial feature extraction capabilities with the temporal sequence learning strengths inherent in BiLSTM networks, effectively addressing the intricate dependencies present within our grasping data. Our study includes findings from an extensive ablation study aimed at optimizing model configurations and hyperparameters. We quantify and compare the classification accuracy across these scenarios: CNN achieved 88.09%, 69.38%, and 93.51% testing accuracies for posture-only, force-only, and combined data, respectively. LSTM exhibited accuracies of 86.02%, 70.52%, and 92.19% for the same scenarios. Notably, the hybrid CNN-BiLSTM proposed model demonstrated superior performance with accuracies of 90.83%, 73.12%, and 98.75% across the respective scenarios. Through rigorous numerical experimentation, our results underscore the significance of multimodal grasp classification and highlight the efficacy of the proposed hybrid Glove-Net architectures in leveraging multisensory data for precise grasp recognition. These insights advance understanding of human-machine interaction and hold promise for diverse real-world applications. ",
    "doi": "10.3390/s24134378"
  },
  {
    "repository": "PubMed",
    "title": "AI & robotics briefing: Pathology chatbot can diagnose disease and talk about tumours.",
    "authors": "Katrina Krämer",
    "abstract": "",
    "doi": "10.1038/d41586-024-02207-w"
  },
  {
    "repository": "PubMed",
    "title": "AI & robotics briefing: How to beat 'superhuman' AI.",
    "authors": "Katrina Krämer",
    "abstract": "",
    "doi": "10.1038/d41586-024-02283-y"
  },
  {
    "repository": "PubMed",
    "title": "Dorsiflexion Specific Ankle Robotics to Enhance Motor Learning After Stroke: A Preliminary Report.",
    "authors": "Anindo Roy; Bradley Hennessie; Charlene Hafer-Macko; Kelly Westlake; Richard Macko",
    "abstract": "Robotics has emerged as a promising avenue for gait retraining of persons with chronic hemiparetic gait and footdrop, yet there is a gap regarding the biomechanical adaptations that occur with locomotor learning. We developed an ankle exoskeleton (AMBLE) enabling dorsiflexion assist-as-needed across gait cycle sub-events to train and study the biomechanics of motor learning stroke. This single-armed, non-controlled study investigates effects of nine hours (9 weeks × 2 sessions/week) locomotor task-specific ankle robotics training on gait biomechanics and functional mobility in persons with chronic hemiparetic gait and foot drop. Subjects include N = 16 participants (8 male, 8 female) age 53 ± 12 years with mean 11 ± 8 years since stroke. All baseline and post-training outcomes including optical motion capture for 3-D gait biomechanics are conducted during unassisted (no robot) over-ground walking conditions. Robotics training with AMBLE produced significant kinematic improvements in ankle peak dorsiflexion angular velocity (°/s, + 44 [49%], p < 0.05), heel-first foot strikes (%steps, + 14 [15%], p < 0.01) toe-off angle (°, + 83[162%], p < 0.05), and paretic knee flexion (°, + 20 [30%], p < 0.05). Improvements in gait temporal-spatial parameters include increased paretic step length (cm, + 12 [20%], p< 0.05), reduced paretic swing duration (%GC, -3[6%], p < 0.05), and trend toward improved step length symmetry (-16 [11%], p = 0.08). Functional improvements include 10-meter comfortable (m/s, + 13 [16%], p < 0.01) and fastest (m/s, + 13 [15%], p<0.01) walking velocities, 6-minute timed walk distance (m, + 16 [19%], p < 0.01) and Dynamic Gait Index scores (+15 [15%], p < 0.01). Subjects' perceived improvements surpassed the minimal clinically important difference on the Stroke Impact Scale (SIS) mobility subscale (+11 [19%], p < 0.05). AMBLE training improves paretic ankle neuromotor control, paretic knee flexion, and gait temporal-distance parameters during unassisted over-ground walking in persons with chronic stroke and foot drop. This locomotor learning indexed by an increase in volitional autonomous (non-robotic) control of paretic ankle across training translated to improvements in functional mobility outcomes. Larger randomized clinical trials are needed to investigate the effectiveness of task-specific ankle robotics, and precise training characteristics to durably improve gait, balance, and home and community-based functional mobility for persons with hemiparetic gait and foot drop. NCT04594837. ",
    "doi": "10.21203/rs.3.rs-4390770/v1"
  },
  {
    "repository": "PubMed",
    "title": "Enhancing unmanned ground vehicle performance in SAR operations: integrated gesture-control and deep learning framework for optimised victim detection.",
    "authors": "Muhammad Hamza Zafar; Syed Kumayl Raza Moosavi; Filippo Sanfilippo",
    "abstract": "In this study, we address the critical need for enhanced situational awareness and victim detection capabilities in Search and Rescue (SAR) operations amidst disasters. Traditional unmanned ground vehicles (UGVs) often struggle in such chaotic environments due to their limited manoeuvrability and the challenge of distinguishing victims from debris. Recognising these gaps, our research introduces a novel technological framework that integrates advanced gesture-recognition with cutting-edge deep learning for camera-based victim identification, specifically designed to empower UGVs in disaster scenarios. At the core of our methodology is the development and implementation of the Meerkat Optimization Algorithm-Stacked Convolutional Neural Network-Bi-Long Short Term Memory-Gated Recurrent Unit (MOA-SConv-Bi-LSTM-GRU) model, which sets a new benchmark for hand gesture detection with its remarkable performance metrics: accuracy, precision, recall, and F1-score all approximately 0.9866. This model enables intuitive, real-time control of UGVs through hand gestures, allowing for precise navigation in confined and obstacle-ridden spaces, which is vital for effective SAR operations. Furthermore, we leverage the capabilities of the latest YOLOv8 deep learning model, trained on specialised datasets to accurately detect human victims under a wide range of challenging conditions, such as varying occlusions, lighting, and perspectives. Our comprehensive testing in simulated emergency scenarios validates the effectiveness of our integrated approach. The system demonstrated exceptional proficiency in navigating through obstructions and rapidly locating victims, even in environments with visual impairments like smoke, clutter, and poor lighting. Our study not only highlights the critical gaps in current SAR response capabilities but also offers a pioneering solution through a synergistic blend of gesture-based control, deep learning, and purpose-built robotics. The key findings underscore the potential of our integrated technological framework to significantly enhance UGV performance in disaster scenarios, thereby optimising life-saving outcomes when time is of the essence. This research paves the way for future advancements in SAR technology, with the promise of more efficient and reliable rescue operations in the face of disaster. ",
    "doi": "10.3389/frobt.2024.1356345"
  },
  {
    "repository": "PubMed",
    "title": "Artificial Intelligence and Machine Learning in Neuroregeneration: A Systematic Review.",
    "authors": "Rajendra P Mulpuri; Nikhitha Konda; Sai T Gadde; Sridhar Amalakanti; Sindhu Chowdary Valiveti",
    "abstract": "Artificial intelligence (AI) and machine learning (ML) show promise in various medical domains, including medical imaging, precise diagnoses, and pharmaceutical research. In neuroscience and neurosurgery, AI/ML advancements enhance brain-computer interfaces, neuroprosthetics, and surgical planning. They are poised to revolutionize neuroregeneration by unraveling the nervous system's complexities. However, research on AI/ML in neuroregeneration is fragmented, necessitating a comprehensive review. Adhering to Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) recommendations, 19 English-language papers focusing on AI/ML in neuroregeneration were selected from a total of 247. Two researchers independently conducted data extraction and quality assessment using the Mixed Methods Appraisal Tool (MMAT) 2018. Eight studies were deemed high quality, 10 moderate, and four low. Primary goals included diagnosing neurological disorders (35%), robotic rehabilitation (18%), and drug discovery (12% each). Methods ranged from analyzing imaging data (24%) to animal models (24%) and electronic health records (12%). Deep learning accounted for 41% of AI/ML techniques, while standard ML algorithms constituted 29%. The review underscores the growing interest in AI/ML for neuroregenerative medicine, with increasing publications. These technologies aid in diagnosing diseases and facilitating functional recovery through robotics and targeted stimulation. AI-driven drug discovery holds promise for identifying neuroregenerative therapies. Nonetheless, addressing existing limitations remains crucial in this rapidly evolving field. ",
    "doi": "10.7759/cureus.61400"
  },
  {
    "repository": "PubMed",
    "title": "Contextual emotion detection in images using deep learning.",
    "authors": "Fatiha Limami; Boutaina Hdioud; Rachid Oulad Haj Thami",
    "abstract": "Computerized sentiment detection, based on artificial intelligence and computer vision, has become essential in recent years. Thanks to developments in deep neural networks, this technology can now account for environmental, social, and cultural factors, as well as facial expressions. We aim to create more empathetic systems for various purposes, from medicine to interpreting emotional interactions on social media. To develop this technology, we combined authentic images from various databases, including EMOTIC (ADE20K, MSCOCO), EMODB_SMALL, and FRAMESDB, to train our models. We developed two sophisticated algorithms based on deep learning techniques, DCNN and VGG19. By optimizing the hyperparameters of our models, we analyze context and body language to improve our understanding of human emotions in images. We merge the 26 discrete emotional categories with the three continuous emotional dimensions to identify emotions in context. The proposed pipeline is completed by fusing our models. We adjusted the parameters to outperform previous methods in capturing various emotions in different contexts. Our study showed that the Sentiment_recognition_model and VGG19_contexte increased mAP by 42.81% and 44.12%, respectively, surpassing the results of previous studies. This groundbreaking research could significantly improve contextual emotion recognition in images. The implications of these promising results are far-reaching, extending to diverse fields such as social robotics, affective computing, human-machine interaction, and human-robot communication. ",
    "doi": "10.3389/frai.2024.1386753"
  },
  {
    "repository": "PubMed",
    "title": "ARBUR, a machine learning-based analysis system for relating behaviors and ultrasonic vocalizations of rats.",
    "authors": "Zhe Chen; Guanglu Jia; Qijie Zhou; Yulai Zhang; Zhenzhen Quan; Xuechao Chen; Toshio Fukuda; Qiang Huang; Qing Shi",
    "abstract": "Deciphering how different behaviors and ultrasonic vocalizations (USVs) of rats interact can yield insights into the neural basis of social interaction. However, the behavior-vocalization interplay of rats remains elusive because of the challenges of relating the two communication media in complex social contexts. Here, we propose a machine learning-based analysis system (ARBUR) that can cluster without bias both non-step (continuous) and step USVs, hierarchically detect eight types of behavior of two freely behaving rats with high accuracy, and locate the vocal rat in 3-D space. ARBUR reveals that rats communicate via distinct USVs during different behaviors. Moreover, we show that ARBUR can indicate findings that are long neglected by former manual analysis, especially regarding the non-continuous USVs during easy-to-confuse social behaviors. This work could help mechanistically understand the behavior-vocalization interplay of rats and highlights the potential of machine learning algorithms in automatic animal behavioral and acoustic analysis. ",
    "doi": "10.1016/j.isci.2024.109998"
  },
  {
    "repository": "PubMed",
    "title": "Magnetic-Controlled Microrobot: Real-Time Detection and Tracking through Deep Learning Approaches.",
    "authors": "Hao Li; Xin Yi; Zhaopeng Zhang; Yuan Chen",
    "abstract": "As one of the most significant research topics in robotics, microrobots hold great promise in biomedicine for applications such as targeted diagnosis, targeted drug delivery, and minimally invasive treatment. This paper proposes an enhanced YOLOv5 (You Only Look Once version 5) microrobot detection and tracking system (MDTS), incorporating a visual tracking algorithm to elevate the precision of small-target detection and tracking. The improved YOLOv5 network structure is used to take magnetic bodies with sizes of 3 mm and 1 mm and a magnetic microrobot with a length of 2 mm as the pretraining targets, and the training weight model is used to obtain the position information and motion information of the microrobot in real time. The experimental results show that the accuracy of the improved network model for magnetic bodies with a size of 3 mm is 95.81%, representing an increase of 2.1%; for magnetic bodies with a size of 1 mm, the accuracy is 91.03%, representing an increase of 1.33%; and for microrobots with a length of 2 mm, the accuracy is 91.7%, representing an increase of 1.5%. The combination of the improved YOLOv5 network model and the vision algorithm can effectively realize the real-time detection and tracking of magnetically controlled microrobots. Finally, 2D and 3D detection and tracking experiments relating to microrobots are designed to verify the robustness and effectiveness of the system, which provides strong support for the operation and control of microrobots in an in vivo environment. ",
    "doi": "10.3390/mi15060756"
  },
  {
    "repository": "PubMed",
    "title": "AI & robotics briefing: Tech giants are 'open-washing' their AI models.",
    "authors": "Katrina Krämer",
    "abstract": "",
    "doi": "10.1038/d41586-024-02122-0"
  },
  {
    "repository": "PubMed",
    "title": "Evaluating the Efficacy and Accuracy of AI-Assisted Diagnostic Techniques in Endometrial Carcinoma: A Systematic Review.",
    "authors": "Jawaria Changhez; Simran James; Fazilat Jamala; Shandana Khan; Muhammad Zarak Khan; Sana Gul; Irta Zainab",
    "abstract": "Diagnosing endometrial carcinoma correctly is essential for appropriate treatment, as it is a major health risk. As machine learning (ML) and artificial intelligence (AI) have grown in popularity, so has interest in their potential to improve cancer diagnosis accuracy. In the context of endometrial cancer, this study attempts to examine the efficacy as well as the accuracy of AI-assisted diagnostic approaches. Additionally, it aims to methodically evaluate the contribution of AI and ML techniques to the improvement of endometrial cancer diagnosis. Following PRISMA guidelines, we performed a thorough search of numerous databases, including Medline via Ovid, PubMed, Scopus, Web of Science, and Google Scholar. Ten years were searched, encompassing both basic and advanced research. Peer-reviewed papers and original research studies that explicitly looked at the application of AI/ML in endometrial cancer diagnosis were the main targets of the well-defined selection criteria. Using the Critical Appraisal Skills Programme (CASP) methodology, two independent researchers conducted a thorough screening process and quality assessment of included studies. The review found a notable inclination towards the effective use of AI in endometrial carcinoma diagnostics, namely in the identification and categorization of endometrial cancer. Artificial intelligence models, particularly Convolutional Neural Networks (CNNs) and deep learning algorithms have shown remarkable precision in detecting endometrial cancer. They frequently achieve or even exceed the diagnostic proficiency of human specialists. The use of artificial intelligence in medical diagnostics signifies revolutionary progress in the field of oncology. AI-assisted diagnostic tools have demonstrated the potential to improve the precision and effectiveness of cancer diagnosis, namely in cases of endometrial carcinoma. This innovation not only enhances the quality of patient care but also indicates a transition towards more individualized and efficient treatment approaches in the field of oncology. The advancement of AI technology is expected to play a crucial role in medical diagnostics, particularly in the field of cancer detection and treatment, perhaps leading to a significant transformation in the approach to these areas. ",
    "doi": "10.7759/cureus.60973"
  },
  {
    "repository": "PubMed",
    "title": "AI & robotics briefing: How AI referees are shaking up football.",
    "authors": "Katrina Krämer",
    "abstract": "",
    "doi": "10.1038/d41586-024-02064-7"
  },
  {
    "repository": "PubMed",
    "title": "Recent advances in flexible hydrogel sensors: Enhancing data processing and machine learning for intelligent perception.",
    "authors": "Derrick Boateng; Xukai Li; Yuhan Zhu; Hao Zhang; Meng Wu; Jifang Liu; Yan Kang; Hongbo Zeng; Linbo Han",
    "abstract": "With the advent of flexible electronics and sensing technology, hydrogel-based flexible sensors have exhibited considerable potential across a diverse range of applications, including wearable electronics and soft robotics. Recently, advanced machine learning (ML) algorithms have been integrated into flexible hydrogel sensing technology to enhance their data processing capabilities and to achieve intelligent perception. However, there are no reviews specifically focusing on the data processing steps and analysis based on the raw sensing data obtained by flexible hydrogel sensors. Here we provide a comprehensive review of the latest advancements and breakthroughs in intelligent perception achieved through the fusion of ML algorithms with flexible hydrogel sensors, across various applications. Moreover, this review thoroughly examines the data processing techniques employed in flexible hydrogel sensors, offering valuable perspectives expected to drive future data-driven applications in this field. ",
    "doi": "10.1016/j.bios.2024.116499"
  },
  {
    "repository": "PubMed",
    "title": "Learning Soft Millirobot Multimodal Locomotion with Sim-to-Real Transfer.",
    "authors": "Sinan Ozgun Demir; Mehmet Efe Tiryaki; Alp Can Karacakol; Metin Sitti",
    "abstract": "With wireless multimodal locomotion capabilities, magnetic soft millirobots have emerged as potential minimally invasive medical robotic platforms. Due to their diverse shape programming capability, they can generate various locomotion modes, and their locomotion can be adapted to different environments by controlling the external magnetic field signal. Existing adaptation methods, however, are based on hand-tuned signals. Here, a learning-based adaptive magnetic soft millirobot multimodal locomotion framework empowered by sim-to-real transfer is presented. Developing a data-driven magnetic soft millirobot simulation environment, the periodic magnetic actuation signal is learned for a given soft millirobot in simulation. Then, the learned locomotion strategy is deployed to the real world using Bayesian optimization and Gaussian processes. Finally, automated domain recognition and locomotion adaptation for unknown environments using a Kullback-Leibler divergence-based probabilistic method are illustrated. This method can enable soft millirobot locomotion to quickly and continuously adapt to environmental changes and explore the actuation space for unanticipated solutions with minimum experimental cost. ",
    "doi": "10.1002/advs.202308881"
  },
  {
    "repository": "PubMed",
    "title": "Editorial: Trustworthy AI for healthcare.",
    "authors": "Oleg Agafonov; Aleksandar Babic; Sonia Sousa; Sharmini Alagaratnam",
    "abstract": "",
    "doi": "10.3389/fdgth.2024.1427233"
  },
  {
    "repository": "PubMed",
    "title": "Autonomous navigation of catheters and guidewires in mechanical thrombectomy using inverse reinforcement learning.",
    "authors": "Harry Robertshaw; Lennart Karstensen; Benjamin Jackson; Alejandro Granados; Thomas C Booth",
    "abstract": "Autonomous navigation of catheters and guidewires can enhance endovascular surgery safety and efficacy, reducing procedure times and operator radiation exposure. Integrating tele-operated robotics could widen access to time-sensitive emergency procedures like mechanical thrombectomy (MT). Reinforcement learning (RL) shows potential in endovascular navigation, yet its application encounters challenges without a reward signal. This study explores the viability of autonomous guidewire navigation in MT vasculature using inverse reinforcement learning (IRL) to leverage expert demonstrations. Employing the Simulation Open Framework Architecture (SOFA), this study established a simulation-based training and evaluation environment for MT navigation. We used IRL to infer reward functions from expert behaviour when navigating a guidewire and catheter. We utilized the soft actor-critic algorithm to train models with various reward functions and compared their performance in silico. We demonstrated feasibility of navigation using IRL. When evaluating single- versus dual-device (i.e. guidewire versus catheter and guidewire) tracking, both methods achieved high success rates of 95% and 96%, respectively. Dual tracking, however, utilized both devices mimicking an expert. A success rate of 100% and procedure time of 22.6 s were obtained when training with a reward function obtained through 'reward shaping'. This outperformed a dense reward function (96%, 24.9 s) and an IRL-derived reward function (48%, 59.2 s). We have contributed to the advancement of autonomous endovascular intervention navigation, particularly MT, by effectively employing IRL based on demonstrator expertise. The results underscore the potential of using reward shaping to efficiently train models, offering a promising avenue for enhancing the accessibility and precision of MT procedures. We envisage that future research can extend our methodology to diverse anatomical structures to enhance generalizability. ",
    "doi": "10.1007/s11548-024-03208-w"
  },
  {
    "repository": "PubMed",
    "title": "AI & robotics briefing: What would you do with a third thumb?",
    "authors": "Katrina Krämer",
    "abstract": "",
    "doi": "10.1038/d41586-024-01686-1"
  },
  {
    "repository": "PubMed",
    "title": "AI & robotics briefing: The year machines will run out of data.",
    "authors": "Katrina Krämer",
    "abstract": "",
    "doi": "10.1038/d41586-024-01760-8"
  },
  {
    "repository": "PubMed",
    "title": "Real-time precision detection algorithm for jellyfish stings in neural computing, featuring adaptive deep learning enhanced by an advanced YOLOv4 framework.",
    "authors": "Chao Zhu; Hua Feng; Liang Xu",
    "abstract": "Sea jellyfish stings pose a threat to human health, and traditional detection methods face challenges in terms of accuracy and real-time capabilities. To address this, we propose a novel algorithm that integrates YOLOv4 object detection, an attention mechanism, and PID control. We enhance YOLOv4 to improve the accuracy and real-time performance of detection. Additionally, we introduce an attention mechanism to automatically focus on critical areas of sea jellyfish stings, enhancing detection precision. Ultimately, utilizing the PID control algorithm, we achieve adaptive adjustments in the robot's movements and posture based on the detection results. Extensive experimental evaluations using a real sea jellyfish sting image dataset demonstrate significant improvements in accuracy and real-time performance using our proposed algorithm. Compared to traditional methods, our algorithm more accurately detects sea jellyfish stings and dynamically adjusts the robot's actions in real-time, maximizing protection for human health. The significance of this research lies in providing an efficient and accurate sea jellyfish sting detection algorithm for intelligent robot systems. The algorithm exhibits notable improvements in real-time capabilities and precision, aiding robot systems in better identifying and addressing sea jellyfish stings, thereby safeguarding human health. Moreover, the algorithm possesses a certain level of generality and can be applied to other applications in target detection and adaptive control, offering broad prospects for diverse applications. ",
    "doi": "10.3389/fnbot.2024.1375886"
  },
  {
    "repository": "PubMed",
    "title": "Bio-inspired multimodal learning with organic neuromorphic electronics for behavioral conditioning in robotics.",
    "authors": "Imke Krauhausen; Sophie Griggs; Iain McCulloch; Jaap M J den Toonder; Paschalis Gkoupidenis; Yoeri van de Burgt",
    "abstract": "Biological systems interact directly with the environment and learn by receiving multimodal feedback via sensory stimuli that shape the formation of internal neuronal representations. Drawing inspiration from biological concepts such as exploration and sensory processing that eventually lead to behavioral conditioning, we present a robotic system handling objects through multimodal learning. A small-scale organic neuromorphic circuit locally integrates and adaptively processes multimodal sensory stimuli, enabling the robot to interact intelligently with its surroundings. The real-time handling of sensory stimuli via low-voltage organic neuromorphic devices with synaptic functionality forms multimodal associative connections that lead to behavioral conditioning, and thus the robot learns to avoid potentially dangerous objects. This work demonstrates that adaptive neuro-inspired circuitry with multifunctional organic materials, can accommodate locally efficient bio-inspired learning for advancing intelligent robotics. ",
    "doi": "10.1038/s41467-024-48881-2"
  },
  {
    "repository": "PubMed",
    "title": "The educational impact of technology-enhanced learning in regional anaesthesia: a scoping review.",
    "authors": "Mairead Savage; Andrew Spence; Lloyd Turbitt",
    "abstract": "Effective training in regional anaesthesia (RA) is paramount to ensuring widespread competence. Technology-based learning has assisted other specialties in achieving more rapid procedural skill acquisition. If applicable to RA, technology-enhanced training has the potential to provide an effective learning experience and to overcome barriers to RA training. We review the current evidence base for use of innovative technologies in assisting learning of RA. Using scoping review methodology, three databases (MEDLINE, Embase, and Web of Science) were searched, identifying 158 relevant citations. Citations were screened against defined eligibility criteria with 27 studies selected for inclusion. Data relating to study details, technological learning interventions, and impact on learner experience were extracted and analysed. Seven different technologies were used to train learners in RA: artificial intelligence, immersive virtual reality, desktop virtual reality, needle guidance technology, robotics, augmented reality, and haptic feedback devices. Of 27 studies, 26 reported a positive impact of technology-enhanced RA training, with different technologies offering benefits for differing components of RA training. Artificial intelligence improved sonoanatomical knowledge and ultrasound skills for RA, whereas needle guidance technologies enhanced confidence and improved needling performance, particularly in novices. Immersive virtual reality allowed more rapid acquisition of needling skills, but its functionality was limited when combined with haptic feedback technology. User friendly technologies enhanced participant experience and improved confidence in RA; however, limitations in technology-assisted RA training restrict its widespread use. Technology-enhanced RA training can provide a positive and effective learning experience, with potential to reduce the steep learning curve associated with gaining RA proficiency. A combined approach to RA education, using both technological and traditional approaches, should be maintained as no single method has been shown to provide comprehensive RA training. ",
    "doi": "10.1016/j.bja.2024.04.045"
  },
  {
    "repository": "PubMed",
    "title": "AI & robotics briefing: AI decodes languages in first 'bilingual' brain-reading device.",
    "authors": "Katrina Krämer",
    "abstract": "",
    "doi": "10.1038/d41586-024-01606-3"
  },
  {
    "repository": "PubMed",
    "title": "How AI could improve robotics, the cockroach's origins, and promethium spills its secrets.",
    "authors": "Benjamin Thompson; Elizabeth Gibney; Flora Graham",
    "abstract": "",
    "doi": "10.1038/d41586-024-01610-7"
  },
  {
    "repository": "PubMed",
    "title": "Advances in the Application of AI Robots in Critical Care: Scoping Review.",
    "authors": "Yun Li; Min Wang; Lu Wang; Yuan Cao; Yuyan Liu; Yan Zhao; Rui Yuan; Mengmeng Yang; Siqian Lu; Zhichao Sun; Feihu Zhou; Zhirong Qian; Hongjun Kang",
    "abstract": "In recent epochs, the field of critical medicine has experienced significant advancements due to the integration of artificial intelligence (AI). Specifically, AI robots have evolved from theoretical concepts to being actively implemented in clinical trials and applications. The intensive care unit (ICU), known for its reliance on a vast amount of medical information, presents a promising avenue for the deployment of robotic AI, anticipated to bring substantial improvements to patient care. This review aims to comprehensively summarize the current state of AI robots in the field of critical care by searching for previous studies, developments, and applications of AI robots related to ICU wards. In addition, it seeks to address the ethical challenges arising from their use, including concerns related to safety, patient privacy, responsibility delineation, and cost-benefit analysis. Following the scoping review framework proposed by Arksey and O'Malley and the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) guidelines, we conducted a scoping review to delineate the breadth of research in this field of AI robots in ICU and reported the findings. The literature search was carried out on May 1, 2023, across 3 databases: PubMed, Embase, and the IEEE Xplore Digital Library. Eligible publications were initially screened based on their titles and abstracts. Publications that passed the preliminary screening underwent a comprehensive review. Various research characteristics were extracted, summarized, and analyzed from the final publications. Of the 5908 publications screened, 77 (1.3%) underwent a full review. These studies collectively spanned 21 ICU robotics projects, encompassing their system development and testing, clinical trials, and approval processes. Upon an expert-reviewed classification framework, these were categorized into 5 main types: therapeutic assistance robots, nursing assistance robots, rehabilitation assistance robots, telepresence robots, and logistics and disinfection robots. Most of these are already widely deployed and commercialized in ICUs, although a select few remain under testing. All robotic systems and tools are engineered to deliver more personalized, convenient, and intelligent medical services to patients in the ICU, concurrently aiming to reduce the substantial workload on ICU medical staff and promote therapeutic and care procedures. This review further explored the prevailing challenges, particularly focusing on ethical and safety concerns, proposing viable solutions or methodologies, and illustrating the prospective capabilities and potential of AI-driven robotic technologies in the ICU environment. Ultimately, we foresee a pivotal role for robots in a future scenario of a fully automated continuum from admission to discharge within the ICU. This review highlights the potential of AI robots to transform ICU care by improving patient treatment, support, and rehabilitation processes. However, it also recognizes the ethical complexities and operational challenges that come with their implementation, offering possible solutions for future development and optimization. ",
    "doi": "10.2196/54095"
  },
  {
    "repository": "PubMed",
    "title": "Incorporating simulated spatial context information improves the effectiveness of contrastive learning models.",
    "authors": "Lizhen Zhu; James Z Wang; Wonseuk Lee; Brad Wyble",
    "abstract": "Visual learning often occurs in a specific context, where an agent acquires skills through exploration and tracking of its location in a consistent environment. The historical spatial context of the agent provides a similarity signal for self-supervised contrastive learning. We present a unique approach, termed environmental spatial similarity (ESS), that complements existing contrastive learning methods. Using images from simulated, photorealistic environments as an experimental setting, we demonstrate that ESS outperforms traditional instance discrimination approaches. Moreover, sampling additional data from the same environment substantially improves accuracy and provides new augmentations. ESS allows remarkable proficiency in room classification and spatial prediction tasks, especially in unfamiliar environments. This learning paradigm has the potential to enable rapid visual learning in agents operating in new environments with unique visual characteristics. Potentially transformative applications span from robotics to space exploration. Our proof of concept demonstrates improved efficiency over methods that rely on extensive, disconnected datasets. ",
    "doi": "10.1016/j.patter.2024.100964"
  },
  {
    "repository": "PubMed",
    "title": "Robotics, remote medicine, AI: Oh my!",
    "authors": "J Peter Weiss",
    "abstract": "",
    "doi": "10.1016/j.hrcr.2024.01.011"
  },
  {
    "repository": "PubMed",
    "title": "AI-Driven Sensing Technology: Review.",
    "authors": "Long Chen; Chenbin Xia; Zhehui Zhao; Haoran Fu; Yunmin Chen",
    "abstract": "Machine learning and deep learning technologies are rapidly advancing the capabilities of sensing technologies, bringing about significant improvements in accuracy, sensitivity, and adaptability. These advancements are making a notable impact across a broad spectrum of fields, including industrial automation, robotics, biomedical engineering, and civil infrastructure monitoring. The core of this transformative shift lies in the integration of artificial intelligence (AI) with sensor technology, focusing on the development of efficient algorithms that drive both device performance enhancements and novel applications in various biomedical and engineering fields. This review delves into the fusion of ML/DL algorithms with sensor technologies, shedding light on their profound impact on sensor design, calibration and compensation, object recognition, and behavior prediction. Through a series of exemplary applications, the review showcases the potential of AI algorithms to significantly upgrade sensor functionalities and widen their application range. Moreover, it addresses the challenges encountered in exploiting these technologies for sensing applications and offers insights into future trends and potential advancements. ",
    "doi": "10.3390/s24102958"
  },
  {
    "repository": "PubMed",
    "title": "AI & robotics briefing: How psychology and neuroscience crack open the AI 'black box'.",
    "authors": "Katrina Krämer",
    "abstract": "",
    "doi": "10.1038/d41586-024-01538-y"
  },
  {
    "repository": "PubMed",
    "title": "Philosophy of cognitive science in the age of deep learning.",
    "authors": "Raphaël Millière",
    "abstract": "Deep learning has enabled major advances across most areas of artificial intelligence research. This remarkable progress extends beyond mere engineering achievements and holds significant relevance for the philosophy of cognitive science. Deep neural networks have made significant strides in overcoming the limitations of older connectionist models that once occupied the center stage of philosophical debates about cognition. This development is directly relevant to long-standing theoretical debates in the philosophy of cognitive science. Furthermore, ongoing methodological challenges related to the comparative evaluation of deep neural networks stand to benefit greatly from interdisciplinary collaboration with philosophy and cognitive science. The time is ripe for philosophers to explore foundational issues related to deep learning and cognition; this perspective paper surveys key areas where their contributions can be especially fruitful. This article is categorized under: Philosophy > Artificial Intelligence Computer Science and Robotics > Machine Learning. ",
    "doi": "10.1002/wcs.1684"
  },
  {
    "repository": "PubMed",
    "title": "Learning Playing Piano with Bionic-Constrained Diffusion Policy for Anthropomorphic Hand.",
    "authors": "Yiming Yang; Zechang Wang; Dengpeng Xing; Peng Wang",
    "abstract": "Anthropomorphic hand manipulation is a quintessential example of embodied intelligence in robotics, presenting a notable challenge due to its high degrees of freedom and complex inter-joint coupling. Though recent advancements in reinforcement learning (RL) have led to substantial progress in this field, existing methods often overlook the detailed structural properties of anthropomorphic hands. To address this, we propose a novel deep RL approach, Bionic-Constrained Diffusion Policy (Bio-CDP), which integrates knowledge of human hand control with a powerful diffusion policy representation. Our bionic constraint modifies the action space of anthropomorphic hand control, while the diffusion policy enhances the expressibility of the policy in high-dimensional continuous control tasks. Bio-CDP has been evaluated in the simulation environment, where it has shown superior performance and data efficiency compared to state-of-the-art RL approaches. Furthermore, our method is resilient to task complexity and robust in performance, making it a promising tool for advanced control in robotics. ",
    "doi": "10.34133/cbsystems.0104"
  },
  {
    "repository": "PubMed",
    "title": "AI & robotics briefing: Why AI needs to see the 'ugly' side of science.",
    "authors": "Katrina Krämer",
    "abstract": "",
    "doi": "10.1038/d41586-024-01447-0"
  },
  {
    "repository": "PubMed",
    "title": "Learning Curves during Implementation of Robotic Stereotactic Surgery.",
    "authors": "Kevin Hines; Rupert D Smit; Shreya Vinjamuri; Arbaz A Momin; Islam Fayed; Kenechi Ebede; Ahmet F Atik; Caio Marconato Matias; Ashwini Sharan; Chengyuan Wu",
    "abstract": "Adoption of robotic techniques is increasing for neurosurgical applications. Common cranial applications include stereoelectroencephalography (sEEG) and deep brain stimulation (DBS). For surgeons to implement robotic techniques in these procedures, realistic learning curves must be anticipated for surgeons to overcome the challenges of integrating new techniques into surgical workflow. One such way of quantifying learning curves in surgery is cumulative sum (CUSUM) analysis. Here, the authors present retrospective review of stereotactic cases to perform a CUSUM analysis of operative time for robotic cases at a single institution performed by 2 surgeons. The authors demonstrate learning phase durations of 20 and 16 cases in DBS and sEEG, respectively. After plateauing of operative time, mastery phases started at cases 132 and 72 in DBS and sEEG. A total of 273 cases (188 DBS and 85 sEEG) were included in the study. The authors observed a learning plateau concordant with change of location of surgery after exiting the learning phase. This study demonstrates the learning curve of 2 stereotactic workflows when integrating robotics as well as being the first study to examine the robotic learning curve in DBS via CUSUM analysis. This work provides data on what surgeons may expect when integrating this technology into their practice for cranial applications. ",
    "doi": "10.1159/000538379"
  },
  {
    "repository": "PubMed",
    "title": "Dynamic Occupancy Grid Map with Semantic Information Using Deep Learning-Based BEVFusion Method with Camera and LiDAR Fusion.",
    "authors": "Harin Jang; Taehyun Kim; Kyungjae Ahn; Soo Jeon; Yeonsik Kang",
    "abstract": "In the field of robotics and autonomous driving, dynamic occupancy grid maps (DOGMs) are typically used to represent the position and velocity information of objects. Although three-dimensional light detection and ranging (LiDAR) sensor-based DOGMs have been actively researched, they have limitations, as they cannot classify types of objects. Therefore, in this study, a deep learning-based camera-LiDAR sensor fusion technique is employed as input to DOGMs. Consequently, not only the position and velocity information of objects but also their class information can be updated, expanding the application areas of DOGMs. Moreover, unclassified LiDAR point measurements contribute to the formation of a map of the surrounding environment, improving the reliability of perception by registering objects that were not classified by deep learning. To achieve this, we developed update rules on the basis of the Dempster-Shafer evidence theory, incorporating class information and the uncertainty of objects occupying grid cells. Furthermore, we analyzed the accuracy of the velocity estimation using two update models. One assigns the occupancy probability only to the edges of the oriented bounding box, whereas the other assigns the occupancy probability to the entire area of the box. The performance of the developed perception technique is evaluated using the public nuScenes dataset. The developed DOGM with object class information will help autonomous vehicles to navigate in complex urban driving environments by providing them with rich information, such as the class and velocity of nearby obstacles. ",
    "doi": "10.3390/s24092828"
  },
  {
    "repository": "PubMed",
    "title": "Bimanual coordinated motor skill learning in patients with a chronic cerebellar stroke.",
    "authors": "Estelle Gathy; Ninon Cadiat; Eloïse Gerardin; Julien Lambert; Benoît Herman; Mie Leeuwerck; Benoît Bihin; Yves Vandermeeren",
    "abstract": "Cerebellar strokes induce coordination disorders that can affect activities of daily living. Evidence-based neurorehabilitation programs are founded on motor learning principles. The cerebellum is a key neural structure in motor learning. It is unknown whether and how well chronic cerebellar stroke individuals (CCSIs) can learn to coordinate their upper limbs through bimanual motor skill learning. The aim was to determine whether CCSIs could achieve bimanual skill learning through a serious game with the REAplan® robot and to compare CCSIs with healthy individuals (HIs). Over three consecutive days, sixteen CCSIs and eighteen HIs were trained on an asymmetric bimanual coordination task (\"\"CIRCUIT\"\" game) with the REAplan® robot, allowing quantification of speed, accuracy and coordination. The primary outcomes were the bimanual speed/accuracy trade-off (BiSAT) and bimanual coordination factor (BiCo). They were also evaluated on a bimanual REACHING task on Days 1 and 3. Correlation analyses between the robotic outcomes and clinical scale scores were computed. Throughout the sessions, BiSAT and BiCo improved during the CIRCUIT task in both HIs and CCSIs. On Day 3, HIs and CCSIs showed generalization of BiSAT, BiCo and transferred to the REACHING task. There was no significant between-group difference in progression. Four CCSIs and two HIs were categorized as \"\"poor learners\"\" according to BiSAT and/or BiCo. Increasing age correlated with reduced BiSAT but not BiCo progression. Over three days of training, HIs and CCSIs improved, retained, generalized and transferred a coordinated bimanual skill. There was no between-group difference, suggesting plastic compensation in CCSIs. Clinical trial NCT04642599 approved the 24th of November 2020. ",
    "doi": "10.1007/s00221-024-06830-x"
  },
  {
    "repository": "PubMed",
    "title": "AI & robotics briefing: 'Risk-of-death' AI save lives in hospital trial.",
    "authors": "Katrina Krämer",
    "abstract": "",
    "doi": "10.1038/d41586-024-01382-0"
  },
  {
    "repository": "PubMed",
    "title": "Learning Curve of Robotic End-to-Side Microanastomoses.",
    "authors": "Corinne Rabbin-Birnbaum; Daniel D Wiggan; Karl L Sangwon; Bruck Negash; Eleanor Gutstadt; Caleb Rutledge; Jacob Baranoski; Eytan Raz; Maksim Shapiro; Vera Sharashidze; Howard A Riina; Peter Kim Nelson; Albert Liu; Osamah Choudhry; Erez Nossek",
    "abstract": "Robotics are becoming increasingly widespread within various neurosurgical subspecialties, but data pertaining to their feasibility in vascular neurosurgery are limited. We present our novel attempt to evaluate the learning curve of a robotic platform for microvascular anastomoses. One hundred and sixty one sutures were performed and assessed. Fourteen anastomoses (10 robotic [MUSA-2 Microsurgical system; Microsure] and 4 hand-sewn) were performed by the senior author on 1.5-mm caliber tubes and recorded with the Kinevo 900 (Zeiss) operative microscope. We separately compared interrupted sutures (from needle insertion until third knot) and running sutures (from needle insertion until loop pull-down). Average suture timing across all groups was compared using an unpaired Student's t test. Exponential smoothing (α = 0.2) was then applied to the robotic data sets for validation and a second set of t tests were performed. We compared 107 robotic sutures with 54 hand-sewn sutures. There was a significant difference between the average time/stitch for the robotic running sutures (n = 55) and the hand-sewn running sutures (n = 31) (31.2 seconds vs 48.3 seconds, respectively; P -value = .00052). Exponential smoothing (α = 0.2) reinforced these results (37.6 seconds vs 48.3 seconds; P -value = .014625). Average robotic running times surpassed hand-sewn by the second anastomosis (38.8 seconds vs 48.3 seconds) and continued to steadily decrease with subsequent stitches. The average of the robotic interrupted sutures (n = 52) was significantly longer than the hand-sewn (n = 23) (171.3 seconds vs 70 seconds; P = .000024). Exponential smoothing (α = 0.2) yielded similar results (196.7 seconds vs 70 seconds; P = .00001). However, average robotic interrupted times significantly decreased from the first to the final anastomosis (286 seconds vs 105.2 seconds; P = .003674). Our results indicate the learning curve for robotic microanastomoses is short and encouraging. The use of robotics warrants further study for potential use in cerebrovascular bypass procedures. ",
    "doi": "10.1227/ons.0000000000001187"
  },
  {
    "repository": "PubMed",
    "title": "AI & robotics briefing: What running robots tell us about gaits.",
    "authors": "Katrina Krämer",
    "abstract": "",
    "doi": "10.1038/d41586-024-01307-x"
  },
  {
    "repository": "PubMed",
    "title": "Reinforcement Learning Algorithms and Applications in Healthcare and Robotics: A Comprehensive and Systematic Review.",
    "authors": "Mokhaled N A Al-Hamadani; Mohammed A Fadhel; Laith Alzubaidi; Harangi Balazs",
    "abstract": "Reinforcement learning (RL) has emerged as a dynamic and transformative paradigm in artificial intelligence, offering the promise of intelligent decision-making in complex and dynamic environments. This unique feature enables RL to address sequential decision-making problems with simultaneous sampling, evaluation, and feedback. As a result, RL techniques have become suitable candidates for developing powerful solutions in various domains. In this study, we present a comprehensive and systematic review of RL algorithms and applications. This review commences with an exploration of the foundations of RL and proceeds to examine each algorithm in detail, concluding with a comparative analysis of RL algorithms based on several criteria. This review then extends to two key applications of RL: robotics and healthcare. In robotics manipulation, RL enhances precision and adaptability in tasks such as object grasping and autonomous learning. In healthcare, this review turns its focus to the realm of cell growth problems, clarifying how RL has provided a data-driven approach for optimizing the growth of cell cultures and the development of therapeutic solutions. This review offers a comprehensive overview, shedding light on the evolving landscape of RL and its potential in two diverse yet interconnected fields. ",
    "doi": "10.3390/s24082461"
  },
  {
    "repository": "PubMed",
    "title": "A Novel Obstacle Traversal Method for Multiple Robotic Fish Based on Cross-Modal Variational Autoencoders and Imitation Learning.",
    "authors": "Ruilong Wang; Ming Wang; Qianchuan Zhao; Yanling Gong; Lingchen Zuo; Xuehan Zheng; He Gao",
    "abstract": "Precision control of multiple robotic fish visual navigation in complex underwater environments has long been a challenging issue in the field of underwater robotics. To address this problem, this paper proposes a multi-robot fish obstacle traversal technique based on the combination of cross-modal variational autoencoder (CM-VAE) and imitation learning. Firstly, the overall framework of the robotic fish control system is introduced, where the first-person view of the robotic fish is encoded into a low-dimensional latent space using CM-VAE, and then different latent features in the space are mapped to the velocity commands of the robotic fish through imitation learning. Finally, to validate the effectiveness of the proposed method, experiments are conducted on linear, S-shaped, and circular gate frame trajectories with both single and multiple robotic fish. Analysis reveals that the visual navigation method proposed in this paper can stably traverse various types of gate frame trajectories. Compared to end-to-end learning and purely unsupervised image reconstruction, the proposed control strategy demonstrates superior performance, offering a new solution for the intelligent navigation of robotic fish in complex environments. ",
    "doi": "10.3390/biomimetics9040221"
  },
  {
    "repository": "PubMed",
    "title": "AI & robotics briefing: Winged robot demystifies insect flight.",
    "authors": "Katrina Krämer",
    "abstract": "",
    "doi": "10.1038/d41586-024-01224-z"
  },
  {
    "repository": "PubMed",
    "title": "Three-arm robotic cholecystectomy: a novel, cost-effective method of delivering and learning robotic surgery in upper GI surgery.",
    "authors": "Gijs I van Boxel; Nicholas C Carter; Veronika Fajksova",
    "abstract": "Cholecystectomy is one of the commonest performed surgeries worldwide. With the introduction of robotic surgery, the numbers of robot-assisted cholecystectomies has risen over the past decade. Despite the proven use of this procedure as a training operation for those surgeons adopting robotics, the consumable cost of routine robotic cholecystectomy can be difficult to justify in the absence of evidence favouring or disputing this approach. Here, we describe a novel method for performing a robot-assisted cholecystectomy using a \"\"three-arm\"\" technique on the newer, 4th generation, da Vinci system. Whilst maintaining the ability to perform precision dissection, this method reduces the consumable cost by 46%. The initial series of 109 procedures proves this procedure to be safe, feasible, trainable and time efficient. ",
    "doi": "10.1007/s11701-024-01919-5"
  },
  {
    "repository": "PubMed",
    "title": "Learning-based personalisation of robot behaviour for robot-assisted therapy.",
    "authors": "Michał Stolarz; Alex Mitrevski; Mohammad Wasil; Paul G Plöger",
    "abstract": "During robot-assisted therapy, a robot typically needs to be partially or fully controlled by therapists, for instance using a Wizard-of-Oz protocol; this makes therapeutic sessions tedious to conduct, as therapists cannot fully focus on the interaction with the person under therapy. In this work, we develop a learning-based behaviour model that can be used to increase the autonomy of a robot's decision-making process. We investigate reinforcement learning as a model training technique and compare different reward functions that consider a user's engagement and activity performance. We also analyse various strategies that aim to make the learning process more tractable, namely i) behaviour model training with a learned user model, ii) policy transfer between user groups, and iii) policy learning from expert feedback. We demonstrate that policy transfer can significantly speed up the policy learning process, although the reward function has an important effect on the actions that a robot can choose. Although the main focus of this paper is the personalisation pipeline itself, we further evaluate the learned behaviour models in a small-scale real-world feasibility study in which six users participated in a sequence learning game with an assistive robot. The results of this study seem to suggest that learning from guidance may result in the most adequate policies in terms of increasing the engagement and game performance of users, but a large-scale user study is needed to verify the validity of that observation. ",
    "doi": "10.3389/frobt.2024.1352152"
  },
  {
    "repository": "PubMed",
    "title": "Advancements in Pancreatic Cancer Detection: Integrating Biomarkers, Imaging Technologies, and Machine Learning for Early Diagnosis.",
    "authors": "Hisham Daher; Sneha A Punchayil; Amro Ahmed Elbeltagi Ismail; Reuben Ryan Fernandes; Joel Jacob; Mohab H Algazzar; Mohammad Mansour",
    "abstract": "Artificial intelligence (AI) has come to play a pivotal role in revolutionizing medical practices, particularly in the field of pancreatic cancer detection and management. As a leading cause of cancer-related deaths, pancreatic cancer warrants innovative approaches due to its typically advanced stage at diagnosis and dismal survival rates. Present detection methods, constrained by limitations in accuracy and efficiency, underscore the necessity for novel solutions. AI-driven methodologies present promising avenues for enhancing early detection and prognosis forecasting. Through the analysis of imaging data, biomarker profiles, and clinical information, AI algorithms excel in discerning subtle abnormalities indicative of pancreatic cancer with remarkable precision. Moreover, machine learning (ML) algorithms facilitate the amalgamation of diverse data sources to optimize patient care. However, despite its huge potential, the implementation of AI in pancreatic cancer detection faces various challenges. Issues such as the scarcity of comprehensive datasets, biases in algorithm development, and concerns regarding data privacy and security necessitate thorough scrutiny. While AI offers immense promise in transforming pancreatic cancer detection and management, ongoing research and collaborative efforts are indispensable in overcoming technical hurdles and ethical dilemmas. This review delves into the evolution of AI, its application in pancreatic cancer detection, and the challenges and ethical considerations inherent in its integration. ",
    "doi": "10.7759/cureus.56583"
  },
  {
    "repository": "PubMed",
    "title": "First-in-human real-time AI-assisted instrument deocclusion during augmented reality robotic surgery.",
    "authors": "Jasper Hofman; Pieter De Backer; Ilaria Manghi; Jente Simoens; Ruben De Groote; Hannes Van Den Bossche; Mathieu D'Hondt; Tim Oosterlinck; Julie Lippens; Charles Van Praet; Federica Ferraguti; Charlotte Debbaut; Zhijin Li; Oliver Kutter; Alexandre Mottrie; Karel Decaestecker",
    "abstract": "The integration of Augmented Reality (AR) into daily surgical practice is withheld by the correct registration of pre-operative data. This includes intelligent 3D model superposition whilst simultaneously handling real and virtual occlusions caused by the AR overlay. Occlusions can negatively impact surgical safety and as such deteriorate rather than improve surgical care. Robotic surgery is particularly suited to tackle these integration challenges in a stepwise approach as the robotic console allows for different inputs to be displayed in parallel to the surgeon. Nevertheless, real-time de-occlusion requires extensive computational resources which further complicates clinical integration. This work tackles the problem of instrument occlusion and presents, to the authors' best knowledge, the first-in-human on edge deployment of a real-time binary segmentation pipeline during three robot-assisted surgeries: partial nephrectomy, migrated endovascular stent removal, and liver metastasectomy. To this end, a state-of-the-art real-time segmentation and 3D model pipeline was implemented and presented to the surgeon during live surgery. The pipeline allows real-time binary segmentation of 37 non-organic surgical items, which are never occluded during AR. The application features real-time manual 3D model manipulation for correct soft tissue alignment. The proposed pipeline can contribute towards surgical safety, ergonomics, and acceptance of AR in minimally invasive surgery. ",
    "doi": "10.1049/htl2.12056"
  },
  {
    "repository": "PubMed",
    "title": "AI & robotics briefing: AI-fuelled election campaigns are here.",
    "authors": "Katrina Krämer",
    "abstract": "",
    "doi": "10.1038/d41586-024-01133-1"
  },
  {
    "repository": "PubMed",
    "title": "AI and robotics demystify the workings of a fly's wing.",
    "authors": "Dan Fox",
    "abstract": "",
    "doi": "10.1038/d41586-024-01122-4"
  },
  {
    "repository": "PubMed",
    "title": "Using machine learning and robotics to discover plastic substitutes.",
    "authors": "Melisa Yashinski",
    "abstract": "Discovery of all-natural thin films with tunable properties was partially automated using robotics and machine learning. ",
    "doi": "10.1126/scirobotics.adp7392"
  },
  {
    "repository": "PubMed",
    "title": "A zero-shot reinforcement learning strategy for autonomous guidewire navigation.",
    "authors": "Valentina Scarponi; Michel Duprez; Florent Nageotte; Stéphane Cotin",
    "abstract": "The treatment of cardiovascular diseases requires complex and challenging navigation of a guidewire and catheter. This often leads to lengthy interventions during which the patient and clinician are exposed to X-ray radiation. Deep reinforcement learning approaches have shown promise in learning this task and may be the key to automating catheter navigation during robotized interventions. Yet, existing training methods show limited capabilities at generalizing to unseen vascular anatomies, requiring to be retrained each time the geometry changes. In this paper, we propose a zero-shot learning strategy for three-dimensional autonomous endovascular navigation. Using a very small training set of branching patterns, our reinforcement learning algorithm is able to learn a control that can then be applied to unseen vascular anatomies without retraining. We demonstrate our method on 4 different vascular systems, with an average success rate of 95% at reaching random targets on these anatomies. Our strategy is also computationally efficient, allowing the training of our controller to be performed in only 2 h. Our training method proved its ability to navigate unseen geometries with different characteristics, thanks to a nearly shape-invariant observation space. ",
    "doi": "10.1007/s11548-024-03092-4"
  },
  {
    "repository": "PubMed",
    "title": "AI & robotics briefing: How AI is improving climate forecasts.",
    "authors": "Katrina Krämer",
    "abstract": "",
    "doi": "10.1038/d41586-024-01064-x"
  },
  {
    "repository": "PubMed",
    "title": "Viability leads to the emergence of gait transitions in learning agile quadrupedal locomotion on challenging terrains.",
    "authors": "Milad Shafiee; Guillaume Bellegarda; Auke Ijspeert",
    "abstract": "Quadruped animals are capable of seamless transitions between different gaits. While energy efficiency appears to be one of the reasons for changing gaits, other determinant factors likely play a role too, including terrain properties. In this article, we propose that viability, i.e., the avoidance of falls, represents an important criterion for gait transitions. We investigate the emergence of gait transitions through the interaction between supraspinal drive (brain), the central pattern generator in the spinal cord, the body, and exteroceptive sensing by leveraging deep reinforcement learning and robotics tools. Consistent with quadruped animal data, we show that the walk-trot gait transition for quadruped robots on flat terrain improves both viability and energy efficiency. Furthermore, we investigate the effects of discrete terrain (i.e., crossing successive gaps) on imposing gait transitions, and find the emergence of trot-pronk transitions to avoid non-viable states. Viability is the only improved factor after gait transitions on both flat and discrete gap terrains, suggesting that viability could be a primary and universal objective of gait transitions, while other criteria are secondary objectives and/or a consequence of viability. Moreover, our experiments demonstrate state-of-the-art quadruped robot agility in challenging scenarios. ",
    "doi": "10.1038/s41467-024-47443-w"
  },
  {
    "repository": "PubMed",
    "title": "Exploring the Use and Implications of AI in Sexual and Reproductive Health and Rights: Protocol for a Scoping Review.",
    "authors": "Tigest Tamrat; Yu Zhao; Denise Schalet; Shada AlSalamah; Sameer Pujari; Lale Say",
    "abstract": "Artificial intelligence (AI) has emerged as a transformative force across the health sector and has garnered significant attention within sexual and reproductive health and rights (SRHR) due to polarizing views on its opportunities to advance care and the heightened risks and implications it brings to people's well-being and bodily autonomy. As the fields of AI and SRHR evolve, clarity is needed to bridge our understanding of how AI is being used within this historically politicized health area and raise visibility on the critical issues that can facilitate its responsible and meaningful use. This paper presents the protocol for a scoping review to synthesize empirical studies that focus on the intersection of AI and SRHR. The review aims to identify the characteristics of AI systems and tools applied within SRHR, regarding health domains, intended purpose, target users, AI data life cycle, and evidence on benefits and harms. The scoping review follows the standard methodology developed by Arksey and O'Malley. We will search the following electronic databases: MEDLINE (PubMed), Scopus, Web of Science, and CINAHL. Inclusion criteria comprise the use of AI systems and tools in sexual and reproductive health and clear methodology describing either quantitative or qualitative approaches, including program descriptions. Studies will be excluded if they focus entirely on digital interventions that do not explicitly use AI systems and tools, are about robotics or nonhuman subjects, or are commentaries. We will not exclude articles based on geographic location, language, or publication date. The study will present the uses of AI across sexual and reproductive health domains, the intended purpose of the AI system and tools, and maturity within the AI life cycle. Outcome measures will be reported on the effect, accuracy, acceptability, resource use, and feasibility of studies that have deployed and evaluated AI systems and tools. Ethical and legal considerations, as well as findings from qualitative studies, will be synthesized through a narrative thematic analysis. We will use the PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analyses Extension for Scoping Reviews) format for the publication of the findings. The database searches resulted in 12,793 records when the searches were conducted in October 2023. Screening is underway, and the analysis is expected to be completed by July 2024. The findings will provide key insights on usage patterns and evidence on the use of AI in SRHR, as well as convey key ethical, safety, and legal considerations. The outcomes of this scoping review are contributing to a technical brief developed by the World Health Organization and will guide future research and practice in this highly charged area of work. OSF Registries osf.io/ma4d9; https://osf.io/ma4d9. PRR1-10.2196/53888. ",
    "doi": "10.2196/53888"
  },
  {
    "repository": "PubMed",
    "title": "Artificial Intelligence (AI)-Robotics Started When Human Capability Reached Limit, Human Creativity Begin Again When the Capability of AI-Robotics Reaches a Plateau.",
    "authors": "Seong Yi",
    "abstract": "",
    "doi": "10.14245/ns.2448234.117"
  },
  {
    "repository": "PubMed",
    "title": "Adaptive Unsupervised Learning-Based 3D Spatiotemporal Filter for Event-Driven Cameras.",
    "authors": "Meriem Ben Miled; Wenwen Liu; Yuanchang Liu",
    "abstract": "In the evolving landscape of robotics and visual navigation, event cameras have gained important traction, notably for their exceptional dynamic range, efficient power consumption, and low latency. Despite these advantages, conventional processing methods oversimplify the data into 2 dimensions, neglecting critical temporal information. To overcome this limitation, we propose a novel method that treats events as 3D time-discrete signals. Drawing inspiration from the intricate biological filtering systems inherent to the human visual apparatus, we have developed a 3D spatiotemporal filter based on unsupervised machine learning algorithm. This filter effectively reduces noise levels and performs data size reduction, with its parameters being dynamically adjusted based on population activity. This ensures adaptability and precision under various conditions, like changes in motion velocity and ambient lighting. In our novel validation approach, we first identify the noise type and determine its power spectral density in the event stream. We then apply a one-dimensional discrete fast Fourier transform to assess the filtered event data within the frequency domain, ensuring that the targeted noise frequencies are adequately reduced. Our research also delved into the impact of indoor lighting on event stream noise. Remarkably, our method led to a 37% decrease in the data point cloud, improving data quality in diverse outdoor settings. ",
    "doi": "10.34133/research.0330"
  },
  {
    "repository": "PubMed",
    "title": "Subtle Visual Latency Can Profoundly Impair Implicit Sensorimotor Learning.",
    "authors": "Alkis M Hadjiosif; George Abraham; Tanvi Ranjan; Maurice A Smith",
    "abstract": "Short sub-100ms visual feedback latencies are common in many types of human-computer interactions yet are known to markedly reduce performance in a wide variety of motor tasks from simple pointing to operating surgical robotics. These latencies are also present in the computer-based experiments used to study the sensorimotor learning that underlies the acquisition of motor performance. Inspired by neurophysiological findings showing that cerebellar LTD and cortical LTP would both be disrupted by sub-100ms latencies, we hypothesized that implicit sensorimotor learning may be particularly sensitive to these short latencies. Remarkably, we find that improving latency by just 60ms, from 85 to 25ms in latency-optimized experiments, increases implicit learning by 50% and proportionally decreases explicit learning, resulting in a dramatic reorganization of sensorimotor memory. We go on to show that implicit sensorimotor learning is considerably more sensitive to latencies in the sub-100ms range than at higher latencies, in line with the latency-specific neural plasticity that has been observed. This suggests a clear benefit for latency reduction in computer-based training that involves implicit sensorimotor learning and that across-study differences in implicit motor learning might often be explained by disparities in feedback latency. ",
    "doi": "10.1101/2024.03.14.585093"
  },
  {
    "repository": "PubMed",
    "title": "AI & robotics briefing: Can AI's bias problem be fixed?",
    "authors": "Katrina Krämer",
    "abstract": "",
    "doi": "10.1038/d41586-024-00947-3"
  },
  {
    "repository": "PubMed",
    "title": "Autonomous Robotic System to Prune Sweet Pepper Leaves Using Semantic Segmentation with Deep Learning and Articulated Manipulator.",
    "authors": "Truong Thi Huong Giang; Young-Jae Ryoo",
    "abstract": "This paper proposes an autonomous robotic system to prune sweet pepper leaves using semantic segmentation with deep learning and an articulated manipulator. This system involves three main tasks: the perception of crop parts, the detection of pruning position, and the control of the articulated manipulator. A semantic segmentation neural network is employed to recognize the different parts of the sweet pepper plant, which is then used to create 3D point clouds for detecting the pruning position and the manipulator pose. Eventually, a manipulator robot is controlled to prune the crop part. This article provides a detailed description of the three tasks involved in building the sweet pepper pruning system and how to integrate them. In the experiments, we used a robot arm to manipulate the pruning leaf actions within a certain height range and a depth camera to obtain 3D point clouds. The control program was developed in different modules using various programming languages running on the ROS (Robot Operating System). ",
    "doi": "10.3390/biomimetics9030161"
  },
  {
    "repository": "PubMed",
    "title": "Towards an AI-driven soft toy for automatically detecting and classifying infant-toy interactions using optical force sensors.",
    "authors": "Rithwik Udayagiri; Jessica Yin; Xinyao Cai; William Townsend; Varun Trivedi; Rohan Shende; O Francis Sowande; Laura A Prosser; James H Pikul; Michelle J Johnson",
    "abstract": "Introduction: It is crucial to identify neurodevelopmental disorders in infants early on for timely intervention to improve their long-term outcomes. Combining natural play with quantitative measurements of developmental milestones can be an effective way to swiftly and efficiently detect infants who are at risk of neurodevelopmental delays. Clinical studies have established differences in toy interaction behaviors between full-term infants and pre-term infants who are at risk for cerebral palsy and other developmental disorders. Methods: The proposed toy aims to improve the quantitative assessment of infant-toy interactions and fully automate the process of detecting those infants at risk of developing motor delays. This paper describes the design and development of a toy that uniquely utilizes a collection of soft lossy force sensors which are developed using optical fibers to gather play interaction data from infants laying supine in a gym. An example interaction database was created by having 15 adults complete a total of 2480 interactions with the toy consisting of 620 touches, 620 punches-\"\"kick substitute,\"\" 620 weak grasps and 620 strong grasps. Results: The data is analyzed for patterns of interaction with the toy face using a machine learning model developed to classify the four interactions present in the database. Results indicate that the configuration of 6 soft force sensors on the face created unique activation patterns. Discussion: The machine learning algorithm was able to identify the distinct action types from the data, suggesting the potential usability of the toy. Next steps involve sensorizing the entire toy and testing with infants. ",
    "doi": "10.3389/frobt.2024.1325296"
  },
  {
    "repository": "PubMed",
    "title": "A survey on autonomous environmental monitoring approaches: towards unifying active sensing and reinforcement learning.",
    "authors": "David Mansfield; Allahyar Montazeri",
    "abstract": "The environmental pollution caused by various sources has escalated the climate crisis making the need to establish reliable, intelligent, and persistent environmental monitoring solutions more crucial than ever. Mobile sensing systems are a popular platform due to their cost-effectiveness and adaptability. However, in practice, operation environments demand highly intelligent and robust systems that can cope with an environment's changing dynamics. To achieve this reinforcement learning has become a popular tool as it facilitates the training of intelligent and robust sensing agents that can handle unknown and extreme conditions. In this paper, a framework that formulates active sensing as a reinforcement learning problem is proposed. This framework allows unification with multiple essential environmental monitoring tasks and algorithms such as coverage, patrolling, source seeking, exploration and search and rescue. The unified framework represents a step towards bridging the divide between theoretical advancements in reinforcement learning and real-world applications in environmental monitoring. A critical review of the literature in this field is carried out and it is found that despite the potential of reinforcement learning for environmental active sensing applications there is still a lack of practical implementation and most work remains in the simulation phase. It is also noted that despite the consensus that, multi-agent systems are crucial to fully realize the potential of active sensing there is a lack of research in this area. ",
    "doi": "10.3389/frobt.2024.1336612"
  },
  {
    "repository": "PubMed",
    "title": "Robot motor learning shows emergence of frequency-modulated, robust swimming with an invariant Strouhal number.",
    "authors": "Hankun Deng; Donghao Li; Colin Nitroy; Andrew Wertz; Shashank Priya; Bo Cheng",
    "abstract": "Fish locomotion emerges from diverse interactions among deformable structures, surrounding fluids and neuromuscular activations, i.e. fluid-structure interactions (FSI) controlled by fish's motor systems. Previous studies suggested that such motor-controlled FSI may possess embodied traits. However, their implications in motor learning, neuromuscular control, gait generation, and swimming performance remain to be uncovered. Using robot models, we studied the embodied traits in fish-inspired swimming. We developed modular robots with various designs and used central pattern generators (CPGs) to control the torque acting on robot body. We used reinforcement learning to learn CPG parameters for maximizing the swimming speed. The results showed that motor frequency converged faster than other parameters, and the emergent swimming gaits were robust against disruptions applied to motor control. For all robots and frequencies tested, swimming speed was proportional to the mean undulation velocity of body and caudal-fin combined, yielding an invariant, undulation-based Strouhal number. The Strouhal number also revealed two fundamental classes of undulatory swimming in both biological and robotic fishes. The robot actuators were also demonstrated to function as motors, virtual springs and virtual masses. These results provide novel insights in understanding fish-inspired locomotion. ",
    "doi": "10.1098/rsif.2024.0036"
  },
  {
    "repository": "PubMed",
    "title": "Hybrid Directed Hypergraph Learning and Forecasting of Skeleton-Based Human Poses.",
    "authors": "Qiongjie Cui; Zongyuan Ding; Fuhua Chen",
    "abstract": "Forecasting 3-dimensional skeleton-based human poses from the historical sequence is a classic task, which shows enormous potential in robotics, computer vision, and graphics. Currently, the state-of-the-art methods resort to graph convolutional networks (GCNs) to access the relationships of human joint pairs to formulate this problem. However, human action involves complex interactions among multiple joints, which presents a higher-order correlation overstepping the pairwise (2-order) connection of GCNs. Moreover, joints are typically activated by the parent joint, rather than driving their parent joints, whereas in existing methods, this specific direction of information transmission is ignored. In this work, we propose a novel hybrid directed hypergraph convolution network (H-DHGCN) to model the high-order relationships of the human skeleton with directionality. Specifically, our H-DHGCN mainly involves 2 core components. One is the static directed hypergraph, which is pre-defined according to the human body structure, to effectively leverage the natural relations of human joints. The second is dynamic directed hypergraph (D-DHG). D-DHG is learnable and can be constructed adaptively, to learn the unique characteristics of the motion sequence. In contrast to the typical GCNs, our method brings a richer and more refined topological representation of skeleton data. On several large-scale benchmarks, experimental results show that the proposed model consistently surpasses the latest techniques. ",
    "doi": "10.34133/cbsystems.0093"
  },
  {
    "repository": "PubMed",
    "title": "Editorial: Employment sustainability and teaching/learning techniques in higher education institutions.",
    "authors": "Arsalan Mujahid Ghouri; Muhammad Awais Bhatti; Ariff Syah Juhari",
    "abstract": "",
    "doi": "10.3389/fpsyg.2024.1373916"
  },
  {
    "repository": "PubMed",
    "title": "AI & robotics briefing: LLMs harbour hidden racism.",
    "authors": "Katrina Krämer",
    "abstract": "",
    "doi": "10.1038/d41586-024-00856-5"
  },
  {
    "repository": "PubMed",
    "title": "Defining the learning curve for robotic pancreaticoduodenectomy for a single surgeon following experience with laparoscopic pancreaticoduodenectomy.",
    "authors": "Isabel DeLaura; Jeremy Sharib; John M Creasy; Samuel I Berchuck; Dan G Blazer; Michael E Lidsky; Kevin N Shah; Sabino Zani",
    "abstract": "Robotic pancreaticoduodenectomy (RPD) has a learning curve of approximately 30-250 cases to reach proficiency. The learning curve for laparoscopic pancreaticoduodenectomy (LPD) at Duke University was previously defined as 50 cases. This study describes the RPD learning curve for a single surgeon following experience with LPD. LPD and RPD were retrospectively analyzed. Continuous pathologic and perioperative metrics were compared and learning curve were defined with respect to operative time using CUSUM analysis. Seventeen LPD and 69 RPD were analyzed LPD had an inverted learning curve possibly accounting for proficiency attained during the surgeon's fellowship and acquisition of new skills coinciding with more complex patient selection. The learning curve for RPD had three phases: accelerated early experience (cases 1-10), skill consolidation (cases 11-40), and improvement (cases 41-69), marked by reduction in operative time. Compared to LPD, RPD had shorter operative time (379 vs 479 min, p < 0.005), less EBL (250 vs 500, p < 0.02), and similar R0 resection. RPD also had improved LOS (7 vs 10 days, p < 0.007), and lower rates of surgical site infection (10% vs 47%, p < 0.002), DGE (19% vs 47%, p < 0.03), and readmission (13% vs 41%, p < 0.02). Experience in LPD may shorten the learning curve for RPD. The gap in surgical quality and perioperative outcomes between LPD and RPD will likely widen as exposure to robotics in General Surgery, Hepatopancreaticobiliary, and Surgical Oncology training programs increase. ",
    "doi": "10.1007/s11701-023-01746-0"
  },
  {
    "repository": "PubMed",
    "title": "RobOCTNet: Robotics and Deep Learning for Referable Posterior Segment Pathology Detection in an Emergency Department Population.",
    "authors": "Ailin Song; Jay B Lusk; Kyung-Min Roh; S Tammy Hsu; Nita G Valikodath; Eleonora M Lad; Kelly W Muir; Matthew M Engelhard; Alexander T Limkakeng; Joseph A Izatt; Ryan P McNabb; Anthony N Kuo",
    "abstract": "To evaluate the diagnostic performance of a robotically aligned optical coherence tomography (RAOCT) system coupled with a deep learning model in detecting referable posterior segment pathology in OCT images of emergency department patients. A deep learning model, RobOCTNet, was trained and internally tested to classify OCT images as referable versus non-referable for ophthalmology consultation. For external testing, emergency department patients with signs or symptoms warranting evaluation of the posterior segment were imaged with RAOCT. RobOCTNet was used to classify the images. Model performance was evaluated against a reference standard based on clinical diagnosis and retina specialist OCT review. We included 90,250 OCT images for training and 1489 images for internal testing. RobOCTNet achieved an area under the curve (AUC) of 1.00 (95% confidence interval [CI], 0.99-1.00) for detection of referable posterior segment pathology in the internal test set. For external testing, RAOCT was used to image 72 eyes of 38 emergency department patients. In this set, RobOCTNet had an AUC of 0.91 (95% CI, 0.82-0.97), a sensitivity of 95% (95% CI, 87%-100%), and a specificity of 76% (95% CI, 62%-91%). The model's performance was comparable to two human experts' performance. A robotically aligned OCT coupled with a deep learning model demonstrated high diagnostic performance in detecting referable posterior segment pathology in a cohort of emergency department patients. Robotically aligned OCT coupled with a deep learning model may have the potential to improve emergency department patient triage for ophthalmology referral. ",
    "doi": "10.1167/tvst.13.3.12"
  },
  {
    "repository": "PubMed",
    "title": "AI & robotics briefing: How AI images and videos could change science.",
    "authors": "Katrina Krämer",
    "abstract": "",
    "doi": "10.1038/d41586-024-00788-0"
  },
  {
    "repository": "PubMed",
    "title": "Advancements in Learning-Based Navigation Systems for Robotic Applications in MRO Hangar: Review.",
    "authors": "Ndidiamaka Adiuku; Nicolas P Avdelidis; Gilbert Tang; Angelos Plastropoulos",
    "abstract": "The field of learning-based navigation for mobile robots is experiencing a surge of interest from research and industry sectors. The application of this technology for visual aircraft inspection tasks within a maintenance, repair, and overhaul (MRO) hangar necessitates efficient perception and obstacle avoidance capabilities to ensure a reliable navigation experience. The present reliance on manual labour, static processes, and outdated technologies limits operation efficiency in the inherently dynamic and increasingly complex nature of the real-world hangar environment. The challenging environment limits the practical application of conventional methods and real-time adaptability to changes. In response to these challenges, recent years research efforts have witnessed advancement with machine learning integration aimed at enhancing navigational capability in both static and dynamic scenarios. However, most of these studies have not been specific to the MRO hangar environment, but related challenges have been addressed, and applicable solutions have been developed. This paper provides a comprehensive review of learning-based strategies with an emphasis on advancements in deep learning, object detection, and the integration of multiple approaches to create hybrid systems. The review delineates the application of learning-based methodologies to real-time navigational tasks, encompassing environment perception, obstacle detection, avoidance, and path planning through the use of vision-based sensors. The concluding section addresses the prevailing challenges and prospective development directions in this domain. ",
    "doi": "10.3390/s24051377"
  },
  {
    "repository": "PubMed",
    "title": "Can an AI-carebot be filial? Reflections from Confucian ethics.",
    "authors": "Kathryn Muyskens; Yonghui Ma; Michael Dunn",
    "abstract": "This article discusses the application of artificially intelligent robots within eldercare and explores a series of ethical considerations, including the challenges that AI (Artificial Intelligence) technology poses to traditional Chinese Confucian filial piety. From the perspective of Confucian ethics, the paper argues that robots cannot adequately fulfill duties of care. Due to their detachment from personal relationships and interactions, the \"\"emotions\"\" of AI robots are merely performative reactions in different situations, rather than actual emotional abilities. No matter how \"\"humanized\"\" robots become, it is difficult to establish genuine empathy and a meaningful relationship with them for this reason. Even so, we acknowledge that AI robots are a significant tool in managing the demands of elder care and the growth of care poverty, and as such, we attempt to outline some parameters within which care robotics could be acceptable within a Confucian ethical system. Finally, the paper discusses the social impact and ethical considerations brought on by the interaction between humans and machines. It is observed that the relationship between humans and technology has always had both utopian and dystopian aspects, and robotic elder care is no exception. AI caregiver robots will likely become a part of elder care, and the transformation of these robots from \"\"service providers\"\" to \"\"companions\"\" seems inevitable. In light of this, the application of AI-augmented robotic elder care will also eventually change our understanding of interpersonal relationships and traditional requirements of filial piety. ",
    "doi": "10.1177/09697330241238332"
  },
  {
    "repository": "PubMed",
    "title": "AI & robotics briefing: GPT-4 can hack websites without human help.",
    "authors": "Katrina Krämer",
    "abstract": "",
    "doi": "10.1038/d41586-024-00737-x"
  },
  {
    "repository": "PubMed",
    "title": "Narrative review in learning curve and pediatric robotic training program.",
    "authors": "Giuseppe Autorino; Mario Mendoza-Sagaon; Maria Grazia Scuderi",
    "abstract": "Studying learning curve (LC) for robotic procedures and developing an adequate training program are two fundamental steps to standardize robotic procedures. With this aim, we analyzed the literature to study the LCs of different robotic procedures and the availability of standardized training problems. The PubMed database was searched in the period from January 1995 to September 2022. Articles presenting LC and potential training programs in the pediatric population were chosen. Twenty papers were screened describing LC of robotic-assisted laparoscopic pyeloplasty (n=12), fundoplication (n=4), cholecystectomy (n=2), choledochal cyst resection (n=1), nephrectomy/partial nephrectomy (n=1) and lingual tonsillectomy (n=1), with a total of 1,251 procedures. In 10 studies there was only one single surgeon; nine had more than one; one did not specify how many surgeons participated. Twelve papers were retrospective single-center, three multicentric retrospective, four prospective and one was compared a retrospective case series to a prospective cohort. Most of these studies focused on operative time as the primary outcome. It was analyzed as the only outcome in three articles, along with complications in 14, time to discharge in eight, blood loss in three and pain killer use in three. The selected studies analyzed LC impacting operative planning (n=20), training (n=10) and costs (n=2). There is still a long way to go to complete a standardized functional training for robotic surgery procedures in pediatric surgery. Moreover, the progressive reduction in costs expected in the years to come will play a key role in progressing the diffusion of this technology enabling the collection of data necessary to create a standardized pediatric surgery robotic training program. ",
    "doi": "10.21037/tp-22-456"
  },
  {
    "repository": "PubMed",
    "title": "Artificial Intelligence in Pediatrics: Learning to Walk Together.",
    "authors": "Kaan Can Demirbaş; Mehmet Yıldız; Seha Saygılı; Nur Canpolat; Özgür Kasapçopur",
    "abstract": "In this era of rapidly advancing technology, artificial intelligence (AI) has emerged as a transformative force, even being called the Fourth Industrial Revolution, along with gene editing and robotics. While it has undoubtedly become an increasingly important part of our daily lives, it must be recognized that it is not an additional tool, but rather a complex concept that poses a variety of challenges. AI, with considerable potential, has found its place in both medical care and clinical research. Within the vast field of pediatrics, it stands out as a particularly promising advancement. As pediatricians, we are indeed witnessing the impactful integration of AI-based applications into our daily clinical practice and research efforts. These tools are being used for simple to more complex tasks such as diagnosing clinically challenging conditions, predicting disease outcomes, creating treatment plans, educating both patients and healthcare professionals, and generating accurate medical records or scientific papers. In conclusion, the multifaceted applications of AI in pediatrics will increase efficiency and improve the quality of healthcare and research. However, there are certain risks and threats accompanying this advancement including the biases that may contribute to health disparities and, inaccuracies. Therefore, it is crucial to recognize and address the technical, ethical, and legal challenges as well as explore the benefits in both clinical and research fields. ",
    "doi": "10.5152/TurkArchPediatr.2024.24002"
  },
  {
    "repository": "PubMed",
    "title": "AI & robotics briefing: AI reveals US climate denial hotspots.",
    "authors": "Katrina Krämer",
    "abstract": "",
    "doi": "10.1038/d41586-024-00607-6"
  },
  {
    "repository": "PubMed",
    "title": "AI-based methodologies for exoskeleton-assisted rehabilitation of the lower limb: a review.",
    "authors": "Omar Coser; Christian Tamantini; Paolo Soda; Loredana Zollo",
    "abstract": "Over the past few years, there has been a noticeable surge in efforts to design novel tools and approaches that incorporate Artificial Intelligence (AI) into rehabilitation of persons with lower-limb impairments, using robotic exoskeletons. The potential benefits include the ability to implement personalized rehabilitation therapies by leveraging AI for robot control and data analysis, facilitating personalized feedback and guidance. Despite this, there is a current lack of literature review specifically focusing on AI applications in lower-limb rehabilitative robotics. To address this gap, our work aims at performing a review of 37 peer-reviewed papers. This review categorizes selected papers based on robotic application scenarios or AI methodologies. Additionally, it uniquely contributes by providing a detailed summary of input features, AI model performance, enrolled populations, exoskeletal systems used in the validation process, and specific tasks for each paper. The innovative aspect lies in offering a clear understanding of the suitability of different algorithms for specific tasks, intending to guide future developments and support informed decision-making in the realm of lower-limb exoskeleton and AI applications. ",
    "doi": "10.3389/frobt.2024.1341580"
  },
  {
    "repository": "PubMed",
    "title": "Learning-Based Control for Soft Robot-Environment Interaction with Force/Position Tracking Capability.",
    "authors": "Zhiqiang Tang; Wenci Xin; Peiyi Wang; Cecilia Laschi",
    "abstract": "Soft robotics promises to achieve safe and efficient interactions with the environment by exploiting its inherent compliance and designing control strategies. However, effective control for the soft robot-environment interaction has been a challenging task. The challenges arise from the nonlinearity and complexity of soft robot dynamics, especially in situations where the environment is unknown and uncertainties exist, making it difficult to establish analytical models. In this study, we propose a learning-based optimal control approach as an attempt to address these challenges, which is an optimized combination of a feedforward controller based on probabilistic model predictive control and a feedback controller based on nonparametric learning methods. The approach is purely data-driven, without prior knowledge of soft robot dynamics and environment structures, and can be easily updated online to adapt to unknown environments. A theoretical analysis of the approach is provided to ensure its stability and convergence. The proposed approach enabled a soft robotic manipulator to track target positions and forces when interacting with a manikin in different cases. Moreover, comparisons with other data-driven control methods show a better performance of our approach. Overall, this work provides a viable learning-based control approach for soft robot-environment interactions with force/position tracking capability. ",
    "doi": "10.1089/soro.2023.0116"
  },
  {
    "repository": "PubMed",
    "title": "AI & robotics briefing: Google AI will help to map methane pollution from space.",
    "authors": "Katrina Krämer",
    "abstract": "",
    "doi": "10.1038/d41586-024-00532-8"
  },
  {
    "repository": "PubMed",
    "title": "AI & robotics briefing: Lack of transparency surrounds Neuralink's 'brain-reading' chip.",
    "authors": "Katrina Krämer",
    "abstract": "",
    "doi": "10.1038/d41586-024-00375-3"
  },
  {
    "repository": "PubMed",
    "title": "Gait quality in prosthesis users is reflected by force-based metrics when learning to walk on a new research-grade powered prosthesis.",
    "authors": "Kinsey R Herrin; Samuel T Kwak; Chase G Rock; Young-Hui Chang",
    "abstract": "Powered prosthetic feet require customized tuning to ensure comfort and long-term success for the user, but tuning in both clinical and research settings is subjective, time intensive, and the standard for tuning can vary depending on the patient's and the prosthetist's experience levels. Therefore, we studied eight different metrics of gait quality associated with use of a research-grade powered prosthetic foot in seven individuals with transtibial amputation during treadmill walking. We compared clinically tuned and untuned conditions with the goal of identifying performance-based metrics capable of distinguishing between good (as determined by a clinician) from poor gait quality. Differences between the tuned and untuned conditions were reflected in ankle power, both the vertical and anterior-posterior impulse symmetry indices, limb-force alignment, and positive ankle work, with improvements seen in all metrics during use of the tuned prosthesis. Notably, all of these metrics relate to the timing of force generation during walking which is information not directly accessible to a prosthetist during a typical tuning process. This work indicates that relevant, real-time biomechanical data provided to the prosthetist through the future provision of wearable sensors may enhance and improve future clinical tuning procedures associated with powered prostheses as well as their long-term outcomes. ",
    "doi": "10.3389/fresc.2024.1339856"
  },
  {
    "repository": "PubMed",
    "title": "Application of artificial intelligence (AI) to control COVID-19 pandemic: Current status and future prospects.",
    "authors": "Sumel Ashique; Neeraj Mishra; Sourav Mohanto; Ashish Garg; Farzad Taghizadeh-Hesary; B H Jaswanth Gowda; Dinesh Kumar Chellappan",
    "abstract": "The impact of the coronavirus disease 2019 (COVID-19) pandemic on the everyday livelihood of people has been monumental and unparalleled. Although the pandemic has vastly affected the global healthcare system, it has also been a platform to promote and develop pioneering applications based on autonomic artificial intelligence (AI) technology with therapeutic significance in combating the pandemic. Artificial intelligence has successfully demonstrated that it can reduce the probability of human-to-human infectivity of the virus through evaluation, analysis, and triangulation of existing data on the infectivity and spread of the virus. This review talks about the applications and significance of modern robotic and automated systems that may assist in spreading a pandemic. In addition, this study discusses intelligent wearable devices and how they could be helpful throughout the COVID-19 pandemic. ",
    "doi": "10.1016/j.heliyon.2024.e25754"
  },
  {
    "repository": "PubMed",
    "title": "'She is failing; he is learning': Gender-differentiated attributions for girls' and boys' errors.",
    "authors": "Silvia Di Battista",
    "abstract": "According to gender-differentiated attributions of failure in the STEM field, errors tend to be attributed to internal factors more to girls than to boys. This experimental study explored factors influencing gender-differentiated teachers' internal attributions of girls' and boys' errors and the consequent likelihood of teachers' hesitancy to offer educational robotics (ER) courses to them. The predictions were as follows: (1) the likelihood of teachers' hesitancy would be related to gender-differentiated internal attributions of errors based on expectations of a low natural aptitude for girls; and (2) teachers with high levels of gender stereotypes would be more hesitant about offering ER to girls than to boys via the mediation of internal attributions of errors as being due to girls' low levels of natural aptitude for ER. In this experimental study, 155 Italian teachers (M = 38.59 years, SD = 8.20) responded to a questionnaire at the end of a course on ER in 2022. Teachers randomly read one of two vignettes describing a girl's or a boy's error during an ER course. Results of multiple regression and moderated mediation analyses confirmed both predictions. In order to reduce the gender STEM gap, the tendency to attribute girls' errors to internal and natural causes should be better inspected. ",
    "doi": "10.1111/bjep.12665"
  },
  {
    "repository": "PubMed",
    "title": "Towards human-centered AI and robotics to reduce hospital falls: finding opportunities to enhance patient-nurse interactions during toileting.",
    "authors": "Hannah Rafferty; Cameron Cretaro; Nicholas Arfanis; Andrew Moore; Douglas Pong; Stephanie Tulk Jesso",
    "abstract": "Introduction: Patients who are hospitalized may be at a higher risk for falling, which can result in additional injuries, longer hospitalizations, and extra cost for healthcare organizations. A frequent context for these falls is when a hospitalized patient needs to use the bathroom. While it is possible that \"\"high-tech\"\" tools like robots and AI applications can help, adopting a human-centered approach and engaging users and other affected stakeholders in the design process can help to maximize benefits and avoid unintended consequences. Methods: Here, we detail our findings from a human-centered design research effort to investigate how the process of toileting a patient can be ameliorated through the application of advanced tools like robots and AI. We engaged healthcare professionals in interviews, focus groups, and a co-creation session in order to recognize common barriers in the toileting process and find opportunities for improvement. Results: In our conversations with participants, who were primarily nurses, we learned that toileting is more than a nuisance for technology to remove through automation. Nurses seem keenly aware and responsive to the physical and emotional pains experienced by patients during the toileting process, and did not see technology as a feasible or welcomed substitute. Instead, nurses wanted tools which supported them in providing this care to their patients. Participants envisioned tools which helped them anticipate and understand patient toileting assistance needs so they could plan to assist at convenient times during their existing workflows. Participants also expressed favorability towards mechanical assistive features which were incorporated into existing equipment to ensure ubiquitous availability when needed without adding additional mass to an already cramped and awkward environment. Discussion: We discovered that the act of toileting served more than one function, and can be viewed as a valuable touchpoint in which nurses can assess, support, and encourage their patients to engage in their own recovery process as they perform a necessary and normal function of life. While we found opportunities for technology to make the process safer and less burdensome for patients and clinical staff alike, we believe that designers should preserve and enhance the therapeutic elements of the nurse-patient interaction rather than eliminate it through automation. ",
    "doi": "10.3389/frobt.2024.1295679"
  },
  {
    "repository": "PubMed",
    "title": "AI & robotics briefing: AI helps to reveal first passages of ancient charred scroll.",
    "authors": "Katrina Krämer",
    "abstract": "",
    "doi": "10.1038/d41586-024-00488-9"
  },
  {
    "repository": "PubMed",
    "title": "Optimizing lower limb rehabilitation: the intersection of machine learning and rehabilitative robotics.",
    "authors": "Xiaoqian Zhang; Xiyin Rong; Hanwen Luo",
    "abstract": "Lower limb rehabilitation is essential for recovery post-injury, stroke, or surgery, improving functional mobility and quality of life. Traditional therapy, dependent on therapists' expertise, faces challenges that are addressed by rehabilitation robotics. In the domain of lower limb rehabilitation, machine learning is progressively manifesting its capabilities in high personalization and data-driven approaches, gradually transforming methods of optimizing treatment protocols and predicting rehabilitation outcomes. However, this evolution faces obstacles, including model interpretability, economic hurdles, and regulatory constraints. This review explores the synergy between machine learning and robotic-assisted lower limb rehabilitation, summarizing scientific literature and highlighting various models, data, and domains. Challenges are critically addressed, and future directions proposed for more effective clinical integration. Emphasis is placed on upcoming applications such as Virtual Reality and the potential of deep learning in refining rehabilitation training. This examination aims to provide insights into the evolving landscape, spotlighting the potential of machine learning in rehabilitation robotics and encouraging balanced exploration of current challenges and future opportunities. ",
    "doi": "10.3389/fresc.2024.1246773"
  },
  {
    "repository": "PubMed",
    "title": "Machine Learning in Neurosurgery: Toward Complex Inputs, Actionable Predictions, and Generalizable Translations.",
    "authors": "Ethan Schonfeld; Nicole Mordekai; Alex Berg; Thomas Johnstone; Aaryan Shah; Vaibhavi Shah; Ghani Haider; Neelan J Marianayagam; Anand Veeravagu",
    "abstract": "Machine learning can predict neurosurgical diagnosis and outcomes, power imaging analysis, and perform robotic navigation and tumor labeling. State-of-the-art models can reconstruct and generate images, predict surgical events from video, and assist in intraoperative decision-making. In this review, we will detail the neurosurgical applications of machine learning, ranging from simple to advanced models, and their potential to transform patient care. As machine learning techniques, outputs, and methods become increasingly complex, their performance is often more impactful yet increasingly difficult to evaluate. We aim to introduce these advancements to the neurosurgical audience while suggesting major potential roadblocks to their safe and effective translation. Unlike the previous generation of machine learning in neurosurgery, the safe translation of recent advancements will be contingent on neurosurgeons' involvement in model development and validation. ",
    "doi": "10.7759/cureus.51963"
  },
  {
    "repository": "PubMed",
    "title": "Machine Learning-Enabled Environmentally Adaptable Skin-Electronic Sensor for Human Gesture Recognition.",
    "authors": "Yongjun Song; Thi Huyen Nguyen; Dawoon Lee; Jaekyun Kim",
    "abstract": "Stretchable sensors have been widely investigated and developed for the purpose of human motion detection, touch sensors, and healthcare monitoring, typically converting mechanical/structural deformation into electrical signals. The viscoelastic strain of stretchable materials often results in nonlinear stress-strain characteristics over a broad range of strains, consequently making the stretchable sensors at the body joints less accurate in predicting and recognizing human gestures. Accurate recognition of human gestures can be further deteriorated by environmental changes such as temperature and humidity. Here, we demonstrated an environment-adaptable high stress-strain linearity (up to ε = 150%) and high-durability (>100,000 cycles) stretchable sensor conformally laminated onto the body joints for human gesture recognition. The serpentine configuration of our ionic liquid-based stretchable film enabled us to construct broad data sets of mechanical strain and temperature changes for machine learning-based gesture recognition. Signal recognition and training of distinct strains and environmental stimuli using a machine learning-based algorithm analysis successfully measured and predicted the joint motion in a temperature-changing environment with an accuracy of 92.86% (R-squared). Therefore, we believe that our serpentine-shaped ion gel-based stretchable sensor harmonized with machine-learning analysis will be a significant achievement toward environmentally adaptive and multianalyte sensing applications. Our proposed machine learning-enabled multisensor system may enable the development of future electronic devices such as wearable electronics, soft robotics, electronic skin, and human-machine interaction systems. ",
    "doi": "10.1021/acsami.3c18588"
  },
  {
    "repository": "PubMed",
    "title": "A machine learning approach to robustly determine director fields and analyze defects in active nematics.",
    "authors": "Yunrui Li; Zahra Zarei; Phu N Tran; Yifei Wang; Aparna Baskaran; Seth Fraden; Michael F Hagan; Pengyu Hong",
    "abstract": "Active nematics are dense systems of rodlike particles that consume energy to drive motion at the level of the individual particles. They exist in natural systems like biological tissues and artificial materials such as suspensions of self-propelled colloidal particles or synthetic microswimmers. Active nematics have attracted significant attention in recent years due to their spectacular nonequilibrium collective spatiotemporal dynamics, which may enable applications in fields such as robotics, drug delivery, and materials science. The director field, which measures the direction and degree of alignment of the local nematic orientation, is a crucial characteristic of active nematics and is essential for studying topological defects. However, determining the director field is a significant challenge in many experimental systems. Although director fields can be derived from images of active nematics using traditional imaging processing methods, the accuracy of such methods is highly sensitive to the settings of the algorithms. These settings must be tuned from image to image due to experimental noise, intrinsic noise of the imaging technology, and perturbations caused by changes in experimental conditions. This sensitivity currently limits automatic analysis of active nematics. To address this, we developed a machine learning model for extracting reliable director fields from raw experimental images, which enables accurate analysis of topological defects. Application of the algorithm to experimental data demonstrates that the approach is robust and highly generalizable to experimental settings that are different from those in the training data. It could be a promising tool for investigating active nematics and may be generalized to other active matter systems. ",
    "doi": "10.1039/d3sm01253k"
  },
  {
    "repository": "PubMed",
    "title": "AI & robotics briefing: Machine vision won't take our jobs yet.",
    "authors": "Katrina Krämer",
    "abstract": "",
    "doi": "10.1038/d41586-024-00284-5"
  },
  {
    "repository": "PubMed",
    "title": "AI & robotics briefing: AlphaFold predicts thousands of possible psychedelics.",
    "authors": "Katrina Krämer",
    "abstract": "",
    "doi": "10.1038/d41586-024-00206-5"
  },
  {
    "repository": "PubMed",
    "title": "Fostering Motivation: Exploring the Impact of ICTs on the Learning of Students with Autism.",
    "authors": "José María Fernández-Batanero; Marta Montenegro-Rueda; José Fernández-Cerero; Eloy López-Meneses",
    "abstract": "Currently, the use of digital tools has led to significant changes in the educational system, favouring equity and the inclusion of students with educational needs. In this context, students with autism spectrum disorder (ASD) benefit from using these electronic devices to improve their learning experience. This study focuses on conducting a bibliometric analysis of the impact of information and communication technologies on the learning of students with ASD, with the aim of addressing two research questions. Through the analysis of three databases (Scopus, Dialnet, and Web of Science), a total of 24 articles related to the subject were collected. The results show that the use of different technological devices has numerous benefits for these students. Among the most prominent are the use of augmented reality and educational robotics, mainly providing improvements in academic performance, motivation and improved retention of knowledge acquired in the classroom. In conclusion, the clear need to train teachers in digital competencies and to intensify efforts in this line of research in order to improve the education of students, as well as to enrich the knowledge available to the scientific community, is highlighted. ",
    "doi": "10.3390/children11010119"
  },
  {
    "repository": "PubMed",
    "title": "AI & robotics briefing: There's a 5% risk that AI will wipe out humanity.",
    "authors": "Katrina Krämer",
    "abstract": "",
    "doi": "10.1038/d41586-024-00147-z"
  },
  {
    "repository": "PubMed",
    "title": "Segmenting mechanically heterogeneous domains via unsupervised learning.",
    "authors": "Quan Nguyen; Emma Lejeune",
    "abstract": "From biological organs to soft robotics, highly deformable materials are essential components of natural and engineered systems. These highly deformable materials can have heterogeneous material properties, and can experience heterogeneous deformations with or without underlying material heterogeneity. Many recent works have established that computational modeling approaches are well suited for understanding and predicting the consequences of material heterogeneity and for interpreting observed heterogeneous strain fields. In particular, there has been significant work toward developing inverse analysis approaches that can convert observed kinematic quantities (e.g., displacement, strain) to material properties and mechanical state. Despite the success of these approaches, they are not necessarily generalizable and often rely on tight control and knowledge of boundary conditions. Here, we will build on the recent advances (and ubiquity) of machine learning approaches to explore alternative approaches to detect patterns in heterogeneous material properties and mechanical behavior. Specifically, we will explore unsupervised learning approaches to clustering and ensemble clustering to identify heterogeneous regions. Overall, we find that these approaches are effective, yet limited in their abilities. Through this initial exploration (where all data and code are published alongside this manuscript), we set the stage for future studies that more specifically adapt these methods to mechanical data. ",
    "doi": "10.1007/s10237-023-01779-2"
  },
  {
    "repository": "PubMed",
    "title": "'Set it and forget it': automated lab uses AI and robotics to improve proteins.",
    "authors": "Ewen Callaway",
    "abstract": "",
    "doi": "10.1038/d41586-024-00093-w"
  },
  {
    "repository": "PubMed",
    "title": "AI & robotics briefing: Why superintelligent AI won't sneak up on us.",
    "authors": "Katrina Krämer",
    "abstract": "",
    "doi": "10.1038/d41586-024-00084-x"
  },
  {
    "repository": "PubMed",
    "title": "Letter to the Editor: \"\"How Can Biomedical Engineers Help Empower Individuals With Intellectual Disabilities? The Potential Benefits and Challenges of AI Technologies to Support Inclusivity and Transform Lives\"\".",
    "authors": "Alessandro Di Nuovo",
    "abstract": "The rapid advancement of Artificial Intelligence (AI) is transforming healthcare and daily life, offering great opportunities but also posing ethical and societal challenges. To ensure AI benefits all individuals, including those with intellectual disabilities, the focus should be on adaptive technology that can adapt to the unique needs of the user. Biomedical engineers have an interdisciplinary background that helps them to lead multidisciplinary teams in the development of human-centered AI solutions. These solutions can personalize learning, enhance communication, and improve accessibility for individuals with intellectual disabilities. Furthermore, AI can aid in healthcare research, diagnostics, and therapy. The ethical use of AI in healthcare and the collaboration of AI with human expertise must be emphasized. Public funding for inclusive research is encouraged, promoting equity and economic growth while empowering those with intellectual disabilities in society. ",
    "doi": "10.1109/JTEHM.2023.3331977"
  },
  {
    "repository": "PubMed",
    "title": "Unsupervised learning-based approach for detecting 3D edges in depth maps.",
    "authors": "Ayush Aggarwal; Rustam Stolkin; Naresh Marturi",
    "abstract": "3D edge features, which represent the boundaries between different objects or surfaces in a 3D scene, are crucial for many computer vision tasks, including object recognition, tracking, and segmentation. They also have numerous real-world applications in the field of robotics, such as vision-guided grasping and manipulation of objects. To extract these features in the noisy real-world depth data, reliable 3D edge detectors are indispensable. However, currently available 3D edge detection methods are either highly parameterized or require ground truth labelling, which makes them challenging to use for practical applications. To this extent, we present a new 3D edge detection approach using unsupervised classification. Our method learns features from depth maps at three different scales using an encoder-decoder network, from which edge-specific features are extracted. These edge features are then clustered using learning to classify each point as an edge or not. The proposed method has two key benefits. First, it eliminates the need for manual fine-tuning of data-specific hyper-parameters and automatically selects threshold values for edge classification. Second, the method does not require any labelled training data, unlike many state-of-the-art methods that require supervised training with extensive hand-labelled datasets. The proposed method is evaluated on five benchmark datasets with single and multi-object scenes, and compared with four state-of-the-art edge detection methods from the literature. Results demonstrate that the proposed method achieves competitive performance, despite not using any labelled data or relying on hand-tuning of key parameters. ",
    "doi": "10.1038/s41598-023-50899-3"
  },
  {
    "repository": "PubMed",
    "title": "AI-powered real-time annotations during urologic surgery: The future of training and quality metrics.",
    "authors": "Laura Zuluaga; Jordan Miller Rich; Raghav Gupta; Adriana Pedraza; Burak Ucpinar; Kennedy E Okhawere; Indu Saini; Priyanka Dwivedi; Dhruti Patel; Osama Zaytoun; Mani Menon; Ashutosh Tewari; Ketan K Badani",
    "abstract": "Real-time artificial intelligence (AI) annotation of the surgical field has the potential to automatically extract information from surgical videos, helping to create a robust surgical atlas. This content can be used for surgical education and qualitative initiatives. We demonstrate the first use of AI in urologic robotic surgery to capture live surgical video and annotate key surgical steps and safety milestones in real-time. While AI models possess the capability to generate automated annotations based on a collection of video images, the real-time implementation of such technology in urological robotic surgery to aid surgeon and training staff it is still pending to be studied. We conducted an educational symposium, which broadcasted 2 live procedures, a robotic-assisted radical prostatectomy (RARP) and a robotic-assisted partial nephrectomy (RAPN). A surgical AI platform system (Theator, Palo Alto, CA) generated real-time annotations and identified operative safety milestones. This was achieved through trained algorithms, conventional video recognition, and novel Video Transfer Network technology which captures clips in full context, enabling automatic recognition and surgical mapping in real-time. Real-time AI annotations for procedure #1, RARP, are found in Table 1. The safety milestone annotations included the apical safety maneuver and deliberate views of structures such as the external iliac vessels and the obturator nerve. Real-time AI annotations for procedure #2, RAPN, are found in Table 1. Safety milestones included deliberate views of structures such as the gonadal vessels and the ureter. AI annotated surgical events included intraoperative ultrasound, temporary clip application and removal, hemostatic powder application, and notable hemorrhage. For the first time, surgical intelligence successfully showcased real-time AI annotations of 2 separate urologic robotic procedures during a live telecast. These annotations may provide the technological framework for send automatic notifications to clinical or operational stakeholders. This technology is a first step in real-time intraoperative decision support, leveraging big data to improve the quality of surgical care, potentially improve surgical outcomes, and support training and education. ",
    "doi": "10.1016/j.urolonc.2023.11.002"
  },
  {
    "repository": "PubMed",
    "title": "A Scoping Review of the Use of Robotics Technologies for Supporting Social-Emotional Learning in Children with Autism.",
    "authors": "Sarika Kewalramani; Kelly-Ann Allen; Erin Leif; Andrea Ng",
    "abstract": "This scoping review synthesises the current research into robotics technologies for promoting social-emotional learning in children with autism spectrum disorder. It examines the types of robotics technologies employed, their applications, and the gaps in the existing literature. Our scoping review adhered to the Preferred Reporting Items for Systematic Review and Meta-Analysis (PRISMA) reporting guidelines. The systematic search of relevant databases allowed us to identify studies that use robotics technologies for fostering social, emotional, and cognitive skills in young children with autism. Our review has revealed that various robots, such as Nao, Kaspar, and Zeno, have been used to support the development of social and emotional skills through imitation games, turn-taking, joint attention, emotional recognition, and conversation. As most of these studies were conducted in clinical settings, there is a need for further research in classroom and community-based environments. Additionally, the literature calls for more high-quality longitudinal studies to assess the long-term effectiveness and sustainability of robot-assisted therapy and to assess adaptive and personalised interventions tailored to individual needs. More emphasis is recommended on professional development for educators, parents, and health professionals to incorporate robotics technologies as evidence-based interventions as a pathway for creating inclusive learning environments for children with autism. ",
    "doi": "10.1007/s10803-023-06193-2"
  },
  {
    "repository": "PubMed",
    "title": "Continuous Tracking using Deep Learning-based Decoding for Non-invasive Brain-Computer Interface.",
    "authors": "Dylan Forenzo; Hao Zhu; Jenn Shanahan; Jaehyun Lim; Bin He",
    "abstract": "Brain-computer interfaces (BCI) using electroencephalography (EEG) provide a non-invasive method for users to interact with external devices without the need for muscle activation. While noninvasive BCIs have the potential to improve the quality of lives of healthy and motor impaired individuals, they currently have limited applications due to inconsistent performance and low degrees of freedom. In this study, we use deep learning (DL)-based decoders for online Continuous Pursuit (CP), a complex BCI task requiring the user to track an object in two-dimensional space. We developed a labeling system to use CP data for supervised learning, trained DL-based decoders based on two architectures, including a newly proposed adaptation of the PointNet architecture, and evaluated the performance over several online sessions. We rigorously evaluated the DL-based decoders in a total of 28 human participants, and found that the DL-based models improved throughout the sessions as more training data became available and significantly outperformed a traditional BCI decoder by the last session. We also performed additional experiments to test an implementation of transfer learning by pre-training models on data from other subjects, and mid-session training to reduce inter-session variability. The results from these experiments showed that pre-training did not significantly improve performance, but updating the models mid-session may have some benefit. Overall, these findings support the use of DL-based decoders for improving BCI performance in complex tasks like CP, which can expand the potential applications of BCI devices and help improve the quality of lives of healthy and motor-impaired individuals. Brain-computer Interfaces (BCI) have the potential to replace or restore motor functions for patients and can benefit the general population by providing a direct link of the brain with robotics or other devices. In this work, we developed a paradigm using deep learning (DL)-based decoders for continuous control of a BCI system and demonstrated its capabilities through extensive online experiments. We also investigate how DL performance is affected by varying amounts of training data and collected more than 150 hours of BCI data that can be used to train new models. The results of this study provide valuable information for developing future DL-based BCI decoders which can improve performance and help bring BCIs closer to practical applications and wide-spread use. ",
    "doi": "10.1101/2023.10.12.562084"
  },
  {
    "repository": "PubMed",
    "title": "Learning to Control a Three-Dimensional Ferrofluidic Robot.",
    "authors": "Reza Ahmed; Roberto Calandra; Hamid Marvi",
    "abstract": "In recent years, ferrofluids have found increased popularity as a material for medical applications, such as ocular surgery, gastrointestinal surgery, and cancer treatment, among others. Ferrofluidic robots are multifunctional and scalable, exhibit fluid properties, and can be controlled remotely; thus, they are particularly advantageous for such medical tasks. Previously, ferrofluidic robot control has been achieved via the manipulation of handheld permanent magnets or in current-controlled electromagnetic fields resulting in two-dimensional position and shape control and three-dimensional (3D) coupled position-shape or position-only control. Control of ferrofluidic liquid droplet robots poses a unique challenge where model-based control has been shown to be computationally limiting. Thus, in this study, a model-free control method is chosen, and it is shown that the task of learning optimal control parameters for ferrofluidic robot control can be performed using machine learning. Particularly, we explore the use of Bayesian optimization to find optimal controller parameters for 3D pose control of a ferrofluid droplet: its centroid position, stretch direction, and stretch radius. We demonstrate that the position, stretch direction, and stretch radius of a ferrofluid droplet can be independently controlled in 3D with high accuracy and precision, using a simple control approach. Finally, we use ferrofluidic robots to perform pick-and-place, a lab-on-a-chip pH test, and electrical switching, in 3D settings. The purpose of this research is to expand the potential of ferrofluidic robots by introducing full pose control in 3D and to showcase the potential of this technology in the areas of microassembly, lab-on-a-chip, and electronics. The approach presented in this research can be used as a stepping-off point to incorporate ferrofluidic robots toward future research in these areas. ",
    "doi": "10.1089/soro.2023.0005"
  },
  {
    "repository": "PubMed",
    "title": "2D Materials Beyond Post-AI Era: Smart Fibers, Soft Robotics, and Single Atom Catalysts.",
    "authors": "Gang San Lee; Jin Goo Kim; Jun Tae Kim; Chan Woo Lee; Sujin Cha; Go Bong Choi; Joonwon Lim; Suchithra Padmajan Sasikala; Sang Ouk Kim",
    "abstract": "Recent consecutive discoveries of various 2D materials have triggered significant scientific and technological interests owing to their exceptional material properties, originally stemming from 2D confined geometry. Ever-expanding library of 2D materials can provide ideal solutions to critical challenges facing in current technological trend of the fourth industrial revolution. Moreover, chemical modification of 2D materials to customize their physical/chemical properties can satisfy the broad spectrum of different specific requirements across diverse application areas. This review focuses on three particular emerging application areas of 2D materials: smart fibers, soft robotics, and single atom catalysts (SACs), which hold immense potentials for academic and technological advancements in the post-artificial intelligence (AI) era. Smart fibers showcase unconventional functionalities including healthcare/environmental monitoring, energy storage/harvesting, and antipathogenic protection in the forms of wearable fibers and textiles. Soft robotics aligns with future trend to overcome longstanding limitations of hard-material based mechanics by introducing soft actuators and sensors. SACs are widely useful in energy storage/conversion and environmental management, principally contributing to low carbon footprint for sustainable post-AI era. Significance and unique values of 2D materials in these emerging applications are highlighted, where the research group has devoted research efforts for more than a decade. ",
    "doi": "10.1002/adma.202307689"
  },
  {
    "repository": "PubMed",
    "title": "Deep Learning for Visual Localization and Mapping: A Survey.",
    "authors": "Changhao Chen; Bing Wang; Chris Xiaoxuan Lu; Niki Trigoni; Andrew Markham",
    "abstract": "Deep-learning-based localization and mapping approaches have recently emerged as a new research direction and receive significant attention from both industry and academia. Instead of creating hand-designed algorithms based on physical models or geometric theories, deep learning solutions provide an alternative to solve the problem in a data-driven way. Benefiting from the ever-increasing volumes of data and computational power on devices, these learning methods are fast evolving into a new area that shows potential to track self-motion and estimate environmental models accurately and robustly for mobile agents. In this work, we provide a comprehensive survey and propose a taxonomy for the localization and mapping methods using deep learning. This survey aims to discuss two basic questions: whether deep learning is promising for localization and mapping, and how deep learning should be applied to solve this problem. To this end, a series of localization and mapping topics are investigated, from the learning-based visual odometry and global relocalization to mapping, and simultaneous localization and mapping (SLAM). It is our hope that this survey organically weaves together the recent works in this vein from robotics, computer vision, and machine learning communities and serves as a guideline for future researchers to apply deep learning to tackle the problem of visual localization and mapping. ",
    "doi": "10.1109/TNNLS.2023.3309809"
  },
  {
    "repository": "PubMed",
    "title": "Variational Hierarchical Mixtures for Probabilistic Learning of Inverse Dynamics.",
    "authors": "Hany Abdulsamad; Peter Nickl; Pascal Klink; Jan Peters",
    "abstract": "Well-calibrated probabilistic regression models are a crucial learning component in robotics applications as datasets grow rapidly and tasks become more complex. Unfortunately, classical regression models are usually either probabilistic kernel machines with a flexible structure that does not scale gracefully with data or deterministic and vastly scalable automata, albeit with a restrictive parametric form and poor regularization. In this paper, we consider a probabilistic hierarchical modeling paradigm that combines the benefits of both worlds to deliver computationally efficient representations with inherent complexity regularization. The presented approaches are probabilistic interpretations of local regression techniques that approximate nonlinear functions through a set of local linear or polynomial units. Importantly, we rely on principles from Bayesian nonparametrics to formulate flexible models that adapt their complexity to the data and can potentially encompass an infinite number of components. We derive two efficient variational inference techniques to learn these representations and highlight the advantages of hierarchical infinite local regression models, such as dealing with non-smooth functions, mitigating catastrophic forgetting, and enabling parameter sharing and fast predictions. Finally, we validate this approach on large inverse dynamics datasets and test the learned models in real-world control scenarios. ",
    "doi": "10.1109/TPAMI.2023.3314670"
  },
  {
    "repository": "PubMed",
    "title": "Reinforced Labels: Multi-Agent Deep Reinforcement Learning for Point-Feature Label Placement.",
    "authors": "Petr Bobak; Ladislav Cmolik; Martin Cadik",
    "abstract": "Over the recent years, Reinforcement Learning combined with Deep Learning techniques has successfully proven to solve complex problems in various domains, including robotics, self-driving cars, and finance. In this article, we are introducing Reinforcement Learning (RL) to label placement, a complex task in data visualization that seeks optimal positioning for labels to avoid overlap and ensure legibility. Our novel point-feature label placement method utilizes Multi-Agent Deep Reinforcement Learning to learn the label placement strategy, the first machine-learning-driven labeling method, in contrast to the existing hand-crafted algorithms designed by human experts. To facilitate RL learning, we developed an environment where an agent acts as a proxy for a label, a short textual annotation that augments visualization. Our results show that the strategy trained by our method significantly outperforms the random strategy of an untrained agent and the compared methods designed by human experts in terms of completeness (i.e., the number of placed labels). The trade-off is increased computation time, making the proposed method slower than the compared methods. Nevertheless, our method is ideal for scenarios where the labeling can be computed in advance, and completeness is essential, such as cartographic maps, technical drawings, and medical atlases. Additionally, we conducted a user study to assess the perceived performance. The outcomes revealed that the participants considered the proposed method to be significantly better than the other examined methods. This indicates that the improved completeness is not just reflected in the quantitative metrics but also in the subjective evaluation by the participants. ",
    "doi": "10.1109/TVCG.2023.3313729"
  },
  {
    "repository": "PubMed",
    "title": "THA-Net: A Deep Learning Solution for Next-Generation Templating and Patient-specific Surgical Execution.",
    "authors": "Pouria Rouzrokh; Bardia Khosravi; John P Mickley; Bradley J Erickson; Michael J Taunton; Cody C Wyles",
    "abstract": "This study introduces THA-Net, a deep learning inpainting algorithm for simulating postoperative total hip arthroplasty (THA) radiographs from a single preoperative pelvis radiograph input, while being able to generate predictions either unconditionally (algorithm chooses implants) or conditionally (surgeon chooses implants). The THA-Net is a deep learning algorithm which receives an input preoperative radiograph and subsequently replaces the target hip joint with THA implants to generate a synthetic yet realistic postoperative radiograph. We trained THA-Net on 356,305 pairs of radiographs from 14,357 patients from a single institution's total joint registry and evaluated the validity (quality of surgical execution) and realism (ability to differentiate real and synthetic radiographs) of its outputs against both human-based and software-based criteria. The surgical validity of synthetic postoperative radiographs was significantly higher than their real counterparts (mean difference: 0.8 to 1.1 points on 10-point Likert scale, P < .001), but they were not able to be differentiated in terms of realism in blinded expert review. Synthetic images showed excellent validity and realism when analyzed with already validated deep learning models. We developed a THA next-generation templating tool that can generate synthetic radiographs graded higher on ultimate surgical execution than real radiographs from training data. Further refinement of this tool may potentiate patient-specific surgical planning and enable technologies such as robotics, navigation, and augmented reality (an online demo of THA-Net is available at: https://demo.osail.ai/tha_net). ",
    "doi": "10.1016/j.arth.2023.08.063"
  },
  {
    "repository": "PubMed",
    "title": "Latent Representation-Based Learning Controller for Pneumatic and Hydraulic Dual Actuation of Pressure-Driven Soft Actuators.",
    "authors": "Taku Sugiyama; Kyo Kutsuzawa; Dai Owaki; Mitsuhiro Hayashibe",
    "abstract": "The pneumatic and hydraulic dual actuation of pressure-driven soft actuators (PSAs) is promising because of their potential to develop novel practical soft robots and expand the range of soft robot applications. However, the physical characteristics of air and water are largely different, which makes it challenging to quickly adapt to a selected actuation method and achieve method-independent accurate control performance. Herein, we propose a novel LAtent Representation-based Feedforward Neural Network (LAR-FNN) for dual actuation. The LAR-FNN consists of an autoencoder (AE) and a feedforward neural network (FNN). The AE generates a latent representation of a PSA from a 30-s stairstep response. Subsequently, the FNN provides an individual inverse model of the target PSA and calculates feedforward control input by using the latent representation. The experimental results with PSAs demonstrate that the LAR-FNN can meet the requirements of dual actuation control (i.e., accurate control performance regardless of the actuation method with a short adaptation time) with a single neural network. The results suggest that a LAR-FNN can contribute to soft dual-actuation robot development and the field of soft robotics. ",
    "doi": "10.1089/soro.2022.0224"
  },
  {
    "repository": "PubMed",
    "title": "Robust Grasping of a Variable Stiffness Soft Gripper in High-Speed Motion Based on Reinforcement Learning.",
    "authors": "Mingzhu Zhu; Junyue Dai; Yu Feng",
    "abstract": "Industrial robots are widely deployed to perform pick-and-place tasks at high speeds to minimize manufacturing time and boost productivity. When dealing with delicate or fragile goods, soft robotic grippers are better end effectors than rigid grippers due to their softness and safe interaction. However, high-speed motion causes the soft robotic gripper to vibrate, leading to damage of the objects or failed grasping. Soft grippers with variable stiffness are considered to be effective in suppressing vibrations by adding damping devices, but it is quite challenging to compromise between stiffness and compliance. In this article, a controller based on deep reinforcement learning is proposed to control the stiffness of the soft robotic gripper, which can accurately suppress the vibration with only a minor influence on its compliance and softness. The proposed controller is a real-time vibration control strategy, which estimates the output of the controller based on the current operating environment. To demonstrate the effectiveness of the proposed controller, experiments were done with a UR5 robotic arm. For different situations, experimental results show that the proposed controller responds quickly and reduces the amplitude of the oscillation substantially. ",
    "doi": "10.1089/soro.2022.0246"
  },
  {
    "repository": "PubMed",
    "title": "Challenges and Opportunities in Deep Reinforcement Learning With Graph Neural Networks: A Comprehensive Review of Algorithms and Applications.",
    "authors": "Sai Munikoti; Deepesh Agarwal; Laya Das; Mahantesh Halappanavar; Balasubramaniam Natarajan",
    "abstract": "Deep reinforcement learning (DRL) has empowered a variety of artificial intelligence fields, including pattern recognition, robotics, recommendation systems, and gaming. Similarly, graph neural networks (GNNs) have also demonstrated their superior performance in supervised learning for graph-structured data. In recent times, the fusion of GNN with DRL for graph-structured environments has attracted a lot of attention. This article provides a comprehensive review of these hybrid works. These works can be classified into two categories: 1) algorithmic contributions, where DRL and GNN complement each other with an objective of addressing each other's shortcomings and 2) application-specific contributions that leverage a combined GNN-DRL formulation to address problems specific to different applications. This fusion effectively addresses various complex problems in engineering and life sciences. Based on the review, we further analyze the applicability and benefits of fusing these two domains, especially in terms of increasing generalizability and reducing computational complexity. Finally, the key challenges in integrating DRL and GNN, and potential future research directions are highlighted, which will be of interest to the broader machine learning community. ",
    "doi": "10.1109/TNNLS.2023.3283523"
  },
  {
    "repository": "PubMed",
    "title": "Learning curves for robotic-assisted spine surgery: an analysis of the time taken for screw insertion, robot setting, registration, and fluoroscopy.",
    "authors": "Tsutomu Akazawa; Yoshiaki Torii; Jun Ueno; Tasuku Umehara; Masahiro Iinuma; Atsuhiro Yoshida; Ken Tomochika; Seiji Ohtori; Hisateru Niki",
    "abstract": "The purpose of this study was to clarify the learning curve for robotic-assisted spine surgery. We analyzed the workflow in robotic-assisted spine surgery and investigated how much experience is required to become proficient in robotic-assisted spine surgery. The data were obtained from consecutive 125 patients who underwent robotic-assisted screw placement soon after introducing a spine robotic system at a single center from April 2021 to January 2023. The 125 cases were divided into phases 1-5 of sequential groups of 25 cases each and compared for screw insertion time, robot setting time, registration time, and fluoroscopy time. There were no significant differences in age, body mass index, intraoperative blood loss, number of fused segments, operation time, or operation time per segment between the 5 phases. There were significant differences in screw insertion time, robot setting time, registration time, and fluoroscopy time between the 5 phases. The screw insertion time, robot setting time, registration time, and fluoroscopy time in phase 1 were significantly longer than those in phases 2, 3, 4, and 5. In an analysis of 125 cases after the introduction of the spine robotic system, the screw insertion time, robot setting time, registration time, and fluoroscopy time were significantly longer in the 25 cases in the period initially after introduction. The times were not significantly different in the subsequent 100 cases. Surgeons can be proficient in robotic-assisted spine surgery after their experience with 25 cases. ",
    "doi": "10.1007/s00590-023-03630-x"
  },
  {
    "repository": "PubMed",
    "title": "Increasing the Robustness of Deep Learning Models for Object Segmentation: A Framework for Blending Automatically Annotated Real and Synthetic Data.",
    "authors": "Artur Istvan Karoly; Sebestyen Tirczka; Huijun Gao; Imre J Rudas; Peter Galambos",
    "abstract": "Recent problems in robotics can sometimes only be tackled using machine learning technologies, particularly those that utilize deep learning (DL) with transfer learning. Transfer learning takes advantage of pretrained models, which are later fine-tuned using smaller task-specific datasets. The fine-tuned models must be robust against changes in environmental factors such as illumination since, often, there is no guarantee for them to be constant. Although synthetic data for pretraining has been shown to enhance DL model generalization, there is limited research on its application for fine-tuning. One limiting factor is that the generation and annotation of synthetic datasets can be cumbersome and impractical for the purpose of fine-tuning. To address this issue, we propose two methods for automatically generating annotated image datasets for object segmentation, one for real-world and another for synthetic images. We also introduce a novel domain adaptation approach called filling the reality gap (FTRG), which can blend elements from real-world and synthetic scenes in a single image to achieve domain adaptation. We demonstrate through experimentation on a representative robot application that FTRG outperforms other domain adaptation techniques, such as domain randomization or photorealistic synthetic images, in creating robust models. Furthermore, we evaluate the benefits of using synthetic data for fine-tuning in transfer learning and continual learning with experience replay using our proposed methods and FTRG. Our findings indicate that fine-tuning with synthetic data can produce superior results compared to solely using real-world data. ",
    "doi": "10.1109/TCYB.2023.3276485"
  },
  {
    "repository": "PubMed",
    "title": "Improving Workers' Musculoskeletal Health During Human-Robot Collaboration Through Reinforcement Learning.",
    "authors": "Ziyang Xie; Lu Lu; Hanwen Wang; Bingyi Su; Yunan Liu; Xu Xu",
    "abstract": "This study aims to improve workers' postures and thus reduce the risk of musculoskeletal disorders in human-robot collaboration by developing a novel model-free reinforcement learning method. Human-robot collaboration has been a flourishing work configuration in recent years. Yet, it could lead to work-related musculoskeletal disorders if the collaborative tasks result in awkward postures for workers. The proposed approach follows two steps: first, a 3D human skeleton reconstruction method was adopted to calculate workers' continuous awkward posture (CAP) score; second, an online gradient-based reinforcement learning algorithm was designed to dynamically improve workers' CAP score by adjusting the positions and orientations of the robot end effector. In an empirical experiment, the proposed approach can significantly improve the CAP scores of the participants during a human-robot collaboration task when compared with the scenarios where robot and participants worked together at a fixed position or at the individual elbow height. The questionnaire outcomes also showed that the working posture resulted from the proposed approach was preferred by the participants. The proposed model-free reinforcement learning method can learn the optimal worker postures without the need for specific biomechanical models. The data-driven nature of this method can make it adaptive to provide personalized optimal work posture. The proposed method can be applied to improve the occupational safety in robot-implemented factories. Specifically, the personalized robot working positions and orientations can proactively reduce exposure to awkward postures that increase the risk of musculoskeletal disorders. The algorithm can also reactively protect workers by reducing the workload in specific joints. ",
    "doi": "10.1177/00187208231177574"
  },
  {
    "repository": "PubMed",
    "title": "More Process, Less Principles: The Ethics of Deploying AI and Robotics in Medicine.",
    "authors": "Amitabha Palmer; David Schwan",
    "abstract": "Current national and international guidelines for the ethical design and development of artificial intelligence (AI) and robotics emphasize ethical theory. Various governing and advisory bodies have generated sets of broad ethical principles, which institutional decisionmakers are encouraged to apply to particular practical decisions. Although much of this literature examines the ethics of designing and developing AI and robotics, medical institutions typically must make purchase and deployment decisions about technologies that have already been designed and developed. The primary problem facing medical institutions is not one of ethical design but of ethical deployment. The purpose of this paper is to develop a practical model by which medical institutions may make ethical deployment decisions about ready-made advanced technologies. Our slogan is \"\"more process, less principles.\"\" Ethically sound decisionmaking requires that the process by which medical institutions make such decisions include participatory, deliberative, and conservative elements. We argue that our model preserves the strengths of existing frameworks, avoids their shortcomings, and delivers its own moral, practical, and epistemic advantages. ",
    "doi": "10.1017/S0963180123000087"
  },
  {
    "repository": "PubMed",
    "title": "A Survey on Offline Reinforcement Learning: Taxonomy, Review, and Open Problems.",
    "authors": "Rafael Figueiredo Prudencio; Marcos R O A Maximo; Esther Luna Colombini",
    "abstract": "With the widespread adoption of deep learning, reinforcement learning (RL) has experienced a dramatic increase in popularity, scaling to previously intractable problems, such as playing complex games from pixel observations, sustaining conversations with humans, and controlling robotic agents. However, there is still a wide range of domains inaccessible to RL due to the high cost and danger of interacting with the environment. Offline RL is a paradigm that learns exclusively from static datasets of previously collected interactions, making it feasible to extract policies from large and diverse training datasets. Effective offline RL algorithms have a much wider range of applications than online RL, being particularly appealing for real-world applications, such as education, healthcare, and robotics. In this work, we contribute with a unifying taxonomy to classify offline RL methods. Furthermore, we provide a comprehensive review of the latest algorithmic breakthroughs in the field using a unified notation as well as a review of existing benchmarks' properties and shortcomings. Additionally, we provide a figure that summarizes the performance of each method and class of methods on different dataset properties, equipping researchers with the tools to decide which type of algorithm is best suited for the problem at hand and identify which classes of algorithms look the most promising. Finally, we provide our perspective on open problems and propose future research directions for this rapidly growing field. ",
    "doi": "10.1109/TNNLS.2023.3250269"
  },
  {
    "repository": "PubMed",
    "title": "Exploration in Deep Reinforcement Learning: From Single-Agent to Multiagent Domain.",
    "authors": "Jianye Hao; Tianpei Yang; Hongyao Tang; Chenjia Bai; Jinyi Liu; Zhaopeng Meng; Peng Liu; Zhen Wang",
    "abstract": "Deep reinforcement learning (DRL) and deep multiagent reinforcement learning (MARL) have achieved significant success across a wide range of domains, including game artificial intelligence (AI), autonomous vehicles, and robotics. However, DRL and deep MARL agents are widely known to be sample inefficient that millions of interactions are usually needed even for relatively simple problem settings, thus preventing the wide application and deployment in real-industry scenarios. One bottleneck challenge behind is the well-known exploration problem, i.e., how efficiently exploring the environment and collecting informative experiences that could benefit policy learning toward the optimal ones. This problem becomes more challenging in complex environments with sparse rewards, noisy distractions, long horizons, and nonstationary co-learners. In this article, we conduct a comprehensive survey on existing exploration methods for both single-agent RL and multiagent RL. We start the survey by identifying several key challenges to efficient exploration. Then, we provide a systematic survey of existing approaches by classifying them into two major categories: uncertainty-oriented exploration and intrinsic motivation-oriented exploration. Beyond the above two main branches, we also include other notable exploration methods with different ideas and techniques. In addition to algorithmic analysis, we provide a comprehensive and unified empirical comparison of different exploration methods for DRL on a set of commonly used benchmarks. According to our algorithmic and empirical investigation, we finally summarize the open problems of exploration in DRL and deep MARL and point out a few future directions. ",
    "doi": "10.1109/TNNLS.2023.3236361"
  },
  {
    "repository": "PubMed",
    "title": "Review of the Factors Affecting Acceptance of AI-Infused Systems.",
    "authors": "Ulugbek Vahobjon Ugli Ismatullaev; Sang-Ho Kim",
    "abstract": "The study aimed to provide a comprehensive overview of the factors impacting technology adoption, to predict the acceptance of artificial intelligence (AI)-based technologies. Although the acceptance of AI devices is usually defined by behavioural factors in theories of user acceptance, the effects of technical and human factors are often overlooked. However, research shows that user behaviour can vary depending on a system's technical characteristics and differences in users. A systematic review was conducted. A total of 85 peer-reviewed journal articles that met the inclusion criteria and provided information on the factors influencing the adoption of AI devices were selected for the analysis. Research on the adoption of AI devices shows that users' attitudes, trust and perceptions about the technology can be improved by increasing transparency, compatibility, and reliability, and simplifying tasks. Moreover, technological factors are also important for reducing issues related to human factors (e.g. distrust, scepticism, inexperience) and supporting users with lower intention to use and lower trust in AI-infused systems. As prior research has confirmed the interrelationship among factors with and without behaviour theories, this review suggests extending the technology acceptance model that integrates the factors studied in this review to define the acceptance of AI devices across different application areas. However, further research is needed to collect more data and validate the study's findings. A comprehensive overview of factors influencing the acceptance of AI devices could help researchers and practitioners evaluate user behaviour when adopting new technologies. ",
    "doi": "10.1177/00187208211064707"
  }
]
